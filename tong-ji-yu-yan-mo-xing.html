<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Shangzhi HUANG" />
        <meta name="copyright" content="Shangzhi HUANG" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="NLP, 语言模型, 自然语言处理, " />

<meta property="og:title" content="统计语言模型 "/>
<meta property="og:url" content="/tong-ji-yu-yan-mo-xing.html" />
<meta property="og:description" content="语言模型(Language model, LM)在自然语言处理中占有重要的地位，尤其是在基于统计语言模型的语音识别、机器翻译、中文分词和句法分析等相关研究中有着广泛的应用。 因为工作上主要做的都是些跟文本挖掘的相关项目，大部分技术都是和NLP相关的。而语言模型又是NLP上一个很基础但很重要的概念，本文就简要总结了一些工作中经常用的语言模型。 1. 统计语言模型 1.1 介绍 通俗的说，语言模型就是用来计算一个句子的概率的概率模型，它通常被描述为字符串s的概率分布\(P(s)\)。这里的字符串s由l个“基元”组成，基元可以是字、词或者短语等。于是句子s的概率可以表示为所有基元的联合概率分布： $$p(s) = p(w_1, w_2, w_3, ..., w_l)$$ 利用bayes公式将其转变为： $$\begin{aligned} p(s) &amp;= p(w_1)p(w_2|w_1)p(w_3|w_1,w_2 …" />
<meta property="og:site_name" content="Shangzhi HUANG&#39;s Blog" />
<meta property="og:article:author" content="Shangzhi HUANG" />
<meta property="og:article:published_time" content="2018-03-10T20:00:00+08:00" />
<meta name="twitter:title" content="统计语言模型 ">
<meta name="twitter:description" content="语言模型(Language model, LM)在自然语言处理中占有重要的地位，尤其是在基于统计语言模型的语音识别、机器翻译、中文分词和句法分析等相关研究中有着广泛的应用。 因为工作上主要做的都是些跟文本挖掘的相关项目，大部分技术都是和NLP相关的。而语言模型又是NLP上一个很基础但很重要的概念，本文就简要总结了一些工作中经常用的语言模型。 1. 统计语言模型 1.1 介绍 通俗的说，语言模型就是用来计算一个句子的概率的概率模型，它通常被描述为字符串s的概率分布\(P(s)\)。这里的字符串s由l个“基元”组成，基元可以是字、词或者短语等。于是句子s的概率可以表示为所有基元的联合概率分布： $$p(s) = p(w_1, w_2, w_3, ..., w_l)$$ 利用bayes公式将其转变为： $$\begin{aligned} p(s) &amp;= p(w_1)p(w_2|w_1)p(w_3|w_1,w_2 …">

        <title>统计语言模型  · Shangzhi HUANG&#39;s Blog
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">
        <link rel="shortcut icon" href="/theme/images/favicon.ico" type="image/x-icon" type="image/png" />
        <link rel="icon" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link href="/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Shangzhi HUANG&#39;s Blog - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="/"><span class=site-name>Shangzhi HUANG's Blog</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="/">Home</a></li>
                            <li ><a href="/categories.html">Categories</a></li>
                            <li ><a href="/tags.html">Tags</a></li>
                            <li ><a href="/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="/tong-ji-yu-yan-mo-xing.html"> 统计语言模型  </a></h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#1">1. 统计语言模型</a><ul>
<li><a href="#11">1.1 介绍</a></li>
<li><a href="#12">1.2 模型缺点</a></li>
</ul>
</li>
<li><a href="#2-nn-gram">2. N元语法(N-gram)模型</a><ul>
<li><a href="#21">2.1 马尔可夫假设</a></li>
<li><a href="#22-n">2.2 N元语法</a></li>
<li><a href="#23">2.3 平滑问题</a></li>
<li><a href="#24-n-gram">2.4 N-gram小节</a></li>
</ul>
</li>
<li><a href="#3">3. 神经网络语言模型</a><ul>
<li><a href="#31">3.1 基础框架</a></li>
<li><a href="#32">3.2 模型介绍</a></li>
<li><a href="#33">3.3 词向量的优势</a></li>
</ul>
</li>
<li><a href="#4-rnn">4. RNN神经网络语言模型</a><ul>
<li><a href="#_1">模型介绍</a></li>
</ul>
</li>
<li><a href="#5">5. 模型评价</a><ul>
<li><a href="#51-extrinsictask-based">5.1 Extrinsic(task-based)</a></li>
<li><a href="#52-intrinsic-perplexity">5.2 Intrinsic - Perplexity</a></li>
</ul>
</li>
<li><a href="#6">6. 语言模型总结</a></li>
<li><a href="#_2">参考文献</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">

            
            
<p>语言模型(Language model, LM)在自然语言处理中占有重要的地位，尤其是在基于统计语言模型的语音识别、机器翻译、中文分词和句法分析等相关研究中有着广泛的应用。</p>
<p>因为工作上主要做的都是些跟文本挖掘的相关项目，大部分技术都是和NLP相关的。而语言模型又是NLP上一个很基础但很重要的概念，本文就简要总结了一些工作中经常用的语言模型。</p>
<h2 id="1">1. 统计语言模型</h2>
<h3 id="11">1.1 介绍</h3>
<p>通俗的说，语言模型就是用来计算一个句子的概率的<strong>概率模型</strong>，它通常被描述为字符串s的概率分布<span class="math">\(P(s)\)</span>。这里的字符串s由l个“基元”组成，基元可以是字、词或者短语等。于是句子s的概率可以表示为所有基元的联合概率分布：
</p>
<div class="math">$$p(s) = p(w_1, w_2, w_3, ..., w_l)$$</div>
<p>
利用bayes公式将其转变为：
</p>
<div class="math">$$\begin{aligned} p(s) &amp;= p(w_1)p(w_2|w_1)p(w_3|w_1,w_2) ... p(w_l|w1,w_2,..,w_{l-1}) \\\\ &amp; = \prod_{i=1}^l p(w_i | w_1...w_{i-1})\end{aligned}$$</div>
<p>
而上式中的条件概率<span class="math">\(p(w_1),p(w_2|w_1), p(w_3,|w_1, w_2)...\)</span>就是语言模型的参数。当这些模型都计算出的时候，对于任何一个给定的句子s都可以通过将对应的条件概率相乘的方法很快得到句子的概率了。不同的模型计算这些参数的值的方式不同，常用的方法有n-gram，决策树，最大熵模型神经网络等等。</p>
<h3 id="12">1.2 模型缺点</h3>
<p>上述模型看上去非常简单，但是通过简单估算我们可以发现其中存在的问题。对于一个大小为N的词典D来说，一个长度为l的句子s，这其中需要的参数就是<span class="math">\(N^l\)</span>个。在这种计算方法中，第i个词是由前i-1个词(历史词)推出的，随着历史词个数的增长，参数个数按指数级增长。这就会导致两个问题：</p>
<ul>
<li>参数空间过大，我们根本无法从训练语料中得到所有的参数，因为大部分的情形在训练语料中根本就没出现</li>
<li>数据稀疏严重，对于没有在语料中出现的组合，根据最大似然估计就会得到这个参数为0，这将导致整个句子的概率为0</li>
</ul>
<h2 id="2-nn-gram">2. N元语法(N-gram)模型</h2>
<h3 id="21">2.1 马尔可夫假设</h3>
<p>为了解决之前讲的参数空间过大的问题，引入马尔科夫假设：一个词出现的概率只和它前面出现的一个或有限的几个词有关：</p>
<div class="math">$$p(s) = \prod_{i=1}^l p(w_i | w_1...w_{i-1} = \prod_{i = 1}^l p(w_i | w_{i-k}, w_{i-(k-1)},...,w_{i-1})$$</div>
<p>可以看到通过这样的方式，模型的参数就得到了大大减少。</p>
<h3 id="22-n">2.2 N元语法</h3>
<p>满足上面的条件的模型就被成为n元语法，其中的n对应考虑的不同历史词个数。通常情况下，n不能取太大的数，否则的话，参数个数过多的问题还是会存在，常用n=1，2，3
n=1时的模型被称为一元语言模型(Uni-gram)，它表示的是句子中每个词之间是条件无关的，也就是说所需要考虑的历史词为0，即
</p>
<div class="math">$$p(w_1, w_2, ..., w_l) = p(w_1)p(w_2)..p(w_l)$$</div>
<p>
对于<span class="math">\(n\geq 2\)</span>的情况，每个词与其前n-1个词有关，即
</p>
<div class="math">$$p(w_1, w_2m .., w_l) = \prod_{i=1}^{l+1} p(w_i|w^{i-1}_{i-n+1})$$</div>
<p>然后根据最大似然估计，这些条件概率的计算就是在语料中统计这些词组合所出现的概率，即：
</p>
<div class="math">$$p(w_k|w^{k-1}_{k-n+1}) = \frac{count(w^{k}_{k-n+1})}{count(w^{k-1}_{k-n+1})}$$</div>
<p>
从中可以看出，n的增大会增加模型的复杂度。虽然理论上n越大，模型的效果越好。但是当n很大的时候，需要的训练语料也会很大，而且当n大到一定程度时，其对于模型效果的提升越来越少，通常使用n=3就已经满足要求了。</p>
<h3 id="23">2.3 平滑问题</h3>
<p>即使是使用n=2的情况，也不能保证语料空间能够涵盖真实的语言模型空间。如果单纯的考虑词频比例作为参数进行计算的话就会碰到两个问题：</p>
<ul>
<li>当<span class="math">\(count(w^k_{k-n+1}) = 0\)</span>时，是不是就能说明概率<span class="math">\(p(w_k|w^{k-1}_1)\)</span>就是0呢？</li>
<li>当<span class="math">\(count(w^k_{k-n+1})=count(w^{k-1}_{k-n+1})\)</span>，能否认为<span class="math">\(p(w_k|w^{k-1}_1)\)</span>为1呢？</li>
</ul>
<p>显然上面的两种情况都是不能，考虑0或1这两个极端情况都会导致整个句子的概率过大或过小，这都是我们不希望看到的。然而，无论语料库有多么大，都无法回避这个问题。平滑化方法就是处理这种问题的，具体的方法有很多，由于平滑算法不是本文的重点，这里就不细说了。</p>
<p>计算n-gram的一个常用工具是<a href="http://www.speech.sri.com/projects/srilm/download.html">SRILM</a>，它是基于C++的，而且里面也集成了很多平滑方法，使用起来非常方便。</p>
<h3 id="24-n-gram">2.4 N-gram小节</h3>
<p>n-gram模型简单来说就是在给定的语料库中统计各个词串出现的次数，并进行适当的平滑处理。之后将这些值存储起来，在计算出现句子的概率时，找到对应的概率参数，相乘即得到总的概率值。</p>
<h2 id="3">3. 神经网络语言模型</h2>
<h3 id="31">3.1 基础框架</h3>
<p>上面N-gram模型的求解看上去更像是一种无监督的方式，简单的统计词串出现的次数，模型也就确定了。然后在使用的时候，再用这种参数相乘做预测就好了。</p>
<p>但是在机器学习领域我们见到的最常见的一种建模方式应该是：</p>
<ul>
<li>对考虑问题建模，这个时候也就是确定模型的参数和输入输出</li>
<li>构造目标函数</li>
<li>利用训练样本对目标函数进行优化，从而得到模型参数的值</li>
</ul>
<p>对于统计语言模型来说，同样可以这样考虑。对于一个词w，它出现的概率由其上下文context(w)决定，为<span class="math">\(p(w|context(w))\)</span>，根据最大似然的定义，对于训练语料中所有词，得到目标函数为：
</p>
<div class="math">$$\prod_{w\in C} p(w|context(w))$$</div>
<p>
其中C为语料，context(w)表示词w的上下文，对于语言模型来说，通常考虑的上下文只有词的上文，即历史部分，上面的n-gram模型就是<span class="math">\(w^{i-1}_{i-n+1}\)</span>。</p>
<p>显然这里的概率是不能按照之前的方式求的，否则就跟直接求n-gram参数没什么两样了。这里概率<span class="math">\(p(w|context(w))\)</span>可以看作是一个关于w和context(w)的函数，即：
</p>
<div class="math">$$p(w|context(w)) = F(w, context(w), \theta$$</div>
<p>
其中<span class="math">\(\theta\)</span>为待定的参数，一旦得到<span class="math">\(\theta\)</span>，模型也就确定了。之后的概率就可以通过F来求了，这样就避免了事先计算并保存所有的词串的概率，而是在需要的时候再计算。通过选择合适的模型，可以使得<span class="math">\(\theta\)</span>的参数个数远远小于n-gram中的参数个数。</p>
<p>这里的关键也就在于函数F的构造，而神经网络语言模型就为其提供了一种构造方法。</p>
<h3 id="32">3.2 模型介绍</h3>
<p>提到神经网络模型可能会有人感到陌生，但是说起word2vec或者说词向量，大家就肯定熟悉了。其实词向量的诞生可以算是神经网络语言模型的附属产物，这个东西最初是构建神经网络语言模型的。但是作为附属产物，词向量由于其强大的语义表示得到了广泛应用。</p>
<p>说起神经网络语言模型，先得祭出Bengio大神论文中的这张图了：</p>
<p><img alt="" src="./images/2018-05-15-20-14-24.jpg"/></p>
<p>这个模型就是上面提到的函数F的一种构造方式。接下来介绍一下在这个模型中，词w是如何和它的上下文context(w)联系起来的。</p>
<p>模型参数：</p>
<ul>
<li>词向量矩阵(N * D)，D为词向量维度，N是词典大小，这个矩阵将每个字对应到一个低维向量</li>
<li>神经网络隐藏层的参数，这里有很多种可能，在Bengio论文中，他表示可以用简单的前向神经网络或者RNN或者其他什么参数模型都是可以的</li>
<li>最后的softmax投影层</li>
</ul>
<p>结合上面的目标函数，这个模型的训练就是给定一个N个字的子句，将其前N-1个词对应到对应的词向量（注意这个词向量并非预先知道的，而是随着模型训练一起得到的），然后经过模型最后经过softmax输出预测新的词，</p>
<p>不过细心的朋友肯定发现了，这个softmax计算量是非常大的，因为通常来说基于词的词典大小经常是10万级的，而字相对来说就好的多，大概1万左右。即使隐藏层维度设置为100，每一步预测都要做一个100*10w = 1000w级的计算是十分耗时的。因此Bengio原始论文中的算法训练起来非常困难，好在后来在Mikolov的word2vec中，对模型进行了优化改进使得训练效率得到了很大提升。</p>
<p>模型优化主要也是针对最后面的softmax输出层的，大致的两个思路是使用Huffman树构建Hierarchy的softmax，这样就讲O(n)转变成了O(logn)，还有一个就是采用Negative Sampling的方法。由于这些优化不是本文重点，以后在讲word2vec的时候再细讲吧。</p>
<h3 id="33">3.3 词向量的优势</h3>
<p>之前的N-gram中，词的特征也可以看作是一种向量，不过是非常稀疏的one-hot向量。这个向量有很多问题，维度非常大，有多少词，就有多少个维度。其次就是向量直接的相似性不好衡量，由于向量之间是正交的，任意两个向量做向量相乘得到的结果都是0.</p>
<p>而通过神经网络语言模型得到的词向量可以看作是将稀疏的高维one-hot词向量转变成了稠密的低维向量，除了相似性以外，词向量还有很多inference相关的有趣特性。</p>
<p>最后相对于N-gram需要进行复杂的平滑处理，词向量自带平滑效果，因为即使是对于词w，没有出现的context(w)时，<span class="math">\(p(w|context(w)) \in (0, 1)\)</span>，并不是为0.</p>
<h2 id="4-rnn">4. RNN神经网络语言模型</h2>
<p>好的语言模型应该至少捕获自然语言的两个特征：语法特性与语义特性。语法特性通常只需考虑生成词的上下文，属于局部特征，在这点上也许N-gram做的不错。但是对于语义特性来说，通常需要考虑的上下文就要长的多。而N-gram通常只能考虑到n=3的情况，明显是无法满足要求的。</p>
<p>而上面的神经网络语言模型虽然较n-gram模型来说具有更强大的表现力和更好的泛化能力，但是同样无法捕捉更长的全局语义信息。比如说，在自然语言中，经常出现的he和himself会出现在比较远的地方，通过上文的神经网络语言模型，即使考虑的句子长度涵盖了这两个字，但是由于混入中间的信息过多，也无法保证能够将其相连(加入attention机制可能对这个问题有帮助)。</p>
<p>为了解决这些问题，Mikolov提出了基于RNN的循环神经网络，使用RNN的隐状态来作为对词序历史信息的记录，使其能够捕捉到句子中的长期依赖。</p>
<p>近年来，随着RNN模型的相关发展，特别是LSTM和GRU等RNN单元的提出，使得基础RNN中在处理长期依赖时遇到的梯度消失问题得到了很大的改善。RNN序列模型处理长期依赖的能力得到了很大的提升，作为一种新的表示方式，RNN/LSTM的语言模型在很多领域都取得了优于N-gram的结果成为主流模型。</p>
<h3 id="_1">模型介绍</h3>
<p>接受了神经网络语言模型引入的词向量概念后，再理解RNN神经网络语言模型就不难了。RNN神经网络语言模型在github有很多相关的项目，很多都是基于LSTM做的生成古诗，歌词等内容的模型。这些在基础结构上大致是一样的，只是用了不同的数据集去训练基于当前数据集的语言模型，然后再利用现有的语言模型做Inference。</p>
<p>RNN的基础结构如下：
<img alt="" src="./images/2018-05-15-21-36-56.png"/>
用在语言模型上，更像是一种sequence模型，其实机器翻译中经常用到的Seq2Seq模型中的decoder部分通常就是这个模型。</p>
<p>因为之后打算再开一篇博客专门讲一下这个模型，这里就先简要介绍一下。这个模型的预测和之前的模型不同的一点是输入是一个一个传入模型的。例如将第一个词作为<span class="math">\(x_0\)</span>，然后得到RNN的隐状态，对于下一个时刻，传入第二个词<span class="math">\(x_1\)</span>和上一刻的隐状态输出，依次这样进行，根据t-1时刻的输出，经过softmax进行t时刻字的预测。这样进行的前提是假设RNN隐藏层的状态已经包含了过去词的信息了。</p>
<h2 id="5">5. 模型评价</h2>
<p>在机器学习中，如何进行模型的评价一直是一个很重要的问题。如果评价的准则都出错了，我们很可能就会错过好的模型而去选择差的模型。而对于语言模型来说，通常有两种评价方式。</p>
<ul>
<li>基于特定任务的extrinsic评价方式</li>
<li>基于模型本身的intrinsic评价方式</li>
</ul>
<h3 id="51-extrinsictask-based">5.1 Extrinsic(task-based)</h3>
<p>语言模型很多时候只是一个中间模型，最终也是为特定任务服务的，有的时候结合任务来看比起单纯的衡量语言模型更有意义。</p>
<p>这种做法就是将不同的语言模型接入到同一套任务中，比如说语音识别，最后通过语音识别任务结果的好坏来评价语言模型。</p>
<h3 id="52-intrinsic-perplexity">5.2 Intrinsic - Perplexity</h3>
<p>对于序列<span class="math">\(w_1, w_2,...,w_n\)</span>, Perplexity混淆度(PPL值)的定义为：
</p>
<div class="math">$$PP(w_1, w_2, ..., w_n)  = P(w_1, w_2, ..., w_n)^{-\frac{1}{n}}$$</div>
<p>
Perplexity表示的是对于一个句子，用语言模型计算其概率值，并且根据序列长度做规范化后的倒数。</p>
<p>于是可以知道最小化perplexity就等价于最大化概率。通常来说，由于概率相乘很容易导致小数溢出的情况，于是通常使用其log表示。</p>
<p>因此，对于一个合理的测试句子集合，一个好的模型的ppl值应该是要小于一个差的模型的。在实际中，我们就可以分出一部分数据作为测试集，在语言模型的训练过程中进行测试保留最优的模型。</p>
<p><strong>注意</strong>: 由于语言模型是和所用到的训练数据挂钩的，如果用一个根据诗歌去训练的语言模型，去计算给定一篇散文的ppl值那肯定是不合理的。因此不同语言模型的ppl值比较必须是使用同一套数据集训练的情况下才有意义。</p>
<p>这里给出一组对于N-gram在不同n值情况下ppl值的比较结果：</p>
<p><img alt="" src="./images/2018-05-15-23-23-33.jpg"/></p>
<p>可以看到n=2对于n=1有个大的提升，n=3对于n=2有提升，但是提升效果已经弱多了。虽然理论上n=4效果会更好，但那需要更多更全的数据集，更耗时的训练时间，通常不会使用。</p>
<h2 id="6">6. 语言模型总结</h2>
<h2 id="_2">参考文献</h2>
<p>[1] <a href="https://book.douban.com/subject/25746399/">统计自然语言处理</a> - 宗成庆</p>
<p>[2] <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a> - Yoshua Bengio, et al</p>
<p>[3] <a href="https://www.cnblogs.com/peghoty/p/3857839.html">word2vec 中的数学原理详解</a> - 博客园用户peghoty</p>
<p>[4] <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a> - Tomas Mikolov, et al</p>
<p>[5] <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Recurrent neural network based language model</a> - Tomas Mikolov, et al</p>
<p>[6] <a href="https://courses.engr.illinois.edu/cs447/fa2015/Slides/Lecture05.pdf">Evaluating language models</a> - Julia Hockenmaier</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            
            
            <hr/>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="/rnnchang-jian-jie-gou.html" title="Previous: RNN常见结构">RNN常见结构</a></li>
            </ul>
            </nav>
            </aside>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2018-03-10T20:00:00+08:00">2018  - 03  - 10</time>
            <h4>Category</h4>
            <a class="category-link" href="/categories.html#zi-ran-yu-yan-chu-li-ref">自然语言处理
                <span>(3)</span>
</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags.html#nlp-ref">NLP
                    <span>3</span>
</a></li>
                <li><a href="/tags.html#yu-yan-mo-xing-ref">语言模型
                    <span>1</span>
</a></li>
            </ul>
                <div class="widget blogroll">
                        <h4>Blogroll</h4>
                        <ul>
                            <li><a href="http://blogwall.us/">Blogwall</a></li>
                            <li><a href="http://www.matrix67.com/">Matrix67</a></li>
                            <li><a href="http://blog.echen.me/">EdwinChen</a></li>
                        </ul>
                </div><!-- /.blogroll -->
<h4>Contact</h4>
    <a href="mailto:shangzhi.huang@gmail.com" title="My email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/ShangzhiH" title="My github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="feeds/all.rss.xml" title="Subscribe in a reader" class="sidebar-social-links" target="_blank">
    <i class="fa fa-rss sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>