<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Shangzhi HUANG" />
        <meta name="copyright" content="Shangzhi HUANG" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="信息论, NLP, 熵, 自然语言处理, " />

<meta property="og:title" content="信息论基本概念  - 各种熵的解释 "/>
<meta property="og:url" content="/xin-xi-lun-ji-ben-gai-nian.html" />
<meta property="og:description" content="最近在看《统计自然语言处理》，觉得第二章预备知识里的关于信息论的一些基本概念总结得很不错。虽然对于熵这个词，我接触过很多次，在机器学习里的很多地方也都有涉及到，比如说最大熵模型，决策树训练时的互信息等等。但是有的时候我还是会经常搞混淆，这里简单介绍一下常用的概念。 一. 熵 对于离散变量\(X\), 假设其取值空间为\(R\)，其概率分布为\(p(x) = P(X = x), x \in R\)，那么定义随机变量\(X\)的熵为： $$H(X) = - \sum_{x \in R} p(x)log_x (p(x))$$ 约定\(0log(0) = 0\)。由于这里使用了2为底，所以该公式定义的熵的单位为二进制单位(比特)，实际情况也有使用其它底数的版本的熵。 熵又被成为自信息(self-information)，可以将其描述为一个随机变量不稳定的程度 …" />
<meta property="og:site_name" content="Shangzhi HUANG&#39;s Blog" />
<meta property="og:article:author" content="Shangzhi HUANG" />
<meta property="og:article:published_time" content="2017-09-30T20:00:00+08:00" />
<meta name="twitter:title" content="信息论基本概念  - 各种熵的解释 ">
<meta name="twitter:description" content="最近在看《统计自然语言处理》，觉得第二章预备知识里的关于信息论的一些基本概念总结得很不错。虽然对于熵这个词，我接触过很多次，在机器学习里的很多地方也都有涉及到，比如说最大熵模型，决策树训练时的互信息等等。但是有的时候我还是会经常搞混淆，这里简单介绍一下常用的概念。 一. 熵 对于离散变量\(X\), 假设其取值空间为\(R\)，其概率分布为\(p(x) = P(X = x), x \in R\)，那么定义随机变量\(X\)的熵为： $$H(X) = - \sum_{x \in R} p(x)log_x (p(x))$$ 约定\(0log(0) = 0\)。由于这里使用了2为底，所以该公式定义的熵的单位为二进制单位(比特)，实际情况也有使用其它底数的版本的熵。 熵又被成为自信息(self-information)，可以将其描述为一个随机变量不稳定的程度 …">

        <title>信息论基本概念  - 各种熵的解释  · Shangzhi HUANG&#39;s Blog
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="/theme/css/custom.css" media="screen">
        <link rel="shortcut icon" href="/theme/images/favicon.ico" type="image/x-icon" type="image/png" />
        <link rel="icon" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link href="/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Shangzhi HUANG&#39;s Blog - Full RSS Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="/"><span class=site-name>Shangzhi HUANG's Blog</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="/">Home</a></li>
                            <li ><a href="/categories.html">Categories</a></li>
                            <li ><a href="/tags.html">Tags</a></li>
                            <li ><a href="/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="/xin-xi-lun-ji-ben-gai-nian.html"> 信息论基本概念  <small> 各种熵的解释 </small>  </a></h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#_1">一. 熵</a></li>
<li><a href="#_2">二. 联合熵和条件熵</a></li>
<li><a href="#_3">三. 互信息</a></li>
<li><a href="#_4">四. 相对熵</a></li>
<li><a href="#_5">五. 交叉熵</a></li>
<li><a href="#_6">六. 困惑度</a></li>
<li><a href="#_7">七. 模拟信道模型</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">

            
            
<p>最近在看《统计自然语言处理》，觉得第二章预备知识里的关于信息论的一些基本概念总结得很不错。虽然对于熵这个词，我接触过很多次，在机器学习里的很多地方也都有涉及到，比如说最大熵模型，决策树训练时的互信息等等。但是有的时候我还是会经常搞混淆，这里简单介绍一下常用的概念。</p>
<h2 id="_1">一. 熵</h2>
<p>对于离散变量<span class="math">\(X\)</span>, 假设其取值空间为<span class="math">\(R\)</span>，其概率分布为<span class="math">\(p(x) = P(X = x), x \in R\)</span>，那么定义随机变量<span class="math">\(X\)</span>的熵为：
</p>
<div class="math">$$H(X) = - \sum_{x \in R} p(x)log_x (p(x))$$</div>
<p>
约定<span class="math">\(0log(0) = 0\)</span>。由于这里使用了2为底，所以该公式定义的熵的单位为二进制单位(比特)，实际情况也有使用其它底数的版本的熵。</p>
<p>熵又被成为自信息(self-information)，可以将其描述为一个随机变量不稳定的程度。通过简单的数学计算可以证明当随机变量<span class="math">\(X\)</span>在其取值空间上对所有值等概率的情况下，熵达到最大值，也就是说随机变量随机性越强，它的熵越大。如果在某一个值上取值概率为1，也就是说这个随机变量其实并不随机，是一个定值，这个时候的熵达到最小值0，它毫无随机性。</p>
<p>熵还可以表示信源<span class="math">\(X\)</span>每发出一个符号所提供的平均信息量。熵越大，越难猜测变量正确的值，因此给予的信息就越多。</p>
<h2 id="_2">二. 联合熵和条件熵</h2>
<p>有了单变量的情况，很自然就想到多变量下联合概率和条件概率的情况。</p>
<p>对于一堆随机变量<span class="math">\(X, Y\)</span>，其联合概率为<span class="math">\(p(x, y)\)</span>，则其联合熵为：
</p>
<div class="math">$$H(X, Y) =  - \sum_{x\in X} \sum_{y \in Y} p(x, y) log_2(p(x, y))$$</div>
<p>
联合熵实际熵就是描述一对随机变量平均所需要的信息量。</p>
<p>条件熵就是给定条件概率<span class="math">\(p(Y|X)\)</span>的情况下：
</p>
<div class="math">$$\begin{aligned} H(Y|X) &amp;= \sum_{x\in X} p(x)H(Y|X = x) = \sum_{x \in X}\sum_{y\in Y}p(x)p(y|x)log(p(y|x) \\\\ &amp;= \sum_{x\in X}\sum_{y \in Y}p(x)\frac{p(x,y)}{p(x)}log(\frac{p(x,y)}{p(x)}) \\\\ &amp;= \sum_{x\in X}\sum_{y \in Y}p(x,y)log(p(x,y) - p(x,y)logp(x) \\\\ &amp;= \sum_{x\in X}\sum_{y \in Y}p(x, y)log(p(x,y) - \sum_{x\in X}p(x)log(p(x)) \\\\ &amp;= H(X, Y) - H(X)\end{aligned}$$</div>
<p>
这个式子也被称为熵的连锁规则，推广到一般情况有：
</p>
<div class="math">$$H(X_1, X_2, ..., X_n) = H(X_1) + H(X_2|X_1) + H(X_3|X_2, X_2) + ... + H(X_n|X_{n-1}, X_{n-2},...,X_1)$$</div>
<p>
条件熵可以看作是在受变量<span class="math">\(X\)</span>影响的情况下，变量<span class="math">\(Y\)</span>的不稳定程度。</p>
<p>对于来自于同一个分布<span class="math">\(X\)</span>的一个随机变量序列<span class="math">\((X_1, X_2, ..., X_n)\)</span>，用<span class="math">\(X_{1n}\)</span>表示。当我们求这个序列的熵的时候，我们可以将其表示为<span class="math">\(n\)</span>个同样的随机分布<span class="math">\(X\)</span>的联合熵，为<span class="math">\(-\sum_{x_{1n}} p(x_{1n})log(p(x_{1n}))\)</span>。</p>
<p>于是，对于一条长度为n的信息，每一个字符或字的熵为：
</p>
<div class="math">$$H_{rate} = \frac{1}{n}H(X_{1n}) = -\frac{1}{n}\sum_{x_{1n}} p(x_{1n})log(p(x_{1n}))$$</div>
<p>
这个数值被称为熵率。</p>
<p>对于语言模型来说，如果假定一种语言是由一系列符号组成的随机过程<span class="math">\(L=(X_i)\)</span>，例如，某报纸的一批预料，那么，可以定义这种语言L的熵率作为其随机过程的熵率：
</p>
<div class="math">$$H_{rate} = lim_{n \rightarrow \infty}\frac{1}{n}H(X_{1n})$$</div>
<h2 id="_3">三. 互信息</h2>
<p>根据上面的链式法则得到：
</p>
<div class="math">$$H(X, Y) = H(X) + H(Y|X)  = H(Y) + H(X|Y)$$</div>
<p>
于是有: </p>
<div class="math">$$H(X) - H(X|Y) = H(Y) - H(Y|X)$$</div>
<p>
这个差值被称为变量<span class="math">\(X\)</span>和<span class="math">\(Y\)</span>之间的互信息，计作<span class="math">\(I(X; Y)\)</span>。它反映了在知道了<span class="math">\(Y\)</span>的值以后，<span class="math">\(X\)</span>的不确定性的减少量，同时也是在知道了<span class="math">\(X\)</span>的值以后，<span class="math">\(Y\)</span>的不确定性的减少量。可以理解为<span class="math">\(Y\)</span>的值透露了多少关于<span class="math">\(X\)</span>的信息量。</p>
<p>将其展开:
</p>
<div class="math">$$\begin{aligned}I(X;Y) &amp;= H(X)-H(X|Y) = H(X) + H(Y) - H(X, Y) \\\\ &amp;= -\sum_x p(x)log(p(x)) - \sum_y p(y)log(p(y)) + \sum_{x, y} p(x, y)log(p(x, y)) \\\\ &amp;= \sum_{x, y}p(x,y)log\frac{p(x, y)}{p(x)p(y)}\end{aligned}$$</div>
<p>
从这个式子可以看出<span class="math">\(I(X;X) = H(X) - H(X|X) = H(X)\)</span>，这也就是把熵称为自信息的原因。另一方面可以看出，如果<span class="math">\(I(X;Y)&gt;&gt;0\)</span>，则表明<span class="math">\(X\)</span>和<span class="math">\(Y\)</span>是高度相关的。如果<span class="math">\(I(X;Y) = 0\)</span>，即<span class="math">\(p(x,y)=p(x)p(y)\)</span>则说明两者完全独立。如果<span class="math">\(I(X;Y)&lt;&lt;0\)</span>，则表明<span class="math">\(Y\)</span>的出现不但未使<span class="math">\(X\)</span>的不确定性降低，反而加大了其不确定性，这通常是不利的。</p>
<p>同样可以推导条件互信息。</p>
<p><strong>条件互信息</strong>
</p>
<div class="math">$$\begin{aligned}I(X;Y|Z) &amp;= I((X;Y | Z)) = H(X|Z) - H(X|Y,Z) \\\\ &amp;= H(X) - I(X;Z) - (H(X) - I(X;Y,Z)) \\\\ &amp;= I(X;Y,Z) - I(X;Z)\end{aligned}$$</div>
<h2 id="_4">四. 相对熵</h2>
<p>相对熵又被称为KL距离，是衡量相同事件空间里两个概率分布相对差距的测度。两个概率分布<span class="math">\(p(x)\)</span>和<span class="math">\(q(x)\)</span>的相对熵定义为：
</p>
<div class="math">$$D(p||q) = \sum_{x\in X}p(x)log\frac{p(x)}{q(x)} = E_p(log\frac{p(x)}{q(x)})$$</div>
<p>
显然，当两个随机分布完全相同，即<span class="math">\(p=q\)</span>时，相对熵为0，当其差别增加时，其相对熵的期望值也增大。</p>
<p>之前证明了</p>
<div class="math">$$I(X;Y) = \sum_{x, y}p(x,y)log\frac{p(x, y)}{p(x)p(y)} = D(p(x,y)||p(x)p(y)$$</div>
<p>于是知道互信息就是衡量一个联合分布与独立性差距多大的测度。</p>
<h2 id="_5">五. 交叉熵</h2>
<p>根据前面的定义，知道熵就是一个不确定性的测度。对于某件事情，我们知道的越多，熵就越小，因而我们对于试验的结果就越不感到意外。交叉熵的概念就是用来衡量估计模型与真是概率分布之间差异情况的。</p>
<p>如果一个随机变量<span class="math">\(X\sim p(x)\)</span>，<span class="math">\(q\)</span>是我们计算得到的模型，<span class="math">\(q(x)\)</span>是模型<span class="math">\(q\)</span>对于真实分布<span class="math">\(p(x)\)</span>的近似表示。那么随机变量<span class="math">\(X\)</span>和模型<span class="math">\(q\)</span>之间的交叉熵定义为：
</p>
<div class="math">$$\begin{aligned}H(X, q) = H(X) + D(p || q) &amp;= -\sum_{x\in X} p(x)log(p(x)) + \sum_{x\in X} p(x)log\frac{p(x)}{q(x)} \\\\ &amp;= -\sum_{x\in X}p(x)log(q(x)) = E_p(log\frac{1}{q(x)})\end{aligned}$$</div>
<p>
这里联想到之前在介绍神经元交叉熵损失的时候给出了这样一个定义<span class="math">\(yln(a) + (1-y)ln(a)\)</span>，可以知道这里的<span class="math">\(y\)</span>并不是对应说真实的标签，而是对于该神经元的两种状态0或1，当其真实值为1时，即<span class="math">\(p(x = 1) = 1\)</span>,损失为<span class="math">\(log(q(x=1)\)</span>，当其真实值为0时，即<span class="math">\(p(x=0) = 1\)</span>,损失就是<span class="math">\(log(1-q(x=1) = log(q(x=0))\)</span>，和我们这里交叉熵的定义完全符合。</p>
<p>注意到这里交叉熵写作<span class="math">\(H(X, q)\)</span>，这似乎就是联合熵的形式，这样表示不会引起误会吗？事实上，交叉熵可以看作是一种特殊场景下的联合熵，它是衡量一个变量<span class="math">\(X\)</span>，和我们对其的近似表示<span class="math">\(q(x)\)</span>的联合熵。如何这个近似非常完美，即<span class="math">\(q(x)\)</span>就是<span class="math">\(X\)</span>的真实分布<span class="math">\(p(x)\)</span>，那么<span class="math">\(D(p||q) = 0, H(X, q) = H(X, X)\)</span>，就是自身的联合熵了。</p>
<p>接着我们定义一个语言<span class="math">\(L = (X) \sim p(x)\)</span>与我们构建的语言模型<span class="math">\(q\)</span>的交叉熵为：
</p>
<div class="math">$$H(L, q) = -lim_{n\rightarrow \infty}\frac{1}{n} \sum_{x^n_1} p(x^n_1)log(q(x^n_1))$$</div>
<p>
其中，<span class="math">\(x^n_1 = x_1, x_2, .. ,x_n\)</span>为语言<span class="math">\(L\)</span>的词序列样本，这里的词包括样本中出现的任意词汇、数字、标点等。我们假设这种语言是“理想”的，于是有n趋于无穷大时，有全部“词汇”的概率和为1，根据信息论的定理，假定语言L是稳态遍历的随机过程，就可以得到：
</p>
<div class="math">$$H(L, q) = -lim_{n\rightarrow \infty}\frac{1}{n} log(q(x^n_1))$$</div>
<p>
就是说可以用样本的熵表示整个语言的熵。</p>
<p>在实际情况下，当我们选择的样本量n足够大的时候，可以将上式子近似表示为<span class="math">\(-\frac{1}{N}log(q(x^N_1)\)</span>，交叉熵越小，表示我们的模型越接近真实的语言模型，效果越好。</p>
<h2 id="_6">六. 困惑度</h2>
<p>在设计语言模型的时候，我们通常并不使用交叉熵而是使用困惑度(perplexity)来表示。给定语言L的样本<span class="math">\(l^n_1 = l_1...l_n\)</span>,L的困惑度PP_q为：
</p>
<div class="math">$$pp_q = 2^{H(L,q)} \approx 2^{-\frac{1}{n}log(q(l^n_1)} = [q(l^n_1)]^{-\frac{1}{n}}$$</div>
<p>
于是语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实语言的情况。</p>
<p>从perplexity的计算式可以看出来，它是对于样本句子出现的概率，在句子长度上Normalize一下的结果。它越小，说明出现概率越大，所得模型就越好。</p>
<h2 id="_7">七. 模拟信道模型</h2>
<p>在学通信原理的时候学习过信道的概念，一个信号经过一个信道，会由于压缩编码，噪声引入，然后在解码的时候就会多少有一点失真。</p>
<p>在自然语言处理中，很多问题也都可以归结为这样的模型。给定输出<span class="math">\(O\)</span>(可能含有误传信息)的情况下，如何从所有可能的输入<span class="math">\(I\)</span>中选出最可能的那个：
</p>
<div class="math">$$\hat{I} = argmax_I p(I|O) = argmax_I \frac{P(I)p(O|I)}{p(O)} = argmax_I p(I)p(O|I)$$</div>
<p>
其中<span class="math">\(p(I)\)</span>成为语言模型，是指在输入语言中“词”序列的概率分布；另一个<span class="math">\(p(O|I)\)</span>成为信道概率。</p>
<p>对应到实际的NLP问题，比如说机器翻译在进行汉译英的时候，汉语句子看作是信道输出O，求出最可能的信道输入英语句子I。</p>
<p>噪声信道模型在NLP中有非常多的用途，除了机器翻译以外，还用于词性标注、语音识别、文字识别等很多问题的研究。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            
            
            <hr/>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-5.html" title="Previous: 《深度学习与神经网络》笔记5 - 深度神经网络学习过程中的梯度消失问题">《深度学习与神经网络》笔记5 <small>深度神经网络学习过程中的梯度消失问题</small></a></li>
                <li class="next-article"><a href="/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-6.html" title="Next: 《深度学习与神经网络》笔记6 - 深度学习">《深度学习与神经网络》笔记6 <small>深度学习</small></a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2017-09-30T20:00:00+08:00">2017  - 09  - 30</time>
            <h4>Category</h4>
            <a class="category-link" href="/categories.html#zi-ran-yu-yan-chu-li-ref">自然语言处理
                <span>(3)</span>
</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="/tags.html#nlp-ref">NLP
                    <span>3</span>
</a></li>
                <li><a href="/tags.html#shang-ref">熵
                    <span>1</span>
</a></li>
                <li><a href="/tags.html#xin-xi-lun-ref">信息论
                    <span>1</span>
</a></li>
            </ul>
                <div class="widget blogroll">
                        <h4>Blogroll</h4>
                        <ul>
                            <li><a href="http://blogwall.us/">Blogwall</a></li>
                            <li><a href="http://www.matrix67.com/">Matrix67</a></li>
                            <li><a href="http://blog.echen.me/">EdwinChen</a></li>
                        </ul>
                </div><!-- /.blogroll -->
<h4>Contact</h4>
    <a href="mailto:shangzhi.huang@gmail.com" title="My email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
    <a href="https://github.com/ShangzhiH" title="My github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="feeds/all.rss.xml" title="Subscribe in a reader" class="sidebar-social-links" target="_blank">
    <i class="fa fa-rss sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    
    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>