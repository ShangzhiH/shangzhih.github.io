<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Shangzhi HUANG's Blog</title><link>/</link><description></description><lastBuildDate>Fri, 16 Feb 2018 20:55:00 +0800</lastBuildDate><item><title>RNN常见结构</title><link>/rnnchang-jian-jie-gou.html</link><description>
&lt;h2 id="_1"&gt;介绍&lt;/h2&gt;
&lt;p&gt;深度学习中，虽然CNN除了被经常用来进行图像相关的任务外，也可以作为一种特征提取的方法用在NLP任务中。但是在NLP任务中，更多的我们还是使用RNN模型，本文就简单介绍几种常用的RNN结构。&lt;/p&gt;
&lt;h2 id="rnn"&gt;基本RNN&lt;/h2&gt;
&lt;p&gt;不同于在CNN模型中，网络的状态只决定于输入。在RNN中，最明显的一个特征就是它还决定于上一个时刻的状态，因此RNN经常被用来处理序列问题，被用在序列性质非常明显的NLP任务上。&lt;/p&gt;
&lt;p&gt;基本的RNN结构如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./images/2018-03-18-19-42-00.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;它跟CNN和DNN区别最大的地方就在于这些前馈神经网络是个&lt;code&gt;有向无环图&lt;/code&gt;的模型(DAG)，而在RNN中是至少包含一个环的，在时间上进行展开我们可以更明显的看到这种特性：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./images/2018-03-18-19-45-05.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;公式可以表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{aligned}&amp;amp; h^t = tanh(W_{hx}x^t + W_{hh}h^{t-1}+b_h) \\\\ &amp;amp;o^t = softmax(W_{oh}h^t + b_o) \end{aligned}$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(x^t\)&lt;/span&gt;是t时刻的输入 …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Fri, 16 Feb 2018 20:55:00 +0800</pubDate><guid isPermaLink="false">tag:None,2018-02-16:/rnnchang-jian-jie-gou.html</guid><category>算法</category><category>RNN</category><category>深度学习</category></item><item><title>条件随机场CRF</title><link>/tiao-jian-sui-ji-chang-crf.html</link><description>
&lt;h2 id="1"&gt;1. 概率无向图&lt;/h2&gt;
&lt;p&gt;不同于HMM中状态序列是有方向的，在CRF中，我们使用的是概率无向图模型。&lt;/p&gt;
&lt;p&gt;状态与状态之间并没有谁推导出谁的先后关系(&lt;span class="math"&gt;\(P(y_n|y_{n-1}, y_{n-2}...y_1)\)&lt;/span&gt;)，这里我们用的是序列整体的分布&lt;span class="math"&gt;\(P(y_1,y_2,...y_n)\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id="_1"&gt;因子分解&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;团：&lt;/strong&gt; 对于无向图中的一个子集，如果该子集中任意两个点都是有边相连接的，那么这样的子集就叫做该无向图上的一个团&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最大团：&lt;/strong&gt; 对于一个团，如果无法添加另外一个点使得团增大，那么现有的团就是最大团&lt;/p&gt;
&lt;p&gt;一个概率无向图总的的联合概率分布&lt;span class="math"&gt;\(P(Y)\)&lt;/span&gt;可以分解为在其所有最大团上定义的一个势函数的积：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y) = \frac{\prod_C{\Phi_C(Y_C)}}{Z}$$&lt;/div&gt;
&lt;p&gt;
其中C是最大团的集合，对于其中每一个最大团，有一个势能函数&lt;span class="math"&gt;\(\Phi_C\)&lt;/span&gt;，&lt;span class="math"&gt;\(Y_C\)&lt;/span&gt;为属于这个最大团的节点，&lt;span class="math"&gt;\(Z=\sum_Y{\prod_C{\Phi_C(Y_C)}}\)&lt;/span&gt;是一个规范化因子 …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Wed, 17 Jan 2018 19:55:00 +0800</pubDate><guid isPermaLink="false">tag:None,2018-01-17:/tiao-jian-sui-ji-chang-crf.html</guid><category>算法</category><category>统计机器学习</category><category>概率图模型</category></item><item><title>指数分布族和广义线性回归</title><link>/zhi-shu-fen-bu-zu-he-yan-yi-xian-xing-hui-gui.html</link><description>
&lt;h2 id="_1"&gt;指数分布族&lt;/h2&gt;
&lt;h3 id="1"&gt;1. 定义&lt;/h3&gt;
&lt;p&gt;指数分布族不是专指一种分布，而是一系列符合特征的分布的统称。常用的诸如正态分布，伯努利分布，指数分布，泊松分布，gamma分布都属于指数分布族。
&lt;/p&gt;
&lt;div class="math"&gt;$$p(y;\theta) = b(y)exp(\eta(\theta)T(y) - A(\theta))$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;b(y) - underlying measure&lt;/li&gt;
&lt;li&gt;T(y) - sufficient statistic&lt;/li&gt;
&lt;li&gt;A(&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;) - log normalizer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通常情况下T(y) = y,  A, b, T, &lt;span class="math"&gt;\(\eta\)&lt;/span&gt;给定的不同，就能得到不同的y的分布&lt;/p&gt;
&lt;p&gt;其中的变量y和参数&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;只在&lt;span class="math"&gt;\(T(y)\eta …&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Fri, 05 Jan 2018 20:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2018-01-05:/zhi-shu-fen-bu-zu-he-yan-yi-xian-xing-hui-gui.html</guid><category>概率分布</category><category>统计机器学习</category><category>回归模型</category><category>算法</category></item><item><title>Early Stopping</title><link>/early-stopping.html</link><description>
&lt;h2 id="1-early-stopping"&gt;1. early stopping简介&lt;/h2&gt;
&lt;h3 id="11"&gt;1.1 介绍&lt;/h3&gt;
&lt;p&gt;在机器学习中经常会遇到过拟合的问题，特别是对于神经网络这种参数非常多，表达能力非常强的复杂模型来说。经常就是随着训练过程的迭代，loss在训练数据上越来越小。看上去我们的模型越来越精确了，但是实际情况却是，我们的模型的泛化能力经常会变的越来越差，也就是说对于新的数据，模型的表现并不见的会好。&lt;/p&gt;
&lt;p&gt;通常解决过拟合都是设法简化模型，这可以通过&lt;code&gt;减少模型的参数个数&lt;/code&gt;或者&lt;code&gt;减少单个参数的大小&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;减少模型的参数个数，决策树里的剪枝，CNN里的weight sharing等都属于这种方法。&lt;/p&gt;
&lt;p&gt;减少单个参数的大小，回归里的L1，L2正则化，nn里的weight decay等和我们本文要讲的early stopping都属于这种方法。&lt;/p&gt;
&lt;h3 id="12-early-stopping"&gt;1.2 基本的early stopping&lt;/h3&gt;
&lt;p&gt;最基本的early stopping就是将训练数据分出一部分作为验证集，然后每次训练迭代结束时，在验证集上检验一下模型的精度，作为模型的泛化误差。理想情况下，随着训练进行，训练误差越来越小，验证集的误差也越来越小，直到某个时间点时，出现过拟合，然后在验证集上误差开始上升。这个时候我们就停止训练，采用上一次训练的模型作为最终模型。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./images/2018-02-22-10-59-14.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;这种方法是假设验证集和真实情况是符合同样的分布 …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Sun, 15 Oct 2017 00:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2017-10-15:/early-stopping.html</guid><category>过拟合</category><category>Paper Reading</category><category>机器学习</category></item><item><title>《深度学习与神经网络》笔记6</title><link>/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-6.html</link><description>
&lt;p&gt;深度神经网络在可以模拟更加复杂的情形，但是在上一章中，我们发现训练深度神经网络的时候会出现梯度消失的问题，从而导致模型训练失败。这一章，将会介绍可以被用在深度学习上的一些技术。&lt;/p&gt;
&lt;p&gt;这章的主要内容是介绍一种应用最广泛的深度神经网络：卷积神经网络。我们将会了解到卷积，池化等概念，通过在之前的代码上利用这些技术进行优化达到了惊人的99.67%的准确率。&lt;/p&gt;
&lt;p&gt;除此之外，本章还将介绍一些其他的基本神经网络，例如循环神经网络。在介绍完这些之后，还会介绍深度学习技术的发展现状及未来的发展方向。&lt;/p&gt;
&lt;h2 id="_1"&gt;一. 卷积神经网络&lt;/h2&gt;
&lt;p&gt;之前我们在进行MNIST数字分类的时候，输入数据是将每张图片按像素展开得到的784维向量，这样训练得到的结果虽然不错，但是仔细想想就会发现它的问题。对于图片来说，不同的不同的像素点之间的距离很远，旧的方法就完全没有考虑像素点之间的空间联系。这一节介绍的卷积神经网络就考虑了这种空间联系，并且训练迅速，在图像分类问题上得到了非常好的效果。&lt;/p&gt;
&lt;p&gt;卷积神经网络涉及到三个基本思想：local receptive fields(局部感受野)，shared weights(参数共享)，pooling(池化)，接下来依次介绍。&lt;/p&gt;
&lt;h3 id="local-receptive-fields"&gt;Local receptive fields:&lt;/h3&gt;
&lt;p&gt;在之前的神经网络中，输入神经网络的数据是一个多维向量，与输入层连接的隐藏层的每一个神经元都和所有的输入层神经元连接。&lt;/p&gt;
&lt;p&gt;&lt;img alt="sdxxsjwl61" src="./images/sdxxsjwl61.png"/&gt;&lt;/p&gt;
&lt;p&gt;这样一副784个像素点的图片中的每一个像素点都是一个输入神经元，后一层的每个神经元和所有这些输入神经元都有连接。&lt;/p&gt;
&lt;p&gt;local …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Mon, 02 Oct 2017 20:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2017-10-02:/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-6.html</guid><category>算法</category><category>深度学习</category><category>《深度学习与神经网络》笔记</category></item><item><title>信息论基本概念</title><link>/xin-xi-lun-ji-ben-gai-nian.html</link><description>
&lt;p&gt;最近在看《统计自然语言处理》，觉得第二章预备知识里的关于信息论的一些基本概念总结得很不错。虽然对于熵这个词，我接触过很多次，在机器学习里的很多地方也都有涉及到，比如说最大熵模型，决策树训练时的互信息等等。但是有的时候我还是会经常搞混淆，这里简单介绍一下常用的概念。&lt;/p&gt;
&lt;h2 id="_1"&gt;一. 熵&lt;/h2&gt;
&lt;p&gt;对于离散变量&lt;span class="math"&gt;\(X\)&lt;/span&gt;, 假设其取值空间为&lt;span class="math"&gt;\(R\)&lt;/span&gt;，其概率分布为&lt;span class="math"&gt;\(p(x) = P(X = x), x \in R\)&lt;/span&gt;，那么定义随机变量&lt;span class="math"&gt;\(X\)&lt;/span&gt;的熵为：
&lt;/p&gt;
&lt;div class="math"&gt;$$H(X) = - \sum_{x \in R} p(x)log_x (p(x))$$&lt;/div&gt;
&lt;p&gt;
约定&lt;span class="math"&gt;\(0log(0) = 0\)&lt;/span&gt;。由于这里使用了2为底，所以该公式定义的熵的单位为二进制单位(比特)，实际情况也有使用其它底数的版本的熵。&lt;/p&gt;
&lt;p&gt;熵又被成为自信息(self-information)，可以将其描述为一个随机变量不稳定的程度 …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Sat, 30 Sep 2017 20:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2017-09-30:/xin-xi-lun-ji-ben-gai-nian.html</guid><category>信息论</category><category>自然语言处理</category><category>熵</category></item><item><title>《深度学习与神经网络》笔记5</title><link>/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-5.html</link><description>
&lt;p&gt;之前的章节，我们利用一个仅包含一层隐藏层的简单神经网络就在MNIST识别问题上获得了98%左右的准确率。我们于是本能会想到用更多的隐藏层，构建更复杂的神经网络将会为我们带来更好的结果。&lt;/p&gt;
&lt;p&gt;就如同在进行图像模式识别的时候，第一层的神经层可以学到边缘特征，第二层的可以学到更复杂的图形特征，例如三角形，长方形等，第三层又会识别更加复杂的图案。这样看来，多层的结构就会带来更强大的模型，进行更复杂的识别。&lt;/p&gt;
&lt;p&gt;那么在这一章，就试着训练这样的神经网络来看看对结果有没有什么提升。不过我们发现，训练的过程将会出现问题，我们的神经网络的效果并没有什么提升。&lt;/p&gt;
&lt;p&gt;为什么会出现这样的情况呢，这一章就是主要围绕着这个问题展开的。我们将会发现，不同层的学习速率是不一样的。例如，在后面的网络层训练正在顺利学习的时候，前面网络层的学习却卡住几乎不动了。而且我们会发现这并不是偶然的，而是在理论上由梯度下降算法导致的。随着我们对问题的深入了解，我们会发现相反的情况也是可能发生的，就是前面网络层学习正常，而后面网络层学习停止。&lt;/p&gt;
&lt;p&gt;这虽然看上去都是坏消息，不过深入探索这些问题也是帮助我们设计更好的更高效的深度神经网络的训练方法。&lt;/p&gt;
&lt;h2 id="_1"&gt;一. 梯度消失问题&lt;/h2&gt;
&lt;p&gt;先回到之前的程序上，当我们选择一个隐藏层的时候得到准确率为96.48%。接着增加一个隐藏层得到96.90%的结果。看上去结果不错，毕竟提升了。接着再加上一个隐藏层，却只得到了96.57%的结果。这个结果虽说下降了没多少，但是我们模型变复杂了 …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Fri, 22 Sep 2017 20:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2017-09-22:/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-5.html</guid><category>算法</category><category>深度学习</category><category>《深度学习与神经网络》笔记</category></item><item><title>《深度学习与神经网络》笔记4</title><link>/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-4.html</link><description>
&lt;p&gt;神经网络最令人激动的一个性质，就是它可以实现任意功能的函数。而且是即使对于只有一个隐藏层的神经网络，这个结论依然成立。&lt;/p&gt;
&lt;p&gt;大部分神经网络的使用者都知道这个性质，但是并不理解为什么神经网络会有这样的性质。而其理论证明对于非数学专业的同学来说并不好理解，所以本章旨在用直观的方式帮助大家理解这个性质。&lt;/p&gt;
&lt;h2 id="_1"&gt;一. 两个前提&lt;/h2&gt;
&lt;p&gt;神经网络可以计算任意函数其实是有前提的。&lt;/p&gt;
&lt;p&gt;首先要明白的是它并不是可以完全准确的计算原函数的值，但是通过增加隐藏层神经元的值我们可以越来越逼近原函数。就是说对于一个需要实现的函数&lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;，要求实现精度为&lt;span class="math"&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;，也就是需要足够的隐藏层神经元使得神经网络输出&lt;span class="math"&gt;\(g(x)\)&lt;/span&gt;满足&lt;span class="math"&gt;\(|g(x) - f(x)| &amp;lt; \epsilon\)&lt;/span&gt;对所有输入&lt;span class="math"&gt;\(x\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;第二个前提是被模拟的函数是连续函数，不过有的时候对于非连续函数，神经网络得到的连续近似已经足够满足要求了。&lt;/p&gt;
&lt;h2 id="_2"&gt;二. 单输入单输出的情况&lt;/h2&gt;
&lt;p&gt;先考虑最基础的单输入单输出的函数情况。为了理解怎么利用神经网络去计算给定函数&lt;span class="math"&gt;\(f\)&lt;/span&gt;，我们先考虑只有一个隐藏层的情况，其中含有两个神经元。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./images/sdxxsjwl41.png"/&gt;&lt;/p&gt;
&lt;p&gt;考虑隐藏层第一个神经元，其输出由&lt;span class="math"&gt;\(\sigma (wx+b)\)&lt;/span&gt;决定。改变其参数&lt;span class="math"&gt;\(w\)&lt;/span&gt;和&lt;span class="math"&gt;\(b …&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Sun, 17 Sep 2017 20:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2017-09-17:/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-4.html</guid><category>算法</category><category>深度学习</category><category>《深度学习与神经网络》笔记</category></item><item><title>《深度学习与神经网络》笔记3</title><link>/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-3.html</link><description>
&lt;p&gt;我们前面已经学习了反向传播算法，它是我们学习神经网络的基础。这一章将会介绍一系列的方法技巧来改善反向传播算法的效果，进而改善学习到的神经网络模型。&lt;/p&gt;
&lt;p&gt;这些技巧包括：
1. 新的损失函数：交叉熵损失函数
2. 四种“正则化”方法（L1，L2，dropout，人造训练数据），这些是为了我们的模型有更好的扩展性，在新的数据集上能有更好的表现，而不至于过拟合
3. 一种更好的初始化权重的方法
4. 一系列帮助选择超参的启发式方法。随后会在之前代码的基础上实现这些优化来改善我们手写数字识别的准确度。&lt;/p&gt;
&lt;p&gt;当然这些也只是优化神经网络的冰山一角，还有很多其他的方法。不过掌握了这些基本的方法，可以加深我们对于问题的理解，对于新的方法技巧也可以很快的上手。&lt;/p&gt;
&lt;h2 id="_1"&gt;一. 交叉熵损失函数&lt;/h2&gt;
&lt;p&gt;失败是成功之母，我们学习的过程中免不了要犯错，知错能改，善莫大焉。神经网络的学习也是如此，如果错误都没有定义好的话，模型的学习过程当然也会很缓慢。&lt;/p&gt;
&lt;p&gt;理想情况下，我们希望我们的学习算法越快收敛到最佳状态越好，那么实际情况呢？我们先来看一个例子：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./images/sdxxsjwl31.png"/&gt;&lt;/p&gt;
&lt;p&gt;这是一个最简单的模型，只有一个神经元，一个输入。我们希望它实现一个简单的功能：当输入为1的时候，输出为0。 这个功能很容易实现，我们手动设置参数都很容易就能满足 …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Sun, 10 Sep 2017 20:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2017-09-10:/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-3.html</guid><category>算法</category><category>深度学习</category><category>《深度学习与神经网络》笔记</category></item><item><title>《深度学习与神经网络》笔记2</title><link>/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-2.html</link><description>
&lt;p&gt;上一章中我们遗留了一个问题，就是在神经网络的学习过程中，在更新参数的时候，如何去计算损失函数关于参数的梯度。这一章，我们将会学到一种快速的计算梯度的算法：反向传播算法。&lt;/p&gt;
&lt;p&gt;这一章相较于后面的章节涉及到的数学知识比较多，如果阅读上有点吃力的话也可以完全跳过这一章，把反向传播当成一个计算梯度的黑盒即可，但是学习这些数学知识可以帮助我们更深入的理解神经网络。&lt;/p&gt;
&lt;p&gt;反向传播算法的核心目的是对于神经网络中的任何weight或bias计算损失函数&lt;span class="math"&gt;\(C\)&lt;/span&gt;关于它们的偏导数&lt;span class="math"&gt;\(\frac{\partial C}{\partial w}\)&lt;/span&gt;. 这个式子能够帮助我们知道当我们改变&lt;span class="math"&gt;\(w\)&lt;/span&gt;或&lt;span class="math"&gt;\(b\)&lt;/span&gt;的时候，损失函数&lt;span class="math"&gt;\(C\)&lt;/span&gt;是怎么变化的。虽然计算这个式子可能有一点复杂，但是它提供了一种自然的，直观的解释，所以说反向传播算法并不仅仅是一种快速学习算法，它提供给我们具体的见解，帮助我们理解改变神经网络的参数是如何改变神经网络行为的。所以说反向传播算法是很值得我们去深入学习的。&lt;/p&gt;
&lt;p&gt;当然跳过这章也没问题，把反向传播当作求梯度的黑盒也并不影响阅读作者后续的章节。虽然后面会有一些涉及到本章知识的地方，不过跳过这些依然不会影响我们理解文章的主要内容。&lt;/p&gt;
&lt;h2 id="_1"&gt;一. 一种基于矩阵运算快速计算神经网络输出的方法&lt;/h2&gt;
&lt;p&gt;在介绍反向传播之前，先介绍怎么利用矩阵运算快速的计算神经网络输出。其实在上一章对这一块也提到过，不过不够详细。这里再介绍一下，帮助大家逐渐适应基于矩阵运算的表示方式。&lt;/p&gt;
&lt;p&gt;我们先引入一个能够明确表示连接神经网络中某两层之间的某两个神经元的权重的符号：&lt;span class="math"&gt;\(w^l_{jk …&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Fri, 01 Sep 2017 20:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2017-09-01:/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-2.html</guid><category>算法</category><category>深度学习</category><category>《深度学习与神经网络》笔记</category></item><item><title>《深度学习与神经网络》笔记1</title><link>/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-1.html</link><description>
&lt;p&gt;深度学习算是现在机器学习领域非常热门的方向了，虽然一直有了解并且简单用过，但是对于其中的详细原理和来龙去脉都是略知一二，于是一直想系统学习一下该领域的相关知识。&lt;a href="http://neuralnetworksanddeeplearning.com/about.html"&gt;《Neural Networks and Deep Learning》&lt;/a&gt;是一份非常好的入门材料，讲解详细而且不光是介绍了理论知识，更重要的是介绍了每一步的来龙去脉以及为什么要这样做。在线文档是英文版的，我这份总结笔记的很大部分是结合原文根据自己的理解加以提炼翻译过来的，英文水平有限，出现问题请指正。想看原版完整文档的同学可以点击上面的链接。&lt;/p&gt;
&lt;h2 id="_1"&gt;一. 本书内容&lt;/h2&gt;
&lt;p&gt;传统的计算机算法在解决问题的时候，通常都是由程序员制定好规则将问题进行分解，一步步进行解决。应用神经网络我们一般并不需要告诉计算机应该怎么去解决问题，而是只要给到足够的观测数据就可以了，它将会自动从这些数据中提取出解决方法。&lt;/p&gt;
&lt;p&gt;本书主要的目的是帮助读者掌握包括深度学习相关技术在内的神经网络领域的核心知识。在掌握了本书内容后，可以使用深度学习模型解决遇到的问题，更进一步可以设计自己的神经网络用以解决特定的问题。&lt;/p&gt;
&lt;p&gt;当然这本书也不会完全是理论知识，作者通过“手写数字识别“这个常见但是普通编程方法很难解决的问题介绍了神经网络的基本知识。通过这本书，他还基于python一步一步实现了一个简单的神经网络库，使得大家在接触到其他新的神经网络库的时候也能很快的理解并读懂代码。&lt;/p&gt;
&lt;h2 id="_2"&gt;二. 手写字体识别&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="./images/sdxxsjwl11.png"/&gt;&lt;/p&gt;
&lt;p&gt;对于这样一副图片，人脑很容易就能识别出来其中的数字。但是对于计算机就没有这么容易了，按照以前老的解决问题的思路，就是规则的堆砌，比如说“上部分有个圈，右下方有条垂直线，这个数字就是9”，很显然，对于手写字体这样肯定是不切实际的。因为手写数字太不规范了 …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Wed, 23 Aug 2017 20:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2017-08-23:/shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-1.html</guid><category>算法</category><category>深度学习</category><category>《深度学习与神经网络》笔记</category></item><item><title>Spark ML下实现的多分类AdaBoost + NaiveBayes算法</title><link>/spark-mlxia-shi-xian-de-duo-fen-lei-adaboost-naivebayessuan-fa.html</link><description>
&lt;h2 id="1-naive-bayes"&gt;1. Naive Bayes算法&lt;/h2&gt;
&lt;p&gt;朴素贝叶斯算法算是生成模型中一个最经典的分类算法之一了，常用的有Bernoulli和Multinomial两种。在文本分类上经常会用到这两种方法。在词袋模型中，对于一篇文档&lt;span class="math"&gt;\(d\)&lt;/span&gt;中出现的词&lt;span class="math"&gt;\(w_0,w_1,...,w_n\)&lt;/span&gt;, 这篇文章被分类为&lt;span class="math"&gt;\(c\)&lt;/span&gt;的概率为&lt;/p&gt;
&lt;div class="math"&gt;$$p(c|w_0,w_1,...,w_n) = \frac{p(c,w_0,w_1,...,w_n)}{p(w_0,w_1,...,w_n)} = \frac{p(w_0,w_1,...,w_n|c)*p(c)}{p(w_0,w_1,...,w_n)}$$&lt;/div&gt;
&lt;p&gt; 对于一篇给定文章，分母为常数，基于朴素贝叶斯的各词在一篇文章中出现独立性假设，最后我们需要比较的就是在不同类别&lt;span class="math"&gt;\(c\)&lt;/span&gt;下&lt;span class="math"&gt;\(p …&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shangzhi HUANG</dc:creator><pubDate>Sat, 05 Aug 2017 20:00:00 +0800</pubDate><guid isPermaLink="false">tag:None,2017-08-05:/spark-mlxia-shi-xian-de-duo-fen-lei-adaboost-naivebayessuan-fa.html</guid><category>AdaBoost</category><category>算法</category><category>Spark</category><category>朴素贝叶斯</category><category>文本分类</category><category>机器学习</category></item></channel></rss>