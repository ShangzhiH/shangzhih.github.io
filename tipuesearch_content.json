{"pages":[{"title":"条件随机场CRF","text":"1. 概率无向图 不同于HMM中状态序列是有方向的，在CRF中，我们使用的是概率无向图模型。 状态与状态之间并没有谁推导出谁的先后关系( \\(P(y_n|y_{n-1}, y_{n-2}...y_1)\\) )，这里我们用的是序列整体的分布 \\(P(y_1,y_2,...y_n)\\) 。 因子分解 团： 对于无向图中的一个子集，如果该子集中任意两个点都是有边相连接的，那么这样的子集就叫做该无向图上的一个团 最大团： 对于一个团，如果无法添加另外一个点使得团增大，那么现有的团就是最大团 一个概率无向图总的的联合概率分布 \\(P(Y)\\) 可以分解为在其所有最大团上定义的一个势函数的积： $$P(Y) = \\frac{\\prod_C{\\Phi_C(Y_C)}}{Z}$$ 其中C是最大团的集合，对于其中每一个最大团，有一个势能函数 \\(\\Phi_C\\) ， \\(Y_C\\) 为属于这个最大团的节点， \\(Z=\\sum_Y{\\prod_C{\\Phi_C(Y_C)}}\\) 是一个规范化因子，目的是为了让 \\(P(Y)\\) 构成一个概率分布。 由于势函数是严格正的，所以一般就定义为指数函数 \\(exp(-E(Y_C))\\) 总结： 概率无向图模型的联合概率分布可以分解为其所有 最大团 上的势函数的积。 2. 条件随机场 定义 CRF里的条件指的是在给定随机变量X的条件下，随机变量Y的马尔科夫随机场。 通常情况，只使用线性链条件随机场，将其用于标注问题，条件概率为P(Y|X)。其中X是给定的观测序列，Y是需要标注的标注序列(状态序列)。 一般形式 $$P(Y_v|X, Y_w w\\neq v) = P(Y_v | X, Y_w w~v)$$ 对于任意节点v成立，则称条件概率分布P(Y|X)为条件随机场。 同HMM的条件概率相比较，可以更好的理解CRF的条件概率定义。首先，在HMM中，观测序列中每个位置的观测值只和它所处的状态有关，而在CRF中，所有的观测序列是作为一个整体X来处理的。其次，对于 \\(w\\neq v\\) 是说除了v以外的所有点， \\(w~v\\) 是说和v相邻的点，也就是状态序列某一位置的状态y只和与它有连接的状态值有关。 线性链形式 线性链形式顾名思义就是状态序列 \\(Y=(Y_1, Y_2, ..., Y_n)\\) 是线性的，也就是说某一位置的状态只和它前后两个状态相连。 $$P(Y_i |X, Y_1,...,Y_{i-1},Y_{i+1},...,Y_n) = P(Y_i|X, Y_{i-1}, Y_{i+1})$$ 参数化形式 根据之前对无向图模型的分解，对于线性链条件随机场，很容易验证每两个相邻的状态节点 \\(y_{i-1},y_i\\) 构成一个最大团。 于是线性链条件随机场的参数化形式可以分解为每个团上的特征函数和X对每个位置i上的特征函数： $$P(y|x) = \\frac{1}{Z(x)}exp(\\sum_{i,k}\\lambda_k t_k (y_{i-1}, y_i, x, i) + \\sum_{i, l}\\mu_l s_l (y_i, x, i)$$ 其中 \\(t_k\\) 是定义在边上的特征函数，成为 转移特征 ，依赖于当前和前一个位置(构成一个最大团)， \\(\\lambda_k\\) 是特征 \\(t_k\\) 的权重。 \\(s_l\\) 是定义在节点上的特征函数，称为 状态特征 ，依赖于当前位置， \\(\\mu_l\\) 为特征 \\(s_l\\) 的权重。 \\(t_k\\) 和 \\(s_l\\) 都依赖于位置，是局部特征函数，通常，它们的取值为1或0，当满足特征条件时值为1，反之为0. 条件随机场完全由特征函数 \\(t_k\\) 和 \\(s_l\\) 及其对应的权重 \\(\\lambda_k\\) 和 \\(\\mu_l\\) 确定。 简化形式 由参数化形式可是对于每个特征函数，它都会计算在所有位置上的值，于是可以对每个局部特征，在所有位置上求和得到一个全局特征函数。 汇总K1个转移特征和K2个状态特征： \\(k = 1, 2, ..., K_1\\) 时， \\(f_k (y_{i-1}, y_i, x, i) = t_k (y_{i-1}, y_i, x, i)\\) \\(k = K_1 +l, l = 1, 2, ... , K_2\\) 时， \\(f_k (y_{i - 1}, y_i, x, i) = s_l (y_i, x, i)\\) 对每个特征函数在所有位置求和： $$f_k (y, x) = \\sum_{i=1}&#94;n f_k(y_{i-1}, y_i, x, i), k = 1, 2, ..., K$$ 用 \\(w_k = (\\lambda_1, \\lambda_2, .., \\lambda_{K_1}, \\mu_{K_1 + 1}, .., \\mu_{K_1 + K_2})\\) 表示每个特征的权重。 于是条件随机场可以表示为： $$P(y|x) = \\frac{1}{Z(x)}exp\\sum_{k=1}&#94;K w_k f_k(y, x)$$ 如果将权重和特征函数看成向量形式， \\(w = (w_1, w_2, ..., w_K)&#94;T\\) 和 \\(F(y, x) = (f_1(y,x), f_2(y,x),...,f_K(y,x)\\) ，则可以将上式化简为： $$P_w(y|x) = \\frac{exp(w\\cdot F(y,x))}{Z_w(x)}$$ 矩阵形式 上面的简化形式是对于每个特征函数在所有位置上求和，同样的我们也可以考虑在每个位置上对所有特征函数的值求和。 $$M_i(y_{i-1}, y_i|x) = exp(W_i (y_{i-1}, y_i | x)) = exp(\\sum_{k=1}&#94;K w_k f_k(y_{i-1}, y_i, x, i)$$ 假设每个位置上的y有m个取值，则 \\(M_i(y_{i-1}, y_i)\\) 就是一个m*m阶的矩阵. 对应上面的表达式可以发现(规定 \\(y_0=start, y_{n+1} = stop\\) ) $$P(y|x) = exp(\\sum&#94;K_{k=1}\\sum&#94;{n+1}_{i=1}w_kf_k(y_{i-1},y_i,x,i) =\\prod_{i=1}&#94;{n+1}exp(\\sum_{k=1}&#94;Kw_kf_k(y_{i-1},y_i,x,i)=\\prod_{i=1}&#94;{n+1}M_i(y_{i-1},y_i|x)$$ 对于一个给定的序列，依此选取在每个矩阵中对应的元素就得到该路径的非规范概率。规范化因子则是这些矩阵的矩阵和，也对应着所有路径的非规范概率之和。 总结 ： 建立一个CRF，只需要定义一系列的特征函数，这些特征函数的值依赖于整个句子(x)，当前位置(i)和相邻的标签( \\(y_{i-1}\\) , \\(y_{i}\\) )，给出这些特征函数对应的权重，学习到这些权重值就得到了CRF的模型。 与逻辑回归和HMM的关系 逻辑回归 可以看出CRF和逻辑回归的概率表达式非常相似，因为它们都是属于log-linear模型，不同的是逻辑回归是属于分类也就是说y是一个值，而CRF的y是一个序列，CRF是一个序列版的逻辑回归。 那么如何将复杂的CRF转换为逻辑回归呢？ 考虑每个位置的特征函数 \\(f_i(x, i, y_{i-1}, y_{i})\\) 为 \\(x_i\\) ，然后其权重为 \\(w_i\\) ，这样就简化成了逻辑回归。 HMM CRF是根据定义的特征函数去求 \\(P(y|x)\\) 的得分，而HMM是用生成的方法先根据数据求 \\(P(y, x) = p(y_0)\\prod_i p(y_i|y_{i-1})p(x_i | y_i)\\) 。求对数可得： $$logp(y,x) = logp(y_0) + \\sum_i logp(y_i|y_{i-1}) + \\sum_i logp(x_i|y_i)$$ 这同样也是一个log-linear模型，将CRF中特征函数的权重等价于这些对数值就可以得到一个HMM模型： 对于HMM的转移概率 \\(p(y_i=b|y_{i-1}=a)\\) ， 定义CRF的转移特征为 \\(f(x, i, y_i, y_{i-1})=1\\) ，如果 \\(y_{i-1} = a, y_i = b\\) ，其权重为 \\(logp(y_i=b|y_{i-1}=a)\\) 对于HMM的发射概率 \\(p(x_i=b|y_i=a)\\) ，定义CRF的发射特征为 \\(f(x, i, y_i, y_{i-1}) = 1\\) 如果 \\(x_i=b, y_i=a\\) ，其权重为 \\(logp(x_i=b|y_i=a)\\) 这样就将CRF模型转换成了一个HMM模型。除此之外，CRF比HMM的更加强大： HMM的 \\(x_i\\) 只与当前的状态 \\(y_i\\) 有关，而CRF的整个序列y是和整个输入x相连的 HMM的发射概率 \\(p(x_i|y_i)\\) 是一个0到1的数，一个状态对所有发射值的概率和为1，因为是个概率值，而CRF中特征函数的权重则可以是任意值。这其实是因为 HMM的归一化发生在每一个位置，而CRF的归一化发生在整体序列上，对于中间的过程并不要求严格符合概率分布的形式。 3. CRF的概率计算 概率的计算是说在给定CRF模型P(Y|X), 输入序列x和输出序列y的情况下，计算条件概率 \\(P(Y_i = y_i|x)\\) 和 \\(P(Y_{i-1} = y_{i-1}, Y_i = y_i, x)\\) 以及相应的数学期望的问题。 前向后向算法 类似在HMM模型的情况，这里同样定义一套前向后向算法。 定义前向概率 \\(\\alpha_i(y_i | x)\\) 表示在位置i的标记是 \\(y_i\\) 并且到位置i的前部分标记序列的非规范化概率，定义后向概率 \\(\\beta_i(y_i|x)\\) 表示在位置i的标记是 \\(y_i\\) 并且从位置i+1到n的后部分标记序列的非规范化概率。 理解起来的话就是说，前向概率 \\(\\alpha_i(y_i|x)\\) 表示的是 \\(\\sum_{y_0, y_1, ..., y_{i-1}} P(y_0, y_1, .., y_{i-1}, y_i | x)\\) 对于除 \\(y_i\\) 以外的部分序列的所有情况的和。而后向概率 \\(\\beta_i(y_i|x)\\) 表示的是 \\(\\sum_{y_{n+1}, y_n, ..., y_{i+1}}P(y_{n+1}, y_n, ..., y_{i+1}, y_i|x)\\) 除 \\(y_i\\) 以外的部分序列的所有情况的和。 对于后向概率 \\(\\beta_i\\) ，这里与HMM的情形有点不同，在HMM中， \\(\\beta_i = P(o_{i+1}, o_{i+2},...|s_i)\\) 也就是说i时刻的状态是作为先验概率出现的，因为HMM的模型是概率有向图，状态是单向传递的 \\(P(y_{i+1}|y_i)\\) 。而CRF中是无向图，两者根据之前矩阵形式的推导，有类似联合概率的形式，才有了以下 \\(\\beta_i\\) 的递推公式，其中的 \\(M_{i+1}(y_i, y_{i+1}|x)\\) 既可以用于正向，也可以用于反向的推导。 对于 \\(\\alpha_0(y_0|x)\\) ，定义 \\(y_0 = start\\) 时， \\(\\alpha = 1\\) else, \\(\\alpha = 0\\) 其他位置 \\(\\alpha_i(y_i | x)\\) ，有 $$\\alpha_{i+1}(y_{i+1}|x) = \\alpha_i(y_i |x) * M_{i+1}(y_i, y_{i+1}|x)$$ 简化为 $$\\alpha_{i+1}&#94;T(x) = \\alpha_i&#94;T(x)M_{i+1}(x)$$ 由于每个位置的y有m个取值，所以这里得到的是个m维列向量。 对于 \\(\\beta_{n+1}(y_{n+1}|x)\\) ,定义 \\(y_{n+1} = stop\\) 时， \\(\\beta = 1\\) else, \\(\\beta = 0\\) 其他位置 \\(\\beta_i(y_i|x)\\) ,有 $$\\beta_i(y_i|x)=M_{i+1}(y_i, y_{i+1}|x)\\beta_{i+1}(y_{i+1}|x)$$ 简写为： $$\\beta_i(x) = M_{i+1}(x) * \\beta_{i+1}(x)$$ 规范化因子可以得出就是从start到stop的路径的非规范化概率和，相当于： $$Z(x) = \\alpha_n&#94;T(x) \\cdot \\vec{1} = \\vec{1}&#94;T \\cdot \\beta_1(x)$$ 稍微说一下个人理解，前向概率最终走到末尾节点 \\(\\alpha_n(x)\\) 的每一个分量m，对应了一个值 \\(\\alpha_n(y_n = m|x)\\) ，对各分量求和，得到整个序列的概率和。后向概率最终走到开始节点 \\(\\beta_1(x)\\) 的每一个分量m，对应了一个值 \\(y_1 = m\\) 时，从 \\(y_2\\) 到stop的这些路径的概率和，对各分量求和，同样得到整个序列的概率和。 概率计算 主要是两个概率 \\(P(Y_i = y_i|x)\\) 和 \\(P(Y_{i-1}=y_{i-1}, Y_i = y_i|x)\\) $$P(Y_i = y_i|x) = \\frac{\\alpha_i&#94;T(y_i|x)\\beta_i(y_i|x)}{Z(x)}$$ $$P(y_{i-1}, y_i |x) = \\frac{\\alpha_{i-1}&#94;T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\\beta_i(y_i|x)}{Z(x)}$$ 期望计算 有了概率的定义，很自然就可以给出特征函数对应的期望。 特征函数 \\(f_k\\) 关于条件分布 \\(P(Y|X)\\) 的数学期望是： $$E_{P(Y|X)}[f_k] = \\sum_yP(y|x)f_k(y,x) = \\sum_{i=1}&#94;{n+1}\\sum_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)*P(y_{i-1},y_i|x)$$ 假设经验分布为 \\(\\tilde{P}(X)\\) ,特征函数关于联合分布 \\(P(Y,X)\\) 的期望为： $$\\begin{aligned}E_{P(Y, X)}[f_k] &= \\sum_{x,y}P(x,y)\\sum_{i=1}&#94;{n+1}f_k(y_{i-1},y_i,x,i) \\\\\\\\&= \\sum_x\\tilde{P}(x)\\sum_yP(y|x)\\sum_{i=1}&#94;{n+1}f_k(y_{i-1},y_i,x,i)\\end{aligned}$$ 后面部分就是条件概率期望的部分。 注意到线性链CRF的特征函数是定义在 \\(y_{i-1}y_i\\) 的边上的，因此概率也拆开为对应边存在的概率 4. CRF的预测算法 CRF的预测算法就是在给定CRF模型P(Y|X)和观测序列x的情况下给出概率最大的输出序列，通常情况也就是对于观测序列进行标注。 用公式表示就是： $$y&#94;* = argmax_y P(Y|X) = argmax_y \\frac{exp(w\\cdot F(y, x))}{Z} = argmax_y (w\\cdot F(y, x))$$ 因为只是求概率最大的路径而不是求真实的概率值，所以这里只要求非规范化概率即可，提高了效率。 将其分解到每个位置就将问题转换为 $$max\\sum_i&#94;nw\\cdot F_i(y_{i-1}, y_i,x)$$ 其中 \\(w=(w_1, w_2, ..., w_k)\\) ，对应k个特征函数的权重， \\(F_i(y_{i-1}, y_i, x) = (f_1(y_{i-1}, y_i, x), f_2(y_{i-1}, y_i, x),...,f_k(y_{i-1}, y_i, x))\\) 对应每个特征函数在位置i上的值，两向量的乘积就表示从 \\(y_{i-1}\\) 到 \\(y_i\\) 上的\"概率\"。 注意上面对于整个路径的概率并没有求乘积，而是求和，这是因为 \\(w\\cdot F\\) 是从 \\(exp(w\\cdot F)\\) 中提取出来的，求和就等价于整体求乘积。 预测算法 对于一条路径，求出所有位置所有状态的组合，最后选择概率最大的，这种方法肯定是不现实的，效率非常低（ \\(O(n&#94;T)\\) ）。 对于这种求最优序列的问题，在每个位置有n个状态，总体概率可以拆解为相邻位置的状态转移概率，求整体最优的序列都可以使用维特比算法，也就是一种动态规划求最优解的方法。 简单描述： 初始位置t=0，计算每个状态 \\(y_0\\) 的概率，就得到n个概率值（假设有n个可能的状态） 在位置t=1，针对每个状态 \\(y_1\\) ，求出使得 \\(y_0, y_1\\) 序列概率最大的 \\(y_0\\) ,这样又得到n个概率（每个 \\(y_1\\) 对应一个 \\(y_0\\) ），记录下此时的n个 \\(y_0\\) 依次递推，每个时刻记录n个概率值，当到达最终点的时候，选出概率最大的状态进行回溯就得到最优路径 通过上面的方法，每个位置都只要计算 \\(n&#94;2\\) 次，时间复杂度为 \\(O(kn&#94;2)\\) 与之前 \\(O(n&#94;k)\\) 相比大为减少。 5. CRF的学习算法 CRF模型的学习就是学习每个特征函数的权重，通常情况下采用最大似然估计的方法进行学习。对于单样本的情况，给定x，y给出CRF的模型的条件概率： $$p(y|x) = \\frac{exp(\\sum_{k=1}&#94;K\\sum_{i=1}&#94;nw_kf_k(y_{i-1}, y_i, x, i)}{\\sum_{y'}exp(\\sum_{k=1}&#94;K\\sum_{i=1}&#94;nw_kf_k(y'_{i-1}, y'_i, x, i)}$$ 先进行log简化一下为： $$logp(y|x) = \\sum_{k=1}&#94;K\\sum_{i=1}&#94;nw_kf_k(y_{i-1}, y_i, x, i) - log\\sum_{y'}exp(\\sum_{k=1}&#94;K\\sum_{i=1}&#94;nw_kf_k(y'_{i-1}, y'_i, x, i)$$ 采用梯度下降法，对第j个特征函数的权重 \\(w_j\\) 求导： $$\\begin{aligned}\\frac{\\partial logp}{\\partial w_j} &= \\sum_{i=1}&#94;nw_jf_j(y_{i-1}, y_i, x, i) - \\frac{\\sum_{y'}exp(\\sum_{k=1}&#94;K\\sum_{i=1}&#94;nw_kf_k(y'_{i-1}, y'_i, x, i))\\sum_{i=1}&#94;nw_jf_j(y'_{i-1}, y'_i, x, i)}{\\sum_{y'}exp(\\sum_{k=1}&#94;K\\sum_{i=1}&#94;nw_kf_k(y'_{i-1}, y'_i, x, i))} \\\\\\\\ &=\\sum_{i=1}&#94;nf_j(y_{i-1}, y_i, x, i) - \\sum_{y'} p(y'|x) \\sum_{i=1}&#94;nf_j(y'_{i-1}, y'_i, x, i)\\end{aligned}$$ 有了梯度函数按照梯度负方向重复进行更新： $$w_j := w_j - \\alpha \\frac{\\partial logp}{\\partial w_j}$$ 就可以得到最后收敛的参数了。 参考资料 [1] 统计学习方法 - 李航 [2] Introduction to Conditional Random Fields - Edwin Chen if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"机器学习","url":"tiao-jian-sui-ji-chang-crf.html"},{"title":"指数分布族和广义线性回归","text":"指数分布族 1. 定义 指数分布族不是专指一种分布，而是一系列符合特征的分布的统称。常用的诸如正态分布，伯努利分布，指数分布，泊松分布，gamma分布都属于指数分布族。 $$p(y;\\theta) = b(y)exp(\\eta(\\theta)T(y) - A(\\theta))$$ 其中： b(y) - underlying measure T(y) - sufficient statistic A( \\(\\theta\\) ) - log normalizer 通常情况下T(y) = y, A, b, T, \\(\\eta\\) 给定的不同，就能得到不同的y的分布 其中的变量y和参数 \\(\\theta\\) 只在 \\(T(y)\\eta(\\theta)\\) 中有联系，T(y)和 \\(\\eta(\\theta)\\) 都是向量形式 2. 伯努利分布 伯努利分布的概率密度函数为： $$p(y;\\theta) = \\theta&#94;y(1 - \\theta)&#94;{1-y} = exp(ylog\\theta + (1-y)log(1-\\theta) = exp(log\\frac{\\theta}{1 - \\theta}y + log(1-\\theta))$$ 对应指数分布族的概率密度函数可以发现： \\(b(y) = 1\\) \\(\\eta(\\theta) = log\\frac{\\theta}{1 - \\theta}\\) \\(T(y) = y\\) \\(A(\\theta) = -log(1 - \\theta) = log(1 + e&#94;{\\eta(\\theta)})\\) 3. 高斯分布 对于均值为 \\(\\mu\\) ，方差为 \\(\\sigma\\) 的高斯分布的概率密度函数为： $$p(y;\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e&#94;{-\\frac{(y-\\mu)&#94;2}{2\\sigma&#94;2}} = \\frac{1}{\\sqrt{2\\pi}}e&#94;{\\eta(\\mu, \\sigma)T(y) - log\\sigma - \\frac{\\mu&#94;2}{2\\sigma&#94;2}}$$ 对应指数分布族的概率密度函数可以发现: \\(b(y) = \\frac{1}{\\sqrt{2\\pi}}\\) \\(\\eta(\\sigma) = [\\frac{\\mu}{\\sigma&#94;2}, -\\frac{1}{2\\sigma&#94;2}]\\) \\(T(y) = [y, y&#94;2]\\) \\(A(\\sigma) = \\frac{\\mu&#94;2}{2\\sigma&#94;2} + log\\sigma\\) 4. 其他指数分布 还有许多其他分布属于指数分布族，如： 多项式分布（multinomial），用来对多元分类问题进行建模； 泊松分布（Poisson），用来对计数过程进行建模，如网站的访客数量、商店的顾客数量等； 伽马分布（gamma）和指数分布（exponential），用来对时间间隔进行建模，如等车时间等； β分布（beta）和Dirichlet分布（Dirichlet），用于概率分布； Wishart分布（Wishart），用于协方差矩阵分布。 广义线性模型(GLM) 之前一直知道线性回归，逻辑回归都属于glm，其中线性回归假设服从高斯分布，逻辑回归假设服从伯努利分布，但是为什么要这样并不是非常清楚。 1. 三个假设 在给定自变量x和参数 \\(\\theta\\) 的情况下，因变量y服从指数分布族 给定x，最终目的是求出T(y)的期望E[T(y)|x] 自然参数 \\(\\eta\\) 可以表示为自变量x的线性关系，即 \\(\\eta = \\theta&#94;T x\\) 广义线性模型通过拟合y的条件均值/期望(在x和参数 \\(\\theta\\) 给定的情况下)，并假设y符合指数分布族中的某种分布，从而扩展了标准线性模型 2. 高斯分布 对于高斯分布，y的均值为参数 \\(\\mu\\) 根据上面的推导， \\(y = \\mu = \\eta = \\theta&#94;T x\\) (假设 \\(\\sigma = 1\\) ) 这就和线性回归对于y作高斯分布的假设相呼应，这里的link function是y=x为identity function 3. 伯努利分布 对于伯努利分布，y的均值为 \\(\\phi\\) ，就是指数分布族下的唯一参数 根据上面的推导， \\(\\eta = log\\frac{\\phi}{1 - \\phi} = \\theta&#94;T x\\) 推导出 \\(y = \\phi = \\frac{1}{1 + e&#94;{-\\eta}} = \\frac{1}{1 + e&#94;{-\\theta&#94;T x}}\\) 这也就是逻辑回归的表达式，对应与逻辑回归下y作伯努利分布的假设，此时的link function为 \\(y = log \\frac{x}{1 - x}\\) ，就是大名鼎鼎的logit函数了。 4. GLM建模过程 总结一下GLM的建模过程。 根据问题在指数分布族中选择一种分布作为对y的假设 计算该分布下的 \\(\\eta\\) ，实际上 \\(\\eta = \\eta(w&#94;T)\\) ，其中 \\(w&#94;T\\) 为该分布的真实参数，而 \\(\\eta\\) 只是以 \\(w&#94;T\\) 为参数的一个link function 计算该分布的期望，将其用 \\(\\eta\\) 表示，例如上面伯努利分布时的 \\(y=\\phi = \\frac{1}{1+e&#94;{-\\eta}}\\) 根据GLM的假设替换 \\(\\eta = \\theta&#94;T x\\) 即得到GLM模型 将这些知识都串联起来，就能更好的理解不同回归模型下的前提假设及其link function的选择了。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"机器学习","url":"zhi-shu-fen-bu-zu-he-yan-yi-xian-xing-hui-gui.html"},{"title":"《深度学习与神经网络》笔记6","text":"深度神经网络在可以模拟更加复杂的情形，但是在上一章中，我们发现训练深度神经网络的时候会出现梯度消失的问题，从而导致模型训练失败。这一章，将会介绍可以被用在深度学习上的一些技术。 这章的主要内容是介绍一种应用最广泛的深度神经网络：卷积神经网络。我们将会了解到卷积，池化等概念，通过在之前的代码上利用这些技术进行优化达到了惊人的99.67%的准确率。 除此之外，本章还将介绍一些其他的基本神经网络，例如循环神经网络。在介绍完这些之后，还会介绍深度学习技术的发展现状及未来的发展方向。 一. 卷积神经网络 之前我们在进行MNIST数字分类的时候，输入数据是将每张图片按像素展开得到的784维向量，这样训练得到的结果虽然不错，但是仔细想想就会发现它的问题。对于图片来说，不同的不同的像素点之间的距离很远，旧的方法就完全没有考虑像素点之间的空间联系。这一节介绍的卷积神经网络就考虑了这种空间联系，并且训练迅速，在图像分类问题上得到了非常好的效果。 卷积神经网络涉及到三个基本思想：local receptive fields(局部感受野)，shared weights(参数共享)，pooling(池化)，接下来依次介绍。 Local receptive fields: 在之前的神经网络中，输入神经网络的数据是一个多维向量，与输入层连接的隐藏层的每一个神经元都和所有的输入层神经元连接。 这样一副784个像素点的图片中的每一个像素点都是一个输入神经元，后一层的每个神经元和所有这些输入神经元都有连接。 local receptive fields局部感受野的概念就是后面一层的神经元只会和部分窗口下的输入神经元连接，例如，对于5*5的窗口： 我们可以将窗口从左到右，从上到下进行平移，每移动一次，当前窗口下的输入神经元就对应一个隐藏层的神经元。 对于28*28的图像，使用5*5的窗口的话，一共可以移动23*23次，也就是说隐藏层将会有24*24个神经元。 上面的例子中，每次平移的步长为1，实际中这个步长其实也是可以根据需要改变的。 Shared weights and biases: 这里的参数共享说的是对于和输入层连接的24*24个隐藏层神经元，每个隐藏层神经元的参数是一样的，也就是说不同窗口和其对应的隐藏层神经元共用一套参数。对于24*24个隐藏神经元中的第j, k个神经元其输出为： $$\\sigma(b+\\sum&#94;4_{l=0} \\sum&#94;4_{m=0}w_{l, m}a_{j+l, k+m}) \\quad (125)$$ 其中 \\(w_{l,m}\\) 对应着24*24个神经元中任意神经元与每个5*5窗口下的所有输入神经元连接的5*5个参数。这样就说明，第一个隐藏层中所有的神经元识别的是同一个特征，区别在于该特征出现在原图中的位置不同。基于这些原因，我们也把从输入层到隐藏层的这个映射关系称为特征映射，其中的共享权重和偏差作为决定了这个特征映射，通常也被成为一个核或滤波器。 上面我们在隐藏层中只用到了一种核，也就是只检测了一个特征，事实上我们也可以使用多个核来检测多个特征： 这个例子中就使用了3个5*5的核来检测三个不同的特征。实际中，可能使用更多的特征，比如我们随后的代码中就分别使用了20和40个核。 这个例子中的20幅图分别对应了20个不同的5*5核代表的权重，其中黑色方块代表高权重，白色方块代表低权重，这些图上的黑色区域就是该核所检测的特征。在这些特征图上，我们看到有很多自区域存在很明显的黑白分界，说明这里的确存在一些空间上的特征，至于这些特征是什么，我们并不是特别清楚，毕竟它不是非常规则的几何图形。 卷积的另外一个好处就是减少了需要学习的参数。考虑到最初的神经网络，在使用30个隐藏层神经元的情况下，一共有784*30（权重）+30（偏差）=23550个参数。而现在的话，5*5的核对应26个参数，如果使用20个特征，则有20*26=520个参数，减少了大概40倍。我们有理由相信这将大大减少训练的时间，使得深度网络的训练成为可能。 (125)式也是\"卷积\"这个名字的由来，卷积操作就不说了，其实和以前信号系统里学的卷积没什么区别，只是由一维变成了二维情形(图片)，并且由连续积分变成了离散求和。 池化层: 卷积神经网络的另外一个部分就是池化层，池化层的使用发生在卷积层之后，它的作用是对卷积层的输出结果进行简化。 对于上面中的24*24的卷积层的输出，池化层的每一个窗口单元，例如2*2的窗口，对卷积层结果进行处理。如果使用max-pooling的方法，就是说用这个2*2的窗口内的最大值作为这四个值的代表： 通过这样的方法，就将24*24的卷积层输出简化为了12*12的池化层输出。 池化的作用当然也可以应用在多个特征的情况下，例如下图中3个特征的情况： max-pooling可以看作是一种检测特征在上一层卷积层是否被检测到的方法。随后max-pooling将不再关心该特征在图像中的具体位置，而是处理特征之间的相对位置。通过这样的方法，除掉了很多不明显的特征，显著减小了后一层的参数。 除了max-pooling以外，还存在其他的池化方法。例如L2-pooling，对于2*2的窗口，它不再是求最大值，而是使用四个数的L2范数代替。虽然和max-pooling有点不一样，但是它们的目的都是为了压缩卷积层输出的信息。 总览: 将上面的卷积层，池化层和输入输出神经元结合起来就得到如下的一个简单的卷积神经网络： 虽然结构上和之前有点不一样，但是还是有很大的共同点的。它们都是有简单的单层相互连接起来的，单层由它们自己的参数决定，输入上一层的输出，将自己的输出又作为后一层的输入这样传递下去。 在后面的内容里，我们将会用随机梯度下降算法和反向传播算法对卷积神经网络进行训练。训练方法大致和之前一样，但是对反向传播过程需要进行一些修改。因为之前的反向传播针对的是全连接的神经网络，而卷积神经网络并是部分连接的。 拓展: 卷积神经网络下的反向传播： 之前的公式bp(1)-bp(4)给出的是全连接神经网络下的反向传播方程。对于由一个输出层，一个卷积层，一个max-pooling层，一个全连接输出层的卷积神经网络，给出该情形下的反向传播方程。 二. 卷积神经网络实践 在了解了卷积神经网络的核心概念之后，再来看看它在实际中的应用。这里的代码中调用了Theano库，一来可以快速的实现卷积神经网络的反向传播算法，另外相比于我们自己的实现神经网络，Theano的计算更快速，使得我们可以实现更复杂的模型。 先从一个简答的模型开始，只有一个包含100个神经元的隐藏层的神经网络： 1 2 3 4 5 6 7 8 9 10 11 import network3 from network3 import Network from network3 import ConvPoolLayer , FullyConnectedLayer , SoftmaxLayer training_data , validation_data , test_data = network3 . load_data_shared () mini_batch_size = 10 net = Network ([ FullyConnectedLayer ( n_in = 784 , n_out = 100 ), SoftmaxLayer ( n_in = 100 , n_out = 10 )], mini_batch_size ) net . SGD ( training_data , 60 , mini_batch_size , 0.1 , validation_data , test_data ) 这里在测试数据上得到了97.8%的准确率，为了避免过拟合的影响，这个准确率是在validation数据上达到最优准确率时对测试数据测得的。 和之前得到的最好的结果98.04%相比较，主要有两点不同。首先，之前的结果是在使用了正则化的基础上得到的，正则化的确可以改善结果但是并不是很大的改善，所以我们随后再考虑。其次，之前实用的是sigmoid输出层加交叉熵的组合，这里我们实用的是softmax加对数似然损失函数。这两者都可以加速神经网络的训练，后者在图像分类问题上应用的更加普遍。 接着我们试验更复杂的深度网络看看结果有没有改善。 我们在隐藏层之前添加一个卷积层，使用5*5的局部窗口，步长为1，共20个特征映射，随后使用2*2的池化窗口： 在这个结构中，卷积和池化层可以看作是学习输入图像中的局部空间的特征，随后的全连接的隐藏层的学习则是更抽象的层次，学习整个图像上的全局信息。 训练该模型： 1 2 3 4 5 6 7 8 net = Network ([ ConvPoolLayer ( image_shape = ( mini_batch_size , 1 , 28 , 28 ), filter_shape = ( 20 , 1 , 5 , 5 ), poolsize = ( 2 , 2 )), FullyConnectedLayer ( n_in = 20 * 12 * 12 , n_out = 100 ), SoftmaxLayer ( n_in = 100 , n_out = 10 )], mini_batch_size ) net . SGD ( training_data , 60 , mini_batch_size , 0.1 , validation_data , test_data ) 得到了98.78%准确率，比之前的任何结果都要好。 这个结果还能继续提升吗？我们在现有卷积层和全连接的隐藏层之间再加入一个卷积层，仍然使用5*5的局部接受野和2*2的池化窗口： 1 2 3 4 5 6 7 8 9 10 11 net = Network ([ ConvPoolLayer ( image_shape = ( mini_batch_size , 1 , 28 , 28 ), filter_shape = ( 20 , 1 , 5 , 5 ), poolsize = ( 2 , 2 )), ConvPoolLayer ( image_shape = ( mini_batch_size , 20 , 12 , 12 ), filter_shape = ( 40 , 20 , 5 , 5 ), poolsize = ( 2 , 2 )), FullyConnectedLayer ( n_in = 40 * 4 * 4 , n_out = 100 ), SoftmaxLayer ( n_in = 100 , n_out = 10 )], mini_batch_size ) net . SGD ( training_data , 60 , mini_batch_size , 0.1 , validation_data , test_data ) 这次我们得到了惊人的99.06%的正确率。 这里我们可能会有两个疑问。第一个问题是第二个卷积层表示了什么？第二个卷积层的输入是第一个卷积层输出的12*12图像，它的像素点表示了局部特征在原始图像中的出现。可以将其看作是原始图片的一个抽象压缩过的版本。由于它仍然存在一些局部空间特征，所以我们仍然可以使用第二个卷积层进行提取。 第二个问题就是既然是特殊的图像，第一个卷积层处理的原始图片只有一张，这里由于20个特征映射，也就是有20张12*12的图片，第二卷积层应该怎么处理呢？事实上，它对所有20幅图片都是可见的。也就是说，每个神经元处理20张图片中的同一块局部接受野。 使用RELU激活函数 上面的神经网络使用的都是sigmoid激活函数，我们接着试用一下RELU函数看看效果。作者还发现使用L2正则化会优化结果，于是我们使用 \\(\\lambda = 0.1\\) 的L2正则化: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from network3 import ReLU net = Network ([ ConvPoolLayer ( image_shape = ( mini_batch_size , 1 , 28 , 28 ), filter_shape = ( 20 , 1 , 5 , 5 ), poolsize = ( 2 , 2 ), activation_fn = ReLU ), ConvPoolLayer ( image_shape = ( mini_batch_size , 20 , 12 , 12 ), filter_shape = ( 40 , 20 , 5 , 5 ), poolsize = ( 2 , 2 ), activation_fn = ReLU ), FullyConnectedLayer ( n_in = 40 * 4 * 4 , n_out = 100 , activation_fn = ReLU ), SoftmaxLayer ( n_in = 100 , n_out = 10 )], mini_batch_size ) net . SGD ( training_data , 60 , mini_batch_size , 0.03 , validation_data , test_data , lmbda = 0.1 ) 得到了99.23%的准确率，相对于99.06%来说算是一个小的进步。不过，作者表示在他的所有试验中，RELU激活函数整体都优于sigmoid激活函数，因此RELU激活函数是更好的选择。 是什么使得RELU比sigmoid效果要好呢？很遗憾，目前我们并没有理论指导如何选择激活函数，都是一些经验上或启发式的结论，类似于我们之前讨论过，RELU在输入增大的情况下不会出现saturated的状态，它可以继续学习，所以结果要好于sigmoid函数。 扩展训练数据 另外一个提升训练效果的方法就是增大训练数据集了。最简单的方法就是我们对原图片进行平移，分别向上下左右四个方向平移得到四张图片，于是原始的50000张的训练数据集就增大到了250000张。接着使用之前一样的使用RELU激活函数的结构，得到99.37%的准确率，又提升了一点。当然这个方法还有改进空间，可以通过对图像进行其他的变换更进一步扩大训练数据集。 增加全连接隐藏层 我们先试着增大之前唯一的一个全连接隐藏层的规模，分别测试了300个和1000个神经元的情况，得到了99.46%和99.43%的结果，相对99.37%来说并不是什么大的提升。 那么如果是增加一层隐藏层呢，仍然是100个神经元的隐藏层： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 net = Network ([ ConvPoolLayer ( image_shape = ( mini_batch_size , 1 , 28 , 28 ), filter_shape = ( 20 , 1 , 5 , 5 ), poolsize = ( 2 , 2 ), activation_fn = ReLU ), ConvPoolLayer ( image_shape = ( mini_batch_size , 20 , 12 , 12 ), filter_shape = ( 40 , 20 , 5 , 5 ), poolsize = ( 2 , 2 ), activation_fn = ReLU ), FullyConnectedLayer ( n_in = 40 * 4 * 4 , n_out = 100 , activation_fn = ReLU ), FullyConnectedLayer ( n_in = 100 , n_out = 100 , activation_fn = ReLU ), SoftmaxLayer ( n_in = 100 , n_out = 10 )], mini_batch_size ) net . SGD ( expanded_training_data , 60 , mini_batch_size , 0.03 , validation_data , test_data , lmbda = 0.1 ) 这里的结果是99.43%，同样没有什么明显的提升。试验300和1000的双层的情况，得到99.48%和99.47%，有效果，但是并不显著。 这是为什么呢？是添加隐藏层真的没有效果还是我们的学习方式出错了呢？我们用之前介绍的dropout的方法尝试减轻过拟合的影响： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 net = Network ([ ConvPoolLayer ( image_shape = ( mini_batch_size , 1 , 28 , 28 ), filter_shape = ( 20 , 1 , 5 , 5 ), poolsize = ( 2 , 2 ), activation_fn = ReLU ), ConvPoolLayer ( image_shape = ( mini_batch_size , 20 , 12 , 12 ), filter_shape = ( 40 , 20 , 5 , 5 ), poolsize = ( 2 , 2 ), activation_fn = ReLU ), FullyConnectedLayer ( n_in = 40 * 4 * 4 , n_out = 1000 , activation_fn = ReLU , p_dropout = 0.5 ), FullyConnectedLayer ( n_in = 1000 , n_out = 1000 , activation_fn = ReLU , p_dropout = 0.5 ), SoftmaxLayer ( n_in = 1000 , n_out = 10 , p_dropout = 0.5 )], mini_batch_size ) net . SGD ( expanded_training_data , 40 , mini_batch_size , 0.03 , validation_data , test_data ) 这次的结果是99.60%，这次总算是得到了一个不错的提升了。 这里有两点参数的变化需要说一下。首先是epoch从之前的60改为了40，因为dropout减轻了过拟合，学习过程就缩短了。其次是全连接的隐藏层都使用了1000个神经元，因为dropout在训练过程中会抛弃掉很多神经元，所以适当的增加其规模是有必要的。事实上我们同样测试了300个情形，发现1000个的时候的确效果要更好。 神经网络的集成 由于初始状态的随机性，一个很简单的想法是用上面的方法训练5个不一样的神经网络。其中每个单独的准确率都在96%左右，然后用这5个模型的结果进行投票，少数服从多数，判断一张图片最终的分类。 这听起来有点简单的不可思议，但是确是神经网络或机器学习上经常使用的方法。在这里也的确获得了提升，得到了99.67%的准确率。也就是说在10000张图片中，我们只识别错了33张，来看一下这33张的结果： 可以看到这些都是非常潦草的手写体，即便是正常的人也不一定能全部识别准确，更别说是机器了。要知道我们的机器已经识别准确9967张图片了，已经非常接近正常人的识别水平了。 为什么只在全连接隐藏层上使用dropout 细心的话可以发现之前的dropout只发生在全连接层上，为什么不对卷积层使用dropout呢？事实上可以这样做，但是没必要。因为卷积层的设计天生就可以防止过拟合。因为共享参数的设置，导致它更多的是从整张图片上去学习规律，而不是仅仅着眼于一些局部的细节。所以对于卷积层，没有必要去使用一些正则化的方法。 为什么可以训练深度网络 之前我们讨论了在神经网络层增加的时候会出现梯度不稳定的问题，我们现在为什么又能训练了呢？事实上，我们并没有避免这个问题，我们只是使用了一些方法来帮助处理：1. 使用卷积神经网络大大减少了需要训练的参数，使得训练更容易 2. 使用了更强大的正则化技术(尤其是dropout)减轻了过拟合 3. 使用了RELU激活函数加速训练 4. 使用GPU加速训练。 除此之外，我们还使用了其它的一些技术：拓展训练数据集(避免过拟合)；使用正确的损失函数(避免训练速度下降)；使用正确的参数初始化方法(避免由于神经元saturation导致的训练速度下降)。 这些技术虽然都是简单的基础技术，但是只要应用得到，将它们合理组合起来，就可以得到强大效果。 写在最后 这系列的笔记到此就结束了，由于在进行这一块学习的时候对神经网络可以说是零基础，出现错误在所难免。 以后等自己对深度学习更加理解之后再回来进行修改把。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"深度学习","url":"shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-6.html"},{"title":"信息论基本概念","text":"最近在看《统计自然语言处理》，觉得第二章预备知识里的关于信息论的一些基本概念总结得很不错。虽然对于熵这个词，我接触过很多次，在机器学习里的很多地方也都有涉及到，比如说最大熵模型，决策树训练时的互信息等等。但是有的时候我还是会经常搞混淆，这里简单介绍一下常用的概念。 一. 熵 对于离散变量 \\(X\\) , 假设其取值空间为 \\(R\\) ，其概率分布为 \\(p(x) = P(X = x), x \\in R\\) ，那么定义随机变量 \\(X\\) 的熵为： $$H(X) = - \\sum_{x \\in R} p(x)log_x (p(x))$$ 约定 \\(0log(0) = 0\\) 。由于这里使用了2为底，所以该公式定义的熵的单位为二进制单位(比特)，实际情况也有使用其它底数的版本的熵。 熵又被成为自信息(self-information)，可以将其描述为一个随机变量不稳定的程度。通过简单的数学计算可以证明当随机变量 \\(X\\) 在其取值空间上对所有值等概率的情况下，熵达到最大值，也就是说随机变量随机性越强，它的熵越大。如果在某一个值上取值概率为1，也就是说这个随机变量其实并不随机，是一个定值，这个时候的熵达到最小值0，它毫无随机性。 熵还可以表示信源 \\(X\\) 每发出一个符号所提供的平均信息量。熵越大，越难猜测变量正确的值，因此给予的信息就越多。 二. 联合熵和条件熵 有了单变量的情况，很自然就想到多变量下联合概率和条件概率的情况。 对于一堆随机变量 \\(X, Y\\) ，其联合概率为 \\(p(x, y)\\) ，则其联合熵为： $$H(X, Y) = - \\sum_{x\\in X} \\sum_{y \\in Y} log_2(p(x, y))$$ 联合熵实际熵就是描述一对随机变量平均所需要的信息量。 条件熵就是给定条件概率 \\(p(Y|X)\\) 的情况下： $$\\begin{aligned} H(Y|X) &= \\sum_{x\\in X} p(x)H(Y|X = x) = \\sum_{x \\in X}\\sum_{y\\in Y}p(x)p(y|x)log(p(y|x) \\\\\\\\ &= \\sum_{x\\in X}\\sum_{y \\in Y}p(x)\\frac{p(x,y)}{p(x)}log(\\frac{p(x,y)}{p(x)}) \\\\\\\\ &= \\sum_{x\\in X}\\sum_{y \\in Y}p(x,y)log(p(x,y) - p(x,y)logp(x) \\\\\\\\ &= \\sum_{x\\in X}\\sum_{y \\in Y}p(x, y)log(p(x,y) - \\sum_{x\\in X}p(x)log(p(x)) \\\\\\\\ &= H(X, Y) - H(X)\\end{aligned}$$ 这个式子也被称为熵的连锁规则，推广到一般情况有： $$H(X_1, X_2, ..., X_n) = H(X_1) + H(X_2|X_1) + H(X_3|X_2, X_2) + ... + H(X_n|X_{n-1}, X_{n-2},...,X_1)$$ 条件熵可以看作是在受变量 \\(X\\) 影响的情况下，变量 \\(Y\\) 的不稳定程度。 对于来自于同一个分布 \\(X\\) 的一个随机变量序列 \\((X_1, X_2, ..., X_n)\\) ，用 \\(X_{1n}\\) 表示。当我们求这个序列的熵的时候，我们可以将其表示为 \\(n\\) 个同样的随机分布 \\(X\\) 的联合熵，为 \\(-\\sum_{x_{1n}} p(x_{1n})log(p(x_{1n}))\\) 。 于是，对于一条长度为n的信息，每一个字符或字的熵为： $$H_{rate} = \\frac{1}{n}H(X_{1n}) = -\\frac{1}{n}\\sum_{x_{1n}} p(x_{1n})log(p(x_{1n}))$$ 这个数值被称为熵率。 对于语言模型来说，如果假定一种语言是由一系列符号组成的随机过程 \\(L=(X_i)\\) ，例如，某报纸的一批预料，那么，可以定义这种语言L的熵率作为其随机过程的熵率： $$H_{rate} = lim_{n \\rightarrow \\infty}\\frac{1}{n}H(X_{1n})$$ 三. 互信息 根据上面的链式法则得到： $$H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$$ 于是有: $$H(X) - H(X|Y) = H(Y) - H(Y|X)$$ 这个差值被称为变量 \\(X\\) 和 \\(Y\\) 之间的互信息，计作 \\(I(X; Y)\\) 。它反映了在知道了 \\(Y\\) 的值以后， \\(X\\) 的不确定性的减少量，同时也是在知道了 \\(X\\) 的值以后， \\(Y\\) 的不确定性的减少量。可以理解为 \\(Y\\) 的值透露了多少关于 \\(X\\) 的信息量。 将其展开: $$\\begin{aligned}I(X;Y) &= H(X)-H(X|Y) = H(X) + H(Y) - H(X, Y) \\\\\\\\ &= -\\sum_x p(x)log(p(x)) - \\sum_y p(y)log(p(y)) + \\sum_{x, y} p(x, y)log(p(x, y)) \\\\\\\\ &= \\sum_{x, y}p(x,y)log\\frac{p(x, y)}{p(x)p(y)}\\end{aligned}$$ 从这个式子可以看出 \\(I(X;X) = H(X) - H(X|X) = H(X)\\) ，这也就是把熵称为自信息的原因。另一方面可以看出，如果 \\(I(X;Y)>>0\\) ，则表明 \\(X\\) 和 \\(Y\\) 是高度相关的。如果 \\(I(X;Y) = 0\\) ，即 \\(p(x,y)=p(x)p(y)\\) 则说明两者完全独立。如果 \\(I(X;Y)<<0\\) ，则表明 \\(Y\\) 的出现不但未使 \\(X\\) 的不确定性降低，反而加大了其不确定性，这通常是不利的。 同样可以推导条件互信息。 条件互信息 $$\\begin{aligned}I(X;Y|Z) &= I((X;Y | Z)) = H(X|Z) - H(X|Y,Z) \\\\\\\\ &= H(X) - I(X;Z) - (H(X) - I(X;Y,Z)) \\\\\\\\ &= I(X;Y,Z) - I(X;Z)\\end{aligned}$$ 四. 相对熵 相对熵又被称为KL距离，是衡量相同事件空间里两个概率分布相对差距的测度。两个概率分布 \\(p(x)\\) 和 \\(q(x)\\) 的相对熵定义为： $$D(p||q) = \\sum_{x\\in X}p(x)log\\frac{p(x)}{q(x)} = E_p(log\\frac{p(x)}{q(x)})$$ 显然，当两个随机分布完全相同，即 \\(p=q\\) 时，相对熵为0，当其差别增加时，其相对熵的期望值也增大。 之前证明了 $$I(X;Y) = \\sum_{x, y}p(x,y)log\\frac{p(x, y)}{p(x)p(y)} = D(p(x,y)||p(x)p(y)$$ 于是知道互信息就是衡量一个联合分布与独立性差距多大的测度。 五. 交叉熵 根据前面的定义，知道熵就是一个不确定性的测度。对于某件事情，我们知道的越多，熵就越小，因而我们对于试验的结果就越不感到意外。交叉熵的概念就是用来衡量估计模型与真是概率分布之间差异情况的。 如果一个随机变量 \\(X\\sim p(x)\\) ， \\(q\\) 是我们计算得到的模型， \\(q(x)\\) 是模型 \\(q\\) 对于真实分布 \\(p(x)\\) 的近似表示。那么随机变量 \\(X\\) 和模型 \\(q\\) 之间的交叉熵定义为： $$\\begin{aligned}H(X, q) = H(X) + D(p || q) &= -\\sum_{x\\in X} p(x)log(p(x)) + \\sum_{x\\in X} p(x)log\\frac{p(x)}{q(x)} \\\\\\\\ &= -\\sum_{x\\in X}p(x)log(q(x)) = E_p(log\\frac{1}{q(x)})\\end{aligned}$$ 这里联想到之前在介绍神经元交叉熵损失的时候给出了这样一个定义 \\(yln(a) + (1-y)ln(a)\\) ，可以知道这里的 \\(y\\) 并不是对应说真实的标签，而是对于该神经元的两种状态0或1，当其真实值为1时，即 \\(p(x = 1) = 1\\) ,损失为 \\(log(q(x=1)\\) ，当其真实值为0时，即 \\(p(x=0) = 1\\) ,损失就是 \\(log(1-q(x=1) = log(q(x=0))\\) ，和我们这里交叉熵的定义完全符合。 注意到这里交叉熵写作 \\(H(X, q)\\) ，这似乎就是联合熵的形式，这样表示不会引起误会吗？事实上，交叉熵可以看作是一种特殊场景下的联合熵，它是衡量一个变量 \\(X\\) ，和我们对其的近似表示 \\(q(x)\\) 的联合熵。如何这个近似非常完美，即 \\(q(x)\\) 就是 \\(X\\) 的真实分布 \\(p(x)\\) ，那么 \\(D(p||q) = 0, H(X, q) = H(X, X)\\) ，就是自身的联合熵了。 接着我们定义一个语言 \\(L = (X) \\sim p(x)\\) 与我们构建的语言模型 \\(q\\) 的交叉熵为： $$H(L, q) = -lim_{n\\rightarrow \\infty}\\frac{1}{n} \\sum_{x&#94;n_1} p(x&#94;n_1)log(q(x&#94;n_1))$$ 其中， \\(x&#94;n_1 = x_1, x_2, .. ,x_n\\) 为语言 \\(L\\) 的词序列样本，这里的词包括样本中出现的任意词汇、数字、标点等。我们假设这种语言是\"理想\"的，于是有n趋于无穷大时，有全部\"词汇\"的概率和为1，根据信息论的定理，假定语言L是稳态遍历的随机过程，就可以得到： $$H(L, q) = -lim_{n\\rightarrow \\infty}\\frac{1}{n} log(q(x&#94;n_1))$$ 就是说可以用样本的熵表示整个语言的熵。 在实际情况下，当我们选择的样本量n足够大的时候，可以将上式子近似表示为 \\(-\\frac{1}{N}log(q(x&#94;N_1)\\) ，交叉熵越小，表示我们的模型越接近真实的语言模型，效果越好。 六. 困惑度 在设计语言模型的时候，我们通常并不使用交叉熵而是使用困惑度(perplexity)来表示。给定语言L的样本 \\(l&#94;n_1 = l_1...l_n\\) ,L的困惑度PP_q为： $$pp_q = 2&#94;{H(L,q)} \\approx 2&#94;{-\\frac{1}{n}log(q(l&#94;n_1)} = [q(l&#94;n_1)]&#94;{-\\frac{1}{n}}$$ 于是语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实语言的情况。 从perplexity的计算式可以看出来，它是对于样本句子出现的概率，在句子长度上Normalize一下的结果。它越小，说明出现概率越大，所得模型就越好。 七. 模拟信道模型 在学通信原理的时候学习过信道的概念，一个信号经过一个信道，会由于压缩编码，噪声引入，然后在解码的时候就会多少有一点失真。 在自然语言处理中，很多问题也都可以归结为这样的模型。给定输出 \\(O\\) (可能含有误传信息)的情况下，如何从所有可能的输入 \\(I\\) 中选出最可能的那个： $$\\hat{I} = argmax_I p(I|O) = argmax_I \\frac{P(I)p(O|I)}{p(O)} = argmax_I p(I)p(O|I)$$ 其中 \\(p(I)\\) 成为语言模型，是指在输入语言中\"词\"序列的概率分布；另一个 \\(p(O|I)\\) 成为信道概率。 对应到实际的NLP问题，比如说机器翻译在进行汉译英的时候，汉语句子看作是信道输出O，求出最可能的信道输入英语句子I。 噪声信道模型在NLP中有非常多的用途，除了机器翻译以外，还用于词性标注、语音识别、文字识别等很多问题的研究。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"NLP","url":"xin-xi-lun-ji-ben-gai-nian.html"},{"title":"《深度学习与神经网络》笔记5","text":"之前的章节，我们利用一个仅包含一层隐藏层的简单神经网络就在MNIST识别问题上获得了98%左右的准确率。我们于是本能会想到用更多的隐藏层，构建更复杂的神经网络将会为我们带来更好的结果。 就如同在进行图像模式识别的时候，第一层的神经层可以学到边缘特征，第二层的可以学到更复杂的图形特征，例如三角形，长方形等，第三层又会识别更加复杂的图案。这样看来，多层的结构就会带来更强大的模型，进行更复杂的识别。 那么在这一章，就试着训练这样的神经网络来看看对结果有没有什么提升。不过我们发现，训练的过程将会出现问题，我们的神经网络的效果并没有什么提升。 为什么会出现这样的情况呢，这一章就是主要围绕着这个问题展开的。我们将会发现，不同层的学习速率是不一样的。例如，在后面的网络层训练正在顺利学习的时候，前面网络层的学习却卡住几乎不动了。而且我们会发现这并不是偶然的，而是在理论上由梯度下降算法导致的。随着我们对问题的深入了解，我们会发现相反的情况也是可能发生的，就是前面网络层学习正常，而后面网络层学习停止。 这虽然看上去都是坏消息，不过深入探索这些问题也是帮助我们设计更好的更高效的深度神经网络的训练方法。 一. 梯度消失问题 先回到之前的程序上，当我们选择一个隐藏层的时候得到准确率为96.48%。接着增加一个隐藏层得到96.90%的结果。看上去结果不错，毕竟提升了。接着再加上一个隐藏层，却只得到了96.57%的结果。这个结果虽说下降了没多少，但是我们模型变复杂了，我们期望得到一个更好的结果，但是却事与愿违了。 这个结果看上去是奇怪的，而外的隐藏层理应使得模型可以处理更复杂的分类函数，不说结果提升多少，但是至少不能下降吧。为了搞清楚这期间到底是出了什么问题，我们回到两个隐藏层的情况，下面的图中，神经元上的柱形的长度表现的是其参数的更新速率，是当参数初始化完成后得到的结果： 大致看上去，第二层整体的更新速率要比第一层的快很多。但是由于权重的初始化也是随机的，我们很难判断这是不是一种巧合。 为了验证这是巧合还是事实，我们先定义 \\(\\delta &#94;l_j = \\frac{\\partial C}{\\partial b&#94;l_j}\\) ，然后 \\(\\delta &#94;l\\) 可以看作是一个向量，其中每个分量表示第 \\(l\\) 层中该神经元上参数更新的速率。于是就可以将 \\(||\\delta&#94;l||\\) 看作是 \\(l\\) 层整体的学习速率，利用该速率的大小就可以比较不同层学习速率间的差别。 根据这些定义，我们发现 \\(||\\delta &#94;1 = 0.07||\\) 和 \\(||\\delta &#94;2 = 0.31||\\) ，这的确印证了一开始观察到的结果，第二层整体比第一层快。 三层隐藏层的时候呢？结果为0.012, 0.060和0.283，也是一样的结果：后面的层比前面的层快。四层的时候为0.003，0.017，0.070和0.285，也是一样。 我们已经验证了参数刚初始完时的情形，也就是训练刚刚开始的情形，那么随着训练的进行，它们之间速率会发生什么变化呢？ 先看两层的情形： 可以看到两者的速率差别，第一层的速率一直比第二层要慢得多。接着看一下三层和四层的情况： 也是一样的结果，速率都是前面的层要慢于后面的层。 我们于是可以得到一个重要的观察现象：在某些神经网络中，通过隐藏层从后向前看，梯度会变的越来越小。这也意味着，前面层的学习会显著慢于后面层的学习。这就是梯度消失问题。 那么是什么导致了梯度消失呢？是否可以避免这样的问题呢？事实上，的确存在替代方案，但是会导致另外一个问题：前面层的梯度会变的很大而不是消失。这就是梯度爆炸问题。也就是说深度神经网络上的梯度要么倾向于爆炸要么倾向于消失，这都是不稳定的。而这个不稳定性也是基于梯度的学习算法都要面临的一个基本问题。 不过我们也许会有疑问，为什么梯度消失就是问题，梯度是不是说明学习已经够了，这个神经元的参数已经被正确学习到了呢？ 事实当然不是这样的，我们一开始初始化产生的参数肯定不可能那么巧合是最优的参数。然而从三层隐藏层的那个例子看到，随机初始化意味着第一层会错过很多的重要信息，即使后面的层训练的再好，也很难识别输入图像。并不是第一层已经训练好了，而是它们无法得到足够的训练。如果我们想要训练这样的神经网络，就必须解决梯度消失问题。 二. 是什么导致了梯度消失问题？ 看一个简单的例子，一个每层只有一个神经元的神经网络： 不过注意到这里的 \\(C\\) 其实表示的是损失函数，其输出分别为： \\(a_1,a_2,a_3,a_4\\) 。 根据求导的链式法则有： 为什么会发生梯度消失： 其实看到这样一个式子： $$\\frac{\\partial C}{\\partial b_1} = \\sigma '(z_1)w_2 \\sigma '(z_2)w_3 \\sigma '(z_3)w_4 \\sigma '(z_4)\\frac{\\partial C}{\\partial a_4} \\quad (122)$$ 如果还记得前面章节神经元saturated发生的原因的话也能知道这里究竟是什么导致了梯度消失。 注意到其间有一系列的 \\(w_j \\sigma '(z_j)\\) 项，先看一下sigmoid函数的导数图像： 最大值也才0.25，然后由于参数的初始化使用 \\(G(0, 1)\\) 的高斯分布，常常会导致 \\(|w_j| < 1\\) ，这样就会导致 \\(w_j \\sigma '(z_j)<\\frac{1}{4}\\) 。然后一系列这些小值的积也会变得更小。 当然这并不是一个严格的数学证明，我们很容易就会举出很多反例。比如在训练过程中权重 \\(w_j\\) 是可能变大的，如果大到使得 \\(|w_j\\sigma '(z_j)<\\frac{1}{4}|\\) 不再满足，或者说大于1，梯度就不会消失了，它将指数增长，从而导致另外一个问题：梯度爆炸问题。 梯度爆炸问题： 再来看一个梯度爆炸发生的例子。我们选择大的权重： \\(w_1=w_2=w_3=w_4=100\\) 。然后选择偏差使得 \\(\\sigma '(z_j)\\) 不太小。 这个并不难做到，例如我们可以选择使得 \\(z_j=0\\) 时的bias，于是得到 \\(w_j \\sigma '(z_j) = 100*0.25 = 25\\) ，这样就会导致梯度爆炸了。 梯度的不稳定性问题： 经过这些讨论我们就会发现，梯度消失也好，梯度爆炸也好，归根结底是由于层数的增加，多个项相乘，势必就会导致不稳定的情况。除非这些积能恰到好处的相等，才可以让不同层之间的学习速率相近。不过实际上，这几乎是不可能发生的。总之，只要我们使用基于梯度的学习算法，不同层的学习速率势必是有很大差距的。 练习： 问题： 之前我们基于 \\(\\sigma '(z) <\\frac{1}{4}\\) 的事实讨论了梯度消失的问题，那么是否可以使用另外一个激活函数，使得其导数足够大，来帮助我们解决梯度不稳定的问题呢？ 答案： 这个当然是不可以的，不管一开始的值是怎么样的，因为多个项相乘，这就会导致积是指数增长或者指数下降，可能浅层时不明显，但是随着层数的增加，都会导致这个问题出现。 梯度消失问题很难消除的原因： 之前已经发现了在深层网络的前几层会出现梯度或者消失或者爆炸的问题。事实上，当使用sigmoid函数作为激活函数的时候，梯度消失几乎是总会出现的。考虑到避免梯度消失，要满足条件 \\(|w\\sigma '(z)| \\geqslant 1\\) 。我们也许会觉得这还不简单，只要 \\(w\\) 大不就可以了，但是注意到 \\(w\\) 大，也会导致 \\(z=wx+b\\) 也大，然后 \\(\\sigma '(z)\\) 就会很小。唯一的方法就是还需要保证 \\(x\\) 只出现在一个很小的范围内，这在实际情况下显然是很难发生的。所以说就总是会导致梯度消失的问题。 拓展： 拓展一： 考虑 \\(|w\\sigma '(wa+b)|\\) ，假设 \\(|w\\sigma '(wa+b)| \\geqslant 1\\) (1) 证明这只可能在 \\(|w|\\geqslant 4\\) 时成立 之前已经知道 \\(|\\sigma '(z)|\\) 在0处取最大值0.25，所以 \\(|w\\sigma '(wa+b)| \\geqslant 1\\) 成立的话势必需要 \\(|w| \\geqslant 1/0.25 = 5\\) 。 (2)假设 \\(|w|\\geqslant 4\\) ，考虑到 \\(|w\\sigma '(wa+b)| \\geqslant 1\\) ，证明此时的 \\(a\\) 的变化区间被限制在宽度 $$\\frac{2}{|w|}ln (\\frac{|w| (1 + \\sqrt{1-4/|w|})}{2} -1)$$ 内 这个就是纯数学问题解方程了，利用一元二次方程的求根公式可以求得 \\(e&#94;{-z}_{max}\\) 和 \\(e&#94;{-z}_{min}\\) ，然后求对数后相减，稍微变换一下形式就可以得到这个结果。 (3)证明上面的范围的最大值大约为0.45，在 \\(|w| \\approx 6.9\\) 处得到。于是可以看到即使所有这些条件都满足，激活函数的符合要求的输入范围还是非常窄，还是很难避免梯度消失的问题。 求导算导数为0的点求得。 拓展二：identity神经元 考虑一个单输入 \\(x\\) 的神经元，中间层参数为 \\(w_1\\) 和 \\(b\\) ，然后输出层参数为 \\(w_2\\) ，证明通过选择适当的权重和偏差，可以使得 \\(w_2 \\sigma (w_1 x + b) \\approx x\\) 对任意 \\(x\\in [0, 1]\\) 成立。这个神经元可以被认为是一种identity神经元，其输出和输入一样（只差一个权重因子的放缩）。提示：可以将 \\(x\\) 写为 \\(x=\\frac{1}{2} + \\Delta\\) , 假设 \\(w_1\\) 很小，使用 \\(w_1 \\Delta\\) 处的泰勒展开。 之前讨论sigmoid函数形状的时候知道，当 \\(|w_1|\\) 增大的时候，函数会变得越来越窄，逼近解约函数。当 \\(|w_1|\\) 非常小的时候，函数越来越宽，在某个区间内会逼近线性函数，但是既然是sigmoid函数，当 \\(x \\rightarrow \\infty\\) 时，函数都是会趋向于1或0的。 这里的证明我没有用泰勒展开，我想的是既然要证明该函数在某个区间的线性，只要证明它导数在该区间趋近于常数即可。 求 \\(\\sigma (w_1 x+b)\\) 的导数为 \\(\\sigma '(w_1 x +b) = \\frac{w_1 e&#94;{-(w_1 x + b)}}{(1+e&#94;{-(w_1 x + b)})&#94;2} = \\frac{w_1 e&#94;{w_1 x + b}}{(1+e&#94;{w_1 x + b})&#94;2}\\) 。 不妨令 \\(x = \\frac{1}{2} + \\Delta\\) 则上式变为： \\(\\frac{w_1 e&#94;{w_1 \\Delta}}{1+2e&#94;{w_1 \\Delta} + e&#94;{2w_1 \\Delta}}\\) ，由于 \\(\\Delta = x - \\frac{1}{2} \\in [-\\frac{1}{2}, \\frac{1}{2}]\\) , 而 \\(w_1\\) 是很小的数，于是可将上式展开为 \\(\\frac{w_1 (1+ w_1 \\Delta)}{1 + 2(1+w_1 \\Delta) + 1 + 2w_1 \\Delta} = \\frac{w_1 (1+ w_1 \\Delta)}{4(1+ w_1 \\Delta)} = \\frac{w_1}{4}\\) 为常数，通过适当调整 \\(w_2\\) 就可以使其输出恰好为 \\(x\\) 。 三. 复杂神经网络中的梯度不稳定问题 前面是在一个简单的例子下讨论的梯度不稳定问题，对于如下这个复杂的情况： 根据之前反向传播的知识，我们可以得到： $$\\delta &#94;l = \\sum '(z&#94;l)(w&#94;{l+1})&#94;T \\sum '(z&#94;{l+1})(w&#94;{l+2})&#94;T...\\sum '(z&#94;L) \\bigtriangledown_a C$$ 其中 \\(\\sum '(z&#94;l)\\) 为一个对角矩阵，其中每个成员为 \\(\\sigma '(z)\\) 为第l层的各个加权输入。 \\(w&#94;l\\) 是不同层的权重矩阵， \\(\\bigtriangledown_a C\\) 为损失函数C对输出层的输出的偏导数。 这个式子看上去比之前复杂的多，但是其实还是能看到其中很多个 \\((w&#94;j)&#94;T\\sum '(z&#94;j)\\) 进行连乘，还是会出现梯度不稳定的问题。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"深度学习","url":"shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-5.html"},{"title":"《深度学习与神经网络》笔记4","text":"神经网络最令人激动的一个性质，就是它可以实现任意功能的函数。而且是即使对于只有一个隐藏层的神经网络，这个结论依然成立。 大部分神经网络的使用者都知道这个性质，但是并不理解为什么神经网络会有这样的性质。而其理论证明对于非数学专业的同学来说并不好理解，所以本章旨在用直观的方式帮助大家理解这个性质。 一. 两个前提 神经网络可以计算任意函数其实是有前提的。 首先要明白的是它并不是可以完全准确的计算原函数的值，但是通过增加隐藏层神经元的值我们可以越来越逼近原函数。就是说对于一个需要实现的函数 \\(f(x)\\) ，要求实现精度为 \\(\\epsilon > 0\\) ，也就是需要足够的隐藏层神经元使得神经网络输出 \\(g(x)\\) 满足 \\(|g(x) - f(x)| < \\epsilon\\) 对所有输入 \\(x\\) 。 第二个前提是被模拟的函数是连续函数，不过有的时候对于非连续函数，神经网络得到的连续近似已经足够满足要求了。 二. 单输入单输出的情况 先考虑最基础的单输入单输出的函数情况。为了理解怎么利用神经网络去计算给定函数 \\(f\\) ，我们先考虑只有一个隐藏层的情况，其中含有两个神经元。 考虑隐藏层第一个神经元，其输出由 \\(\\sigma (wx+b)\\) 决定。改变其参数 \\(w\\) 和 \\(b\\) 可以发现如下规律： 改变b不会使函数形状发生改变，只会使其左右偏移。因为 \\(f(x) = \\frac{1}{1+e&#94;{wx+b+b_0}} = \\frac{1}{e&#94;{w(x+\\frac{b_0}{w}) + b}}\\) ， \\(b_0>0\\) 时即 \\(b\\) 增大时，相当于图像向左移动， \\(b_0<0\\) 时即 \\(b\\) 减小时，相当于图像向右移动。 改变w则会改变函数的形状，因为 \\(f(x) = \\frac{1}{1+e&#94;{(w+w_0)x+b}} = \\frac{1}{1+e&#94;{wx\\frac{w+w_0}{w} + b}}\\) ，相当于在横轴上进行了放缩。当 \\(|w|\\) ( \\(w<0\\) 的时候，函数会在x轴上反转，不过不影响我们理解)变的越来越大的时候，函数就会变的越来越陡峭。实际中观察，当 \\(w=100\\) 左右时，函数的陡峭程度已经接近于阶跃函数了。 继续增大 \\(w\\) 的值使得其输出更加接近阶跃函数： 当我们对隐藏层的神经元的输出进行加权求和的时候，分析阶跃函数要比分析sigmoid函数容易。我们很容易可以知道两个阶跃函数的和是什么形式，但是对于两个sigmoid函数就没有这么容易得到。所以我们先假设这些神经元输出函数就是阶跃函数，当然这也是一种近似，我们随后再看这和真正的情况相比会有什么影响。 再考虑另外一个问题，我们知道标准的 \\(\\sigma (z)\\) 的中点在 \\(z=0\\) 处，当它近似为一个阶跃函数的时候，阶跃点在哪呢？容易知道就是在 \\(wx+b = 0\\) 处，即 \\(s = -\\frac{b}{w}\\) 。于是对于已经近似被认为是阶跃函数的神经元就可以只用一个参数 \\(s = -\\frac{b}{w}\\) 来代替之前的两个参数 \\(w\\) 和 \\(b\\) 。 接着再来考虑整个神经网络的情况。 右边展示的是加权输出的结果 \\(w_1 a_1 + w_2 a_2\\) ，其中 \\(a_1\\) 和 \\(a_2\\) 分别是隐藏层两个神经元的输出。不过注意到这里神经网络最终的输出为 \\(\\sigma (w_1 a_1 + w_2 a_2 +b)\\) 。 通过调节这些参数可以观察到： 调节 \\(s_1\\) 的大小 \\(s_2\\) 分别控制上下两个神经元的激活前后顺序。例如 \\(s_1<s_2\\) 的情况下，右边函数的第一个阶梯由第一个神经元输出决定，因为它先被激活，第二个阶梯才是两者之和 调节 \\(w_1\\) 和 \\(w_2\\) 分别控制两个神经元输出在输出神经元的权重，当其中一个为0时，只剩下一个输入，右边的函数也显示为只有一个阶跃函数。 最后，试着让 \\(w_1 = 0.8, w_2 = -0.8\\) ，然后 \\(s_1 = 0.4, s_2 = 0.6\\) 就得到一个在(0.4, 0.6)上的门函数： 如果我们固定 \\(s_1\\) 和 \\(s_2\\) ，然后 \\(w_1 = -w_2\\) ，这样就可以将门函数看作是只有一个参数 \\(h\\) 的模型，其中 \\(h = w_1\\) ，对应着就是门函数的\"门梁\"的位置。 通过组合神经元我们就可以轻易得到两个门函数组合的情况： 同样的方法，我们可以构造任意数量任意高度的门函数。因为对于[0,1]这个区间的划分是可以有无限多N个的，只要使用N对隐藏层神经元就，然后分别配上对应的 \\(h\\) 就可以达到要求了。 上图就是一个五个宽度一样的门函数的情形，高度由各自的参数 \\(h\\) 决定。但是仅仅这样就能说明神经网络可以计算任意函数吗？看下面这个例子： 这个函数来自： $$f(x)=0.2 + 0.4x&#94;2 + 0.3x sin(15x) + 0.05 cos(50x) \\quad (113)$$ 看上去这个函数形式和神经网络完全没什么联系，接下来就来看看神经网络是怎么去近似计算它的。 前面提到过 \\(w_1 a_1 + w_2 a_2 + ...\\) 并不是神经网络最终输出，最终输出为 \\(\\sigma (\\sum_j w_j a_j + b)\\) ，那么为了让最终输出为需要的函数 \\(f(x)\\) ，就要求 \\(\\sigma\\) 函数的输入为 \\(\\sigma&#94;{-1} (f(x))\\) ，即隐藏层输出的加权和为 \\(\\sigma&#94;{-1} (f(x))\\) ，其中 \\(\\sigma&#94;{-1}\\) 为 \\(\\sigma\\) 的反函数。 于是只要有方法可以让隐藏层输出和近似等于上面反函数，就相当于神经网络的最终输出近似等于 \\(f(x)\\) 了。对于上面五个门函数的情形，通过调节各自的 \\(h\\) 得到符号近似要求的结果： 这虽然只是一个粗略的近似，结果也不唯一，但是只要通过增加门函数的个数，即增加隐藏层神经元的个数，就可以让结果越来越精确。将这个得到的模型转换到我们的神经网络参数上，隐藏层的 \\(w\\) 取了很大的数 \\(w=1000\\) ，由于 \\(s=-\\frac{b}{w}=0.2\\) ，得到 \\(b = -1000*0.2=-200\\) 。 输出层的权重由 \\(h\\) 决定，例如第一个 \\(h=-1.3\\) ，说明它代表的两个权重分别为-1.3和1.3，以此类推，输出层的bias这里被设置为0。 这样就完成了通过构造一个神经网络来逼近目标函数的目的了，而且通过增加隐藏层神经元的个数可以使得这个近似结果更加准确。事实上，我们的目标函数 \\(f(x)=0.2+0.4x&#94;2+0.3sin(15x)+0.05cos(50x)\\) 的形式无关紧要，本质上我们使用一个单层神经网络构建了一个lookup表，不同区间对应不同的值，区间分的越细小，就越准确。 三. 一般情形：多输入多输出情况 先考虑两个输入的情况： 我们将 \\(x\\) 和 \\(y\\) 看作是变量，其加权输出和为因变量，这样就将之前的平面图像转变为了3d图像，不妨先设 \\(w_2 = 0\\) ，这样图像为： 可以看到平行于x轴的任意截面都是之前看到的平面上的曲线形式。同样的原理，改变 \\(w_1\\) 和 \\(b\\) 分别改变曲线的形状和位置。同样将 \\(w_1\\) 设定为一个很大的数，曲线转变为阶跃函数，位置为 \\(s_x = -\\frac{b}{w_1}\\) ： 这里对应着 \\(w_1 = 1000, w_2 = 0\\) 。同样可以设定 \\(w_2 = 1000, w_1 = 0\\) 这样就成了平行于y轴的曲线： 接着通过组合，我们得到了对应门函数的一个3d情况： 注意这里的y并没有起到作用，和y相连的权重都被设置成了0。类似也有只有y的版本，将和x的相连的权重设置为0: 如果我们将这两个方向垂直的门函数相加呢： 改变 \\(h\\) 的大小可以改变图像的高度，很容易可以知道中间最高的地方是边上的两倍高。我们于是想到能否用中间的高度作为中间区间上的值，这种方法去将定义域分割成一个个区间呢，然后每个区间对应一个值，区间分的越细就越逼近原函数。类似于之前单输入的情况，这次是一个二维的lookup表。 但是这就需要我们得到的是一个下图类似的塔函数： 但是我们得到的情况是除了中间是高的（ \\(2h\\) ），边上并不是平的，也有一定的高度（ \\(h\\) ），并不满足塔函数的条件。怎么将其转变为塔函数的形状呢？注意到，这里只是隐藏层的加权输出和，并不是输出神经元的输出，也就是说还有一个 \\(\\sigma\\) 函数的作用没有考虑，就可以尝试调节输出神经元的bias \\(b\\) 来进行调节。 考虑门函数的输出值由 \\(h\\) 决定，我们可以得到神经网络的输出值（不再是隐藏层的输出了）中间区域的值近似为 \\(f_{max} = \\frac{1}{1+e&#94;{-(2h+b)}}\\) ，边上区域的值近似为 \\(f_{min} = \\frac{1}{1+e&#94;{-(h+b)}}\\) 。我们想要 \\(f_{max}\\) 能够近似等于需要的值 \\(g\\) ，这种情况下得到条件一： \\(2h+b=c\\) ，其中 \\(c\\) 为一个常数。又为了让 \\(f_{min} \\approx 0\\) ，就需要条件二： \\(h+b<<0\\) 。这样就可以通过调节 \\(h\\) 与 \\(b\\) 的值使得这两个条件都成立，因为只要 \\(h\\) 足够大， \\(h+b = c - h\\) 就会足够小。不妨选择 \\(h=10, b \\approx -\\frac{3h}{2}\\) ，得到： 注意到这里的图像是输出神经元的输出，也就是经过 \\(\\sigma\\) 函数后的结果。可以看到，这已经得到了一个形式不错的塔函数了，继续增加 \\(h\\) 的值，调节 \\(b=-\\frac{3h}{2}\\) ，效果会更加明显。 接着我们将两个类似这样的神经网络组合去得到两个塔函数： 每个塔函数对应着第二个隐藏层的每个神经元的输出，调节两个 \\(w\\) 的值可以分别调节其高度。同样的方式我们可以得到任意多个自定义高度的塔函数，每个塔函数对应2维平面上的一个区域，通过这样的2维lookup就可以使得第二个隐藏层的加权输出可以近似等价于任意关于两个变量的函数 \\(f(x,y)\\) 。 但是这毕竟不是输出层的输出，于是类似的方法我们使用 \\(\\sigma\\) 函数的反函数，让第二个隐藏层加权输出等价于 \\(\\sigma&#94;{-1} (f(x,y))\\) 即可。 如果输入变量个数多余两个呢？ 先看一下三个变量的情况 \\(x_1, x_2, x_3\\) 。类似于上面的情况，下面这个神经网络可以得到一个四维空间上的塔函数： 这里 \\(x_1,x_2,x_3\\) 为神经网络的输入， \\(s_1,t_1,s_2,t_2\\) 都是控制门函数的位置，其中第一层隐藏层的权重已经足够大了使得它们各自的输出为阶跃函数，然后各自的偏差由 \\(b=-sw\\) 得到。然后第二层隐藏层的权重全部为 \\(h\\) 和 \\(-h\\) ，然后不妨令其bias为 \\(-\\frac{5h}{2}\\) ，验证仍然满足之前构造塔函数的条件 \\(h+b = -\\frac{3h}{2} << 0\\) 和 \\(3h+b = \\frac{h}{2} = c\\) (c为一常数)。 随着增大 \\(h\\) 到一定程度，这个神经网络于是就相当于对于3维上一块区域 \\(x_1 \\in (s_1,t_1), x_2 \\in (s_2, t_2), x_3 \\in (s_3, t_3)\\) ，其值为1，其他任意位置为0的塔函数。 通过组合这样的神经网络就可以将多个塔函数相组合就可以近似替代任意三个变量的函数。同样的想法可以拓展到 \\(m\\) 维空间，只要记得将进行组合处的神经元bias设置为 \\((-m+\\frac{1}{2})h\\) 使得函数形式为中间凸起，旁边为0。 这样就得到了使用神经网络逼近任意输出为一维的函数的方法了。但是神经网络的输出经常是多维的 \\(f(x_1, ..., x_m) \\in R&#94;n\\) ，例如前面的MNIST问题时输出为10维，这种情况怎么处理呢？ 这种情况可以看作是 \\(n\\) 个独立的函数： \\(f&#94;1 (x_1, ..., x_,m), f&#94;2 (x_1, ..., x_m)\\) ，我们分别设计神经网络近似表示 \\(f_1, f_2\\) 等等，然后简单的将它们组合即可。 拓展： 上面介绍了如何用两个隐藏层的神经网络去近似表示函数，能否证明只需要一层隐藏层就可以完成这样的设计？试着在两个输入的情况下，依次证明：(a)除了x轴，y轴以外，可以构造任意方向的阶跃函数；(b)通过将大量的(a)中的函数叠加，可以得到一个底面形状不再是长方形而是圆形的塔函数；(c)使用这些圆形的塔函数，可以近似表示任意函数。 (a)： 先看一下之前得到的方向为什么是x轴，y轴，或者说这些方向是由什么决定的。 \\(\\frac{1}{1+e&#94;{-(w_1 x+ w_2 y +b)}}\\) ，之前选择w_2为0时，得到的阶跃函数时x轴方向的，可以发现阶跃函数的方向是和直线 \\(w_1 x+ w_2 y +b = 0\\) 垂直的方向。所以说只要根据需要的方向构造与该方向垂直的直线 \\(w_1 x + w_2 y + b = 0\\) 中的 \\(w_1\\) 和 \\(w_2\\) 即可，也就是说现在的阶跃函数的构造不仅仅依靠一个输入神经元，而是两个都需要。 (b): 两个长方形垂直相交的重叠部分是方形，如果三个，四个呢？这个也许不好想象，但是换一个思路，对一个长方形绕着其中心旋转，中间不变的部分就是以中心为圆心的内切圆，所以只要组合足够多不同方向的(a)中阶跃函数，就可以得到近似圆形底面的塔函数。 (c)： 长方形区域很容易可以拼成一块大的区域，但是圆形却不行，它们之间总是有缝隙的。可以通过本文后面部分介绍的方法，将这些未覆盖的部分当成是\"失败区域\"，使用函数近似表示原函数的 \\(\\frac{1}{M}\\) ，每次移动 \\(\\frac{1}{M}\\) 单位个步长，最后叠加就得到覆盖满所有区域的目标函数。 这里按照作者给出的提示解答完了这三步，但是我还是不大清楚这和能用一个隐藏层有什么联系，因为圆形的塔函数依然面临之前的一个问题，就是除了中间高的区域外，还存在边上的不为0的区域，还是要依靠一个神经元将其转变为真正的塔的形式，中间凸出，边缘为0。我理解的一个隐藏层可以解决的原因是类似于在进行傅立叶级数展开的时候，我们将函数表示成一组三角函数基函数的线性叠加。其实任意连续函数都可以看作为一组基函数的叠加，然后在一个隐藏层选择合适的基函数叠加即可。 四. 使用sigmoid以外的神经元 前面已经证明了以sigmoid神经元为基础的神经网络可以计算任意函数。回忆一下sigmoid函数的形式，对于输入 \\(z=\\sum_j w_j x_j +b\\) ， \\(\\sigma(z)\\) 的形式为： 如果将其换成一个不一样的激活函数 \\(s(z)\\) : 会出现什么情况呢？ 同样得方法，我们用这个函数也可以得到阶跃函数。试着增大 \\(w=100\\) ： 类似于sigmoid函数的情况，新的激活函数一样会收缩，随着 \\(w\\) 的继续增大，它最终也会成为一个阶跃函数的近似表示。然后通过改变 \\(b\\) 的值就能实现对该阶跃函数的移动。使用之前一样的方法就可以构造神经网络近似表示所需的目标函数。 那么是不是所有的激活函数 \\(s(z)\\) 都满足这样的要求可以得到阶跃函数呢？事实上只需要 \\(s(z)\\) 在 \\(z\\rightarrow -\\infty\\) 和 \\(z \\rightarrow \\infty\\) 时极限存在，并且这两个极限的值不相等即可。相等的话就不是阶跃了，而是一个\"平\"的常量函数了。激活函数满足这些性质之后，神经网络就可以逼近任意函数了。 拓展： 拓展一： 证明之前介绍的RELU神经元不满足上述的条件，但是RELU函数一样可以构造上述神经网络近似表示任意函数。 答案： RELU在 \\(x>0\\) 时是一个线性增长的函数，当 \\(x\\rightarrow \\infty\\) 时， \\(RELU(x) \\rightarrow \\infty\\) ，极限值并不存在，所以不满足要求。 虽然一个RELU函数按照之前改变参数的方法怎么也变不成阶跃函数，但是可以通过两个RELU函数相叠加得到。对于 \\(y=wx\\) 和 \\(y=-wx+b(k>0)\\) ，后者相对前者向右移动了 \\(\\frac{b}{w}\\) ，于是两者相加得到了一个分段函数 $$s(x) = \\left\\{\\begin{align*} & 0 \\quad if x \\leqslant 0 \\\\ & wx \\quad if 0<x\\leqslant \\frac{b}{w} \\\\ & b \\quad if x > \\frac{b}{w} \\end{align*}\\right.$$ 通过增大 \\(w\\) 就可以减小 \\(\\frac{b}{w}\\) 从而让这个分段函数中间增长的部分变的更窄，使之近似为阶跃函数。 拓展二： 考虑激活函数为线性函数 \\(s(z)=z\\) 的神经元，证明该函数不满足上述条件，也无法构造上述神经网络近似表示任意函数。 答案： 这个函数在 \\(x\\rightarrow \\infty\\) 和 \\(x \\rightarrow -\\infty\\) 处的极限都不存在，显然不满足上面的条件。 线性函数自身不具有这样的性质，线性函数的线性组合还是连续的线性函数，依然是无法满足阶跃函数的要求的。 五. 修正阶跃函数 之前已经证明了我们通过对神经元进行组合得到近似的阶跃函数，但这始终是近似表示，其中还存在一段函数并非阶跃函数的\"失败区域\"： 虽然通过增大wegiths，可以使得这块区域越来越窄，也就是说与阶跃函数的差别越来越小。不过还有其他的处理这个问题的方法的话当然就更好了。 事实上，这个问题并不难解决。设想对于一个一维函数 \\(f(x)\\) ，按照之前的方法，我们需要隐藏层的输出可以表示成： 按照之前的方法，我们使用一系列的门函数去表示这个函数： 可以看到，只要门函数足够多，门的宽度足够窄就可以使得对目标函数的近似表示越好，除了在交界处的\"失败区域\"。 这次我们不用上面的近似，我们使用目标函数值的一半作为近似对象， \\(\\sigma &#94;{-1}(\\frac{f(x)}{2})\\) : 接着使用另外一套隐藏层去近似表示这样的函数：它是上面门函数组成的函数横向平移半个门宽度得到的： 当我们把这两个函数叠加到一起的时候就会得到一个 \\(\\sigma &#94;{-1}(f(x))\\) 的近似。这个近似虽然还是会存在\"失败区域\"，但是比之前的近似已经好多了。这是因为，对于一个点在一个近似函数的\"失败区域\"时，它将不会出现在另一个近似函数的\"失败区域\"。 通过减小每次移动的步长为窗口的 \\(\\frac{1}{M}\\) ，将M个 \\(\\sigma&#94; {-1}(f(x)/M)\\) 的近似函数叠加，即得到更好的原始函数的近似表示。 六.总结 这一章描述了神经网络拟合函数的普遍性，但是这当然不是实际中我们使用神经网络计算的方式。不过它让我们知道以后在面对一个问题的时候，不是去怀疑神经网络能不能表示这个函数，而是考虑更重要的事，怎么才能找到一个这个函数的好的近似。 我们的讨论用了两层神经元，不过一层神经元也可以完成这样的工作。那我们为什么还要去用那些层数更多的深度神经网络呢？ 理论上这是可能的，不过实际上我们有很多理由相信深度网络更好。在第一章已经讨论过，现实中的问题和知识体系往往是由浅入深的层级关系，而深度网络的层级结构就非常切合这一点。例如对于图像识别的问题，只是着眼于单个像素点的理解当然是不够的，我们更希望它能够识别出更复杂的模式：从简单的线条到复杂的几何形状等等。在随后的章节也会看到，深度网络对于这种层级的问题处理结果的确要好于浅层网络。 总之就是，单层的神经网络就已经可以计算任何函数，但是经验告诉我们，在解决现实问题上，深度网络更加适合。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"深度学习","url":"shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-4.html"},{"title":"《深度学习与神经网络》笔记3","text":"我们前面已经学习了反向传播算法，它是我们学习神经网络的基础。这一章将会介绍一系列的方法技巧来改善反向传播算法的效果，进而改善学习到的神经网络模型。 这些技巧包括： 1. 新的损失函数：交叉熵损失函数 2. 四种\"正则化\"方法（L1，L2，dropout，人造训练数据），这些是为了我们的模型有更好的扩展性，在新的数据集上能有更好的表现，而不至于过拟合 3. 一种更好的初始化权重的方法 4. 一系列帮助选择超参的启发式方法。随后会在之前代码的基础上实现这些优化来改善我们手写数字识别的准确度。 当然这些也只是优化神经网络的冰山一角，还有很多其他的方法。不过掌握了这些基本的方法，可以加深我们对于问题的理解，对于新的方法技巧也可以很快的上手。 一. 交叉熵损失函数 失败是成功之母，我们学习的过程中免不了要犯错，知错能改，善莫大焉。神经网络的学习也是如此，如果错误都没有定义好的话，模型的学习过程当然也会很缓慢。 理想情况下，我们希望我们的学习算法越快收敛到最佳状态越好，那么实际情况呢？我们先来看一个例子： 这是一个最简单的模型，只有一个神经元，一个输入。我们希望它实现一个简单的功能：当输入为1的时候，输出为0。 这个功能很容易实现，我们手动设置参数都很容易就能满足。不过为了说明情况，我们还是使用梯度下降学习算法来学习这些参数。 我们设置初始参数为： \\(w = 0.6, b = 0.9\\) 。可以计算得出此时对于输入1的情况输出为 \\(\\frac{1}{1+e&#94;{-(0.6*1+0.9)}}\\approx 0.82\\) 。这个结果距离我们理想的输出0还是差的很远。我们设定一个合适的学习率 \\(\\eta = 0.15\\) ，选择二次损失函数，学习的过程如下： 可以看到，学习的过程很快，在前期的epoch中，损失函数迅速下降。最后得到0.09的损失，虽然不是0，但是已经很低了。假设现在我们选另外一组初始参数，此时 \\(w = 2, b = 2\\) , 此时初始输出为 \\(\\frac{1}{1+e&#94;{-(2*1+2)}}=0.98\\) ，距离0更远了。此时学习过程如下： 可以看到，学习的开始过程非常慢，前150个epochs，参数基本就没有变化，最后结束时得到的输出为0.2跟0还差的比较远，可见这次训练得到的模型并不理想。 这个现象拿来跟人的行为比较的话就显得很奇怪。我们人类在学习的时候，一般错的越离谱的地方改正起来当然就越快，可是这个神经网络却不是这样，初始时距离正确值更远的时候学习的却更慢。这当然不是我们希望的，那我们能找到一种好的方法来避免这种效率低下的学习情况吗？ 为了理解这中情况发生的原因，考虑到神经元学习时改变率是跟损失函数的偏导数 \\(\\frac{\\partial C}{\\partial w}\\) 和 \\(\\frac{\\partial C}{\\partial b}\\) 有关的，学习慢就说明这些值很小。为了理解为什么小，我们先看一下之前定义的二次损失函数： $$C = \\frac{(y-a)&#94;2}{2} \\quad (54)$$ 这里 \\(a=\\sigma (z)\\) 是输入为1时神经网络的输出，然后 \\(y=0\\) 是理想的输出值，求其偏导数得到： $$\\frac{\\partial C}{\\partial w} = (a - y)\\sigma '(z)x = a\\sigma '(z) \\quad (55)$$ $$\\frac{\\partial C}{\\partial b} = (a-y)\\sigma '(z) = a\\sigma '(z) \\quad (56)$$ 为了理解这些式子的行为，先看一下 \\(\\sigma '(z)\\) 的函数图像： 我们发现在输出接近1的时候，函数非常平缓，也就是说此时 \\(\\sigma '(z)\\) 很小，这也会导致 \\(\\frac{\\partial C}{\\partial w}\\) 和 \\(\\frac{\\partial C}{\\partial b}\\) 很小，从而导致学习速度缓慢。后面会看到，对于大多数神经网络来说，学习速率慢都主要是由于这个原因，而不仅仅是针对这个单个神经元的情况。 1.1 交叉熵损失函数介绍 在介绍交叉熵函数之前先看一个稍微复杂点的例子： 由之前的一个输入改为了多个输入，其中 \\(z = \\sum_jw_jx_j+b\\) 。交叉熵损失函数定义为： $$C = -\\frac{1}{n}\\sum_x [yln(a) + (1-y) ln (1-a)] \\quad (57)$$ 初看这个式子可能会觉得很奇怪，为什么可以把它当成是一个损失函数呢？ 这主要是由于它的两个性质，首先，它是非负的，即 \\(C>0\\) ，这个从公式(57)的形式可以验证。其次，当神经元输出接近我们理想输出的时候，交叉熵将会接近0（当然这需要假设理想输出为0或1，对于一般的分类问题是满足这个条件的）。 然后来看为什么交叉熵损失就可以避免训练速度下降，求偏导： $$\\frac{\\partial C}{\\partial w_j}=-\\frac{1}{n}\\sum_x (\\frac{y}{\\sigma (z)} - \\frac{(1-y)}{1-\\sigma (z)}) \\frac{\\partial \\sigma}{\\partial w_j} \\quad (58)$$ $$\\frac{\\partial C}{\\partial w_j}=-\\frac{1}{n}\\sum_x (\\frac{y}{\\sigma (z)} - \\frac{(1-y)}{1-\\sigma (z)}) \\sigma '(z) x_j \\quad (59)$$ $$\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n}\\sum_x \\frac{\\sigma '(z)x_j}{\\sigma (z)(1-\\sigma (z))}(\\sigma (z) - y) \\quad (60)$$ 由于 \\(\\sigma (z) = \\frac{1}{1+e&#94;{-z}}\\) ，容易验证 \\(\\sigma '(z) = \\frac{\\sigma (z)}{1-\\sigma (z)}\\) 。代入上式得到： $$\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n}\\sum_x x_j (\\sigma (z) - y) \\quad (61)$$ 这个式子表明，这个权重的梯度跟 \\(\\sigma (z) -y\\) 有关，也就是说，误差越大，学习的就越快，和我们希望的情况一样。它避免了二次损失函数中由 \\(\\sigma '(z)\\) 导致的学习速率下降的问题。 同样的方法，对bias有： $$\\frac{\\partial C}{\\partial b} = \\frac{1}{n}\\sum_x (\\sigma (z) -y) \\quad (62)$$ 练习 问题 ： 证明 \\(\\sigma '(z) = \\sigma (z) (1 - \\sigma (z))\\) 答案： \\(\\sigma '(z) = -\\frac{-e&#94;{-z}}{(1+e&#94;{-z})&#94;2} = \\frac{e&#94;{-z}}{1+e&#94;{-z}} \\frac{1}{1+e&#94;{-z}} = \\sigma (z) (1-\\sigma (z))\\) 回到一开始一个神经元的那个例子，这次使用交叉熵损失函数。初始参数一样为 \\(w=0.6, b=0.9\\) ，学习过程结果为： 和之前二次损失函数一样，学习过程很迅速。再来看之前二次损失函数出问题的初始参数 \\(w=2,b=2\\) ，使用交叉熵损失函数： 这次初始的学习过程并没有因为输出偏离理想值过多而导致学习缓慢，可以看到损失函数的值在开始阶段很陡峭，也就是说学习速率很快。 多层多个输出神经元的情况，我们也可以写出起交叉熵损失函数： $$C=-\\frac{1}{n} \\sum_x \\sum_j [y_j ln(a&#94;L_j) + (1 - y_j) ln (1 - a&#94;L_j)] \\quad (65)$$ 这里和之前的公式（57）没什么区别，除了之前是一个输出，这里是多个输出，将每个神经元的输出求一个交叉熵然后再相加。 对于交叉熵的计算，一般情况下，假设有两个概率分布 \\(p_j\\) 和 \\(q_j\\) ，有 \\(\\sum_j p_j ln (q_j)\\) ，将上面一个神经元的输出a和1-a看成两个概率分布就和这个对应起来了。 但是，当输出层是多个神经元的时候，向量 \\(a&#94;L\\) 通常就不构成一个概率分布了，那么 \\(\\sum_j p_j ln (q_j)\\) 就没什么意义了。但是我们可以将公式(63)看作是对单个输出神经元求交叉熵再求和的过程，因为每个神经元输出都可以被看成是一个在{0, 1}上的概率分布，概率为a和1-a。 那么什么时候该使用交叉熵损失呢？事实上，在激活函数为sigmoid函数时，交叉熵损失基本上总是更好的选择。考虑到我们需要随机初始化参数，这就可能会导致输出严重偏离理想输出的情况（例如本该输出0，却输出了1）。如果使用二次损失，就会导致学习过程很慢。 练习 问题一： 有的时候大家大家可能会出现把 \\(-[yln(a) + (1-y) ln(1-a)]\\) 中的y和a记反的情况，记成了 \\(-[aln(y) + (1-a)ln(1-y)]\\) ，当 \\(y=0\\) 或 \\(y=1\\) 时，第二个式子会出现什么问题，第一个式子会出现这样的问题吗，为什么？ 答案： \\(y=0\\) 时， \\(aln(y)\\) 只在 \\(a=0\\) 时为0，其余为 \\(\\infty\\) ，而 \\((1-a)ln(1-y)=0\\) ； \\(y=1\\) 的情况类似。 对于第一个式子则没有这样的问题， \\(y=0\\) 时，有 \\(-ln(1-a)\\) , \\(y=1\\) 时，有 \\(-ln(a)\\) 。 问题二： 在一开始的单神经元讨论中，我们知道当对所有输入优 \\(\\sigma (z) \\approx y\\) 时，交叉熵最小，这个讨论当时是建立在y为0或1的情况，也就是在分类的情况下讨论的。证明对于其他问题（例如回归问题），当y是[0,1]之间的数的时候，仍然有 \\(\\sigma (z) =y\\) 对于所有输入成立将最小化交叉熵损失，即此时交叉熵为 $$C = -\\frac{1}{n}\\sum_x [y ln(y) + (1-y) ln(1-y)] \\quad (64)$$ 答案： 因为每个输出样本是独立的，这其实就是求 \\(C(a) = yln(a) + (1-y) ln(1-a)\\) 最大值时的a。 求导 \\(\\frac{\\partial C}{\\partial a} = \\frac{y}{a} + \\frac{y-1}{1-a} = \\frac{y-a}{a(1-a)}\\) ，因为 \\(a(1-a)>0\\) ,所以当 \\(a<y\\) 时， \\(\\frac{\\partial C}{\\partial a}>0\\) , 当 \\(a>y\\) 时， \\(\\frac{\\partial C}{\\partial a} <0\\) ，函数先增后减，在 \\(y=a\\) 时取最大值，即 \\(a = \\sigma (z) \\approx y\\) 的时候，交叉熵损失最小。 拓展 多层网络多神经元输出的情况： 对于二次损失函数，此时有： $$\\frac{\\partial C}{\\partial w&#94;L_{jk}} = \\frac{1}{n}\\sum_x a&#94;{L-1}_k (a&#94;L_j - y_j) \\sigma '(z&#94;L_j) \\quad (65)$$ 同样的， \\(\\sigma '(z&#94;L_j)\\) 会导致学习速率下降当该神经元处于错误的saturated状态时。对于交叉熵的情况，有： $$\\delta &#94;L = a&#94;L -y \\quad (66)$$ 代入上式有： $$\\frac{\\partial C}{\\partial w&#94;L_{jk}} = \\frac{1}{n}\\sum_x a&#94;{L-1}_{k}(a&#94;L_j - y_j) \\quad (67)$$ \\(\\sigma '(z&#94;L_j)\\) 就被去掉了，说明交叉熵损失同样可以解决这种情况下学习速率下降的问题。 线性激活函数的情况： 此时，对于二次损失函数有： $$\\delta &#94;L = a&#94;L - y \\quad (68)$$ 由于没有了sigmoid激活函数有 \\(a=z\\) ，于是得到： $$\\frac{\\partial C}{\\partial w&#94;L_{jk}} = \\frac{1}{n}\\sum_x a&#94;{L-1}_k (a&#94;L_j - y_j) \\quad (69)$$ $$\\frac{\\partial C}{\\partial b&#94;L_j} = \\frac{1}{n}\\sum_x (a&#94;L_j - y_j) \\quad$$ 可以看出，此时，二次损失函数并不会导致学习速率下降，是合适的损失函数。 1.2 在MNIST数字分类上使用交叉熵损失函数 类似于第一章的神经网络结构，30个神经元的隐藏层，不过这里使用交叉熵损失函数。 1 2 3 4 5 6 import mnist_loader import network2 training_data , validation_data , test_data = mnist_loader . load_data_wrapper () net = network2 . Network ([ 784 , 30 , 10 ], cost = network2 . CrossEntropyCost ) net . large_weight_initializer () net . SGD ( training_data , 30 , 10 , 0.5 , evaluation_data = test_data , 10 monitor_evaluation_accuracy = True ) 注意到，现在我们移除了模型初始化时的参数初始化过程，而改为了使用不同的初始化函数来手动初始化这些参数。不过一开始我们还是使用之前一样的初始化方法，只是改为了在net.large_weight_initializer()中进行。这次得到的结果是95.49%和之前使用二次损失的情况下的95.42%几乎一致。 当我们使用100个隐藏层时，得到的结果为96.82%，比二次损失时的96.59提高了不少。考虑到错误率从3.41下降到3.18，接近14个百分点，已经称得上不错的改进了。 虽然交叉熵损失给我们带来了或多或少的改进，但是这并不足以证明它就是一个更好的选择。这是因为这里并没有对超参进行最优的选择。在进行超参的优化选择后，这个改善就会更加明显，不过就目前来说，这足以使我们相信交叉熵损失是一种更好的选择。 就本章乃至本书的后续内容来说，我们经常会对算法进行一些优化，然后会获得一些改善的结果。但是要使这些改善非常明显通常需要大量的超参优化，为了避免这些工作，我们仅仅对这些优化浅尝辄止，基本上能改善效果就行，就说明我们的优化起到了作用，而不纠结于是否获得了最优的结果。 到目前为止，我们已经花了很大的篇幅来介绍交叉熵损失函数。可能我们会有疑问，为什么要花这么大力气去使用它，最后在MNIST上也就得到了一个马马虎虎的结果？随后我们还会看到其他技术，尤其是正则化将会带来更大的改善。那为什么要用交叉熵损失呢？一方面是因为它是一个广泛使用的损失函数，值得去学习理解。更主要的原因是神经元的saturation是神经网络中一个很重要的问题，而交叉熵是一个很好的理解这个问题进而去处理这个问题的开端。 1.3 交叉熵的意义以及来历 目前为止，关于交叉熵的讨论还是仅仅停留在公式的推导和实现上。但是它到底有什么意义呢，它是怎么被创造出来的呢？ 我们先看后一个问题，一开始是出于什么样的动机才想出了交叉熵这个概念。假设我们已经知道了算法学习过程变慢是由于 \\(\\sigma '(z)\\) 导致的，那么我们就在想能不能找到一个合适的损失函数使得 \\(\\sigma '(z)\\) 消失。使得： $$\\frac{\\partial C}{\\partial w_j} = x_j (a-y) \\quad (71)$$ $$\\frac{\\partial C}{\\partial b} = (a-y) \\quad (72)$$ 为了找到这样的损失函数，注意到： $$\\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial a}\\frac{\\partial a}{\\partial z}\\frac{\\partial z}{\\partial b} = \\frac{\\partial C}{\\partial a}\\sigma '(z) \\quad (73)$$ 代入 \\(\\sigma '(z) = \\sigma(z)(1-\\sigma(z)) = a(1-a)\\) 得到： $$\\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial a}a(1-a) \\quad (74)$$ 跟（72）式对比得到： $$\\frac{\\partial C}{\\partial a} = \\frac{a-y}{a(1-a)} \\quad (75)$$ 通过积分可以得到： $$C = -[yln(a) + (1-y)ln(1-a)] + constant \\quad (76)$$ 对于多个样本的情况也是一样的： $$C = -\\frac{1}{n}\\sum_x [yln(a) + (1-y)ln(1-a)] + constant \\quad (77)$$ 通过这样的方式就找到了交叉熵损失函数。 但是它的意义是什么呢？我们怎么去理解它呢？详细解释需要更多的篇幅，这里简单说一下，它来自信息论领域，更确切的说它描述的是状态的随机性。例如，假设我们神经元的输出a表示的是输出为1的概率，1-a为输出为0的概率，那么交叉熵测量的就是输出是0还是1的随机性。如果我们对于结果很确定，比如说概率为1或者0，那么这种随机性就很低，交叉熵就小，反之则大。 拓展 ： 之前讨论了在使用二次损失函数的时候，当神经元处于saturated状态的时候，学习速率很慢。但是注意到影响学习速率的还有另外一个因素： $$\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n}\\sum_x x_j (\\sigma (z) - y)$$ 其中的 \\(x_j\\) 同样会影响到这个导数值，当其接近0的时候， \\(w_j\\) 的学习速率会变得很慢。解释为什么没办法通过选择合适的损失函数来删除掉 \\(x_j\\) 的影响。 如果还记得这个式子的由来的话就很容易知道， \\(x_j\\) 这项是由于 \\(\\frac{\\partial z}{\\partial w_j}\\) 引入的，无论怎么选择损失函数都没办法将其去除。 1.4 Softmax 这一章我们主要将会使用交叉熵损失函数来解决训练过程缓慢的问题，但是这里还是要简要介绍一下另外一种基于softmax层神经元的方法。 softmax的想法是定义一种新的神经网络输出层，在该输出层上， \\(z&#94;L_j = \\sum_k w&#94;L_{jk}a&#94;{L-1}_k + b&#94;L_j\\) ，随后并不使用sigmoid函数计算输出，而是对每个 \\(z&#94;L_j\\) 使用softmax函数: $$a&#94;L_j = \\frac{e&#94;{z&#94;L_j}}{\\sum_k e&#94;{z&#94;L_k}} \\quad (78)$$ 很容易验证 $$\\sum_j a&#94;L_j = \\frac{\\sum_j e&#94;{z&#94;L_j}}{\\sum_k e&#94;{z&#94;L_k}} = 1 \\quad (79)$$ 所以当其中一个输出增加的时候，其他的输出会减小。而且由于指数函数的原因，这些输出值都是正数，它们的和为1，所以可以将softmax层的输出看作是一个概率分布。 softmax输出层被当作概率分布是很有意义的，因为它可以将输出 \\(a&#94;L_j\\) 看成是输出为j的概率。例如，对于MNIST分类问题， \\(a&#94;L_j\\) 可以认为是神经网络估计这个数字是j的概率。而如果使用sigmoid函数，则没有这样的性质。输出值也没有这样直观的解释。 练习 问题： 举出一个例子，证明在使用sigmoid输出层的时候， \\(a&#94;L_j\\) 的和不为1 答案： 如果用sigmoid函数，基本跟1就没有联系吧。随便举个例子，三个输出层，分别为 \\(z&#94;L_1 = z&#94;L_2 = z&#94;L_3 = 1\\) ，然后 \\(a&#94;L_1 = a&#94;L_2 = a&#94;L_3 = \\frac{1}{1+e&#94;{-1}} \\approx 0.73\\) ，它们的和当然不为1。 指数使得softmax输出为正数，它们之间的和为1，所以我们还可以将softmax看作是一种对 \\(z&#94;L_j\\) 进行放缩的一种方法，使得它们之间构成一个概率分布。 练习 问题一： softmax函数的单调性：证明 \\(\\frac{\\partial a&#94;L_j}{\\partial z&#94;L_k}\\) 在 \\(j=k\\) 时为正，在 \\(j \\neq k\\) 时为负，于是有增大 \\(z&#94;L_j\\) 将会增大对应神经元j的输出，会减小其他神经元的输出。 答案： 由定义有 \\(a&#94;L_j = \\frac{e&#94;{z&#94;L_j}}{\\sum_k e&#94;{z&#94;L_k}}\\) , \\(\\frac{\\partial a&#94;L_j}{\\partial z&#94;L_j} = \\frac{e&#94;{z&#94;L_j}(\\sum_k e&#94;{z&#94;L_k}) -e&#94;{z&#94;L_j}e&#94;{z&#94;L_j}}{(\\sum_k e&#94;{z&#94;L_k})&#94;2}=\\frac{e&#94;{z&#94;L_j}\\sum_{k\\neq j} e&#94;{z&#94;L_k}}{(\\sum_k e&#94;{z&#94;L_k})&#94;2}>0\\) 对于任意 \\(k \\neq j\\) 的情况， \\(\\frac{\\partial a&#94;L_j}{\\partial z&#94;L_k} = -\\frac{e&#94;{z&#94;L_j + z&#94;L_k}}{(\\sum_k e&#94;{z&#94;L_k})&#94;2} < 0\\) 问题二： sigmoid输出层的一个优点是 \\(a&#94;L_j\\) 只和它的加权输入有关，因为 \\(a&#94;L_j = \\sigma (z&#94;L_j)\\) , 解释为什么softmax输出层就没有这样的性质，它的任意输出 \\(a&#94;L_j\\) 都和所有神经元的加权输入有关 答案： 这个从softmax的定义式就能看出来，分母中受到其它 \\(z&#94;L_k\\) 的影响，而且上面的导数值也表明 \\(\\frac{\\partial a&#94;L_j}{\\partial z&#94;L_k}\\) 对于其他 \\(k \\neq j\\) 的情形，导数值并不等于0，表明它会受到这些 \\(z&#94;L_k\\) 的影响。 拓展 设想我们有一个有softmax输出层的神经网络，激活量 \\(a&#94;L_j\\) 已知，证明对应的加权输入的形式为： \\(z&#94;L_j = ln(a&#94;L_j) + C\\) ，其中 \\(C\\) 为与 \\(j\\) 无关的常数。 由定义可知 \\(a&#94;L_j = \\frac{e&#94;{z&#94;L_j}}{\\sum_k e&#94;{z&#94;L_k}}\\) ， \\(e&#94;{z&#94;L_j} = a&#94;L_j \\sum_k e&#94;{z&#94;L_k}\\) ， \\(z&#94;L_j = ln(a&#94;L_j \\sum_k e&#94;{z&#94;L_k}) = ln(a&#94;L_j) + C\\) ，其中 \\(C = ln(\\sum_k e&#94;{z&#94;L_k})\\) 为与 \\(j\\) 无关的常数。 在了解了softmax的这些概念后，我们来看一下它是怎么处理学习变慢的问题的。先定义对数似然损失函数。 \\(x\\) 为一个训练样本输入， \\(y\\) 是理想输出，那么对数似然损失为： $$C = -ln(a&#94;L_y) \\quad (80)$$ 例如，在训练MNIST图片的时候，当输入为7的图片时，这个损失为 \\(-ln(a&#94;L_7)\\) 。为了理解这个定义，考虑如果我们的神经网络准确度很高，有足够的信心判断它时7，则 \\(a&#94;L_7\\) 将会接近1，对数损失将会很小，否则 \\(a&#94;L_7\\) 接近0时，损失就会很大。所以对数似然符合我们对于损失函数的要求。 对该损失函数求偏导数得： $$\\frac{\\partial C}{\\partial b&#94;L_j} = a&#94;L_j - y_j \\quad (81)$$ $$\\frac{\\partial C}{\\partial w&#94;L_{jk}} = a&#94;{L-1}_k (a&#94;L_j -y_j) \\quad (82)$$ 可以看出这个函数和交叉熵损失函数时计算的偏导数是一样的。事实上，softmax输出层结合对数似然损失的情况和sigmoid输出层结合交叉熵损失的情况非常相似。 既然这两种情况很相似，那么该如何做出选择呢？事实上，对于大多数情况这两种方案都是可行的。在这章的剩余部分，我们会使用sigmoid+交叉熵损失的组合，在第六章再使用softmax+log似然损失。转变的原因只是我们希望迎合一些学术论文中的神经网络的例子。大多数情况下，softmax+log似然损失更适合于讲输出解释为概率的情况，尤其适合于类似MNIST的分类问题。 拓展 拓展一： 证明方程（81）和（82） \\(\\frac{\\partial C}{\\partial w&#94;L_{jk}} = \\frac{\\partial C}{\\partial a&#94;L_j}\\frac{\\partial a&#94;L_j}{\\partial z&#94;L_j}\\frac{\\partial z&#94;L_j}{\\partial w&#94;L_{jk}}\\) 其中 \\(\\frac{\\partial C}{\\partial a&#94;L_j} = -\\frac{1}{a&#94;L_j} = -\\frac{\\sum_k e&#94;{z&#94;L_k}}{e&#94;{z&#94;L_j}}\\) 由之前计算可得 \\(\\frac{\\partial a&#94;L_j}{\\partial z&#94;L_j} = \\frac{e&#94;{z&#94;L_j}\\sum_{k\\neq j} e&#94;{z&#94;L_k}}{(\\sum_k e&#94;{z&#94;L_k})&#94;2}\\) \\(\\frac{\\partial z&#94;L_j}{\\partial w&#94;L_{jk}} = \\frac{\\partial (w&#94;L_{jk}a&#94;{L-1}_k + b&#94;L_j)}{\\partial w&#94;L_{jk}} = a&#94;{L-1}_k\\) 三式相乘得到 \\(\\frac{\\partial C}{\\partial w&#94;L_{jk}} = -a&#94;{L-1}_k\\frac{\\sum_{k\\neq j} e&#94;{z&#94;L_k}}{\\sum_k e&#94;{z&#94;L_k}} = a&#94;{L-1}_k (a&#94;L_j - y_j)\\) ，这里的 \\(y_j\\) 需要说明一下， \\(y\\) 是对应j的输出向量，也就是在分量j上为1，其余均为0 同理 \\(\\frac{\\partial C}{\\partial b&#94;L_j} = a&#94;L_j -y_j\\) 因为 \\(\\frac{\\partial z&#94;L_j}{\\partial b&#94;L_j} = 1\\) 拓展二：softmax加对数似然损失情况下的反向传播算法 证明 $$\\delta &#94;L_j = \\frac{\\partial C}{\\partial z&#94;L_j} = a&#94;L_j - y_j \\quad (84)$$ 这个其实在拓展一的过程中已经证明了，就是前面两个偏导数的积。 二. 过拟合和正则化 2.1 过拟合 毋庸置疑，一个参数非常多的模型具有很强的拟合数据的能力，但是仅仅能够拟合已知的测试数据并不能证明它是一个好模型，它很有可能在面对新的数据的时候就无能无力了，这样的模型就不是一个可信的模型，这就是我们经常会遇到的过拟合问题。 回到我们隐藏层为30个神经元的那个模型，有将近24000个参数，100个神经元的时候，变成了80000个参数，这么多参数的情况，我们的模型是否可信呢？ 我们先测试一下，我们使用30个神经元的情形训练模型，但是这次不用50000个训练图片，而是只用1000张测试图片。训练结果如下： 结果看上去不错，损失函数平滑的下降。再来看一下在测试集上的表现： 仅仅在82%左右，虽然我们的损失函数显示是在一路下降，但是准确率在达到一定水平之后就没有继续提高，只是在这个这个值上下波动。所以我们可以认为在epoch280之后，我们的模型在测试数据上就不在具有足够的泛化能力了，也就是说此时的神经网络出现了过拟合的问题。 这里我们可能会有疑问，为什么用训练数据的损失函数变化去对比测试数据的准确度？让我们来看一下测试数据的损失函数： 可以看到在epoch15左右的时候，损失就已经开始上升了，而此时训练数据的损失还在下降，这也是模型已经过拟合的另一个体现。于是就有了一个问题，将epoch15还是epoch280作为过拟合的开始呢？从实际出发，我们一般更关心模型的准确率，而不是损失函数的变化，所以epoch280通常是更好的选择。 从训练数据的准确度变化也可以看出过拟合的另一个体现： 准确率几乎达到了100%，然而我们在测试数据上试验却只达到了82.27%的准确率。此时我们的模型只是在学习训练数据的特点，而不是在学会去识别数字。就类似于仅仅是在记忆训练集，而没有学会真正的识别数字的方法，导致在测试集上表现差。 由于神经网络中通常都有大量的权重和偏差，使得过拟合在神经网络中是一个经常发生的问题，于是就有了各种各样的检测和消除过拟合的方法。 判断是否存在过拟合最简单明了的方法就是上面的方法，对比训练数据和测试数据的准确率。看到测试数据上准确率不再提升时，我们就可以停止训练了。当然，这并不是过拟合真正的表现，有可能出现训练准确率和测试准确率都不提升的情况。 在实际中，我们增加一个验证集来替代测试集去检测过拟合的问题。 1 2 import mnist_loader training_data , validation_data , test_data = mnist_loader . load_data_wrapper () 回到MNIST上，这次我们就使用50000的训练，10000的验证，10000的测试。那为什么不能直接使用测试集去避免过拟合呢？简单的说，可以将验证集的使用是帮助我们去进行超参选择的过程，如果使用测试集可能会使得当前的超参是对于测试集最好的，而不是真实的最佳，同样会导致过拟合问题。 当然，实际上，当我们看到模型在test数据上表现不好的时候，肯定会重新去训练数据，选择超参，这样做下去是不是又会导致过拟合呢？这的确是个难题，不过本文并不讨论这些内容。在本文的内容中，并不需要去特意考虑这个问题，将数据分成training，validation和test就可以了。 之前过拟合的时候我们使用了仅仅1000张训练数据，这次我们使用50000张训练数据看一下： 可以看出，训练数据和测试数据的表现很接近。训练数据上最好的结果97.86%对应了在测试上的95.33%，这期间相差了2.53%对比之前17.73%的差距，说明此时过拟合已经被减少了非常多了。 通常来说，这种增大训练数据的方法是应对过拟合最好的方法，只要有足够的数据，过拟合就不容易发生。不幸的是，训练数据的获取是要付出代价的，有的时候并不是那么容易就能获得，而且数据过多的话，算法的执行效率有的时候也会成为瓶颈。 2.2 正则化 抛开增加数据量这种方法，是否还存在其他从技术角度解决过拟合的方法呢？一种方法是降低模型的复杂度，也就是减小神经网络的模型规模。然而，复杂的神经网络具有解决更复杂问题的能力，所以我们通常并不想使用这种方法。 所幸我们还有一种被称为正则化的技术。接下来先讲一种使用最广发的正则化技术：L2正则化，通过在损失函数后加上一项模型参数的平方项得到，例如对于交叉熵损失函数： $$C = -\\frac{1}{n}\\sum_x \\sum_j [y_j ln(a&#94;L_j) + (1-y_j)ln(1-a&#94;L_j)] + \\frac{\\lambda}{2n}\\sum_w w&#94;2 \\quad (85)$$ 第一项就是之前的交叉熵损失， \\(\\lambda>0\\) 是正则化系数。 以同样的方式可以定义平方损失函数的L2正则化形式： $$C = \\frac{1}{2n}\\sum_x ||y-a&#94;L||&#94;2 + \\frac{\\lambda}{2n} \\sum_w w&#94;2 \\quad (86)$$ 更通用的形式为： $$C = C_0 + \\frac{\\lambda}{2n}\\sum_w w&#94;2 \\quad (87)$$ \\(C_0\\) 为其他任意损失函数。 正则化的影响在于引入了模型参数大小的影响，更偏向于选择参数小的模型。正则化还可以看作是一种在最小化损失和最小化参数之间的折中， \\(\\lambda\\) 越大，更偏向于使参数小，反之，更偏向于选择损失小。 先来看一个正则化解决过拟合的例子。由（87）式求导得： $$\\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n}w \\quad (88)$$ $$\\frac{\\partial C}{\\partial b} = \\frac{\\partial C_0}{\\partial b} \\quad (89)$$ 其中关于 \\(C_0\\) 的求导部分可以根据之前的反向传播算法求，于是可以得到参数的更新公式： $$b \\rightarrow b - \\eta \\frac{\\partial C_0}{\\partial b} \\quad (90)$$ $$w \\rightarrow w - \\eta \\frac{\\partial C_0}{\\partial w} - \\frac{\\eta \\lambda}{n}w \\quad (91)$$ $$w \\rightarrow (1 - \\frac{\\eta \\lambda}{n})w - \\eta \\frac{\\partial C_0}{\\partial w} \\quad (92)$$ 由于 \\(1-\\frac{\\eta \\lambda}{n}\\) 的存在，权重每次更新前都会被缩小。 对于随机梯度的情况，有： $$w \\rightarrow (1-\\frac{\\eta \\lambda}{n})w - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial w} \\quad (93)$$ 对于bias则没有变化： $$b \\rightarrow - \\frac{\\eta}{m}\\sum_x \\frac{\\partial C_x}{\\partial b} \\quad (94)$$ 使用正则化的代码运行（ \\(\\eta = 0.5, \\lambda = 0.1\\) ），在训练数据上，损失函数下降如下： 但是这一次在测试数据上，准确度却是一直上升的： 这就说明在正则化的帮助下，过拟合被解决了。而且新的准确率87.1%也高于之前没有正则项时的82.27%。 那么如果我们使用50000张图片训练呢，之前在50000张图片的情况下，并没有遇到很严重的过拟合问题，那么正则项能否进一步改善模型呢？注意到由于 \\(n=1000\\) 这次变成了 \\(n=50000\\) ，所以 \\(1-\\frac{\\eta \\lambda}{n}\\) 也发生了变化，再使用 \\(\\lambda = 0.1\\) 将导致权重变小的很少，正则化影响很少，于是将其改为 \\(\\lambda = 5.0\\) 可以看出正则项发生了作用。首先，测试数据上的准确率由95.49%提高到了96.49%。其次可以看到训练数据的准确率曲线和测试数据的准确率曲线之间的间隔变小了，两者更加接近了，说明过拟合的情况被减轻了。 这些都表明正则化可以减轻过拟合的影响进而提高分类准确率，除此之外，正则化还有一个好处。从训练过程中发现，在未使用过拟合之前，由于权重初始化的随机性，导致每次运行的结果之间差异很大，经常会出现损失函数掉入局部最小值的情况。但是在加入了正则项之后，不同的运行之间更容易出现相同的结果。 为什么会这样呢？直觉上来看，当没有正则项的时候，weights向量会增加，导致算法更难正确的在参数空间上找到正确的下降方向。 2.3 为什么正则化可以减轻过拟合问题 现在有如下图中的数据来构建一个简答的模型： 暂且使用多项式模型来拟合这些数据。图中有10个点，很显然使用一个9阶的多项式可以完美拟合这些点： 当然我们也可以使用一条直线，也就是1阶多项式来拟合它们： 哪一个模型更好呢？哪个模型在面对新数据的时候会有更好的泛化能力呢？ 在不知道其他信息的情况下，我们并不好直接作出结论。先考虑两种可能：1. 9阶多项是真实的模型，可以完美拟合新的数据 2. 1阶多项式是真实模型，但是它在预测的时候会出现一些误差。 在图中给定的数据上看，两者之间并没有太大的差异。但是设想有一个非常大的 \\(x\\) ，由于 \\(x&#94;9\\) 等高阶的影响，两个模型预测的 \\(y\\) 就有非常大的区别了。一种观点是选择更简单的模型，简单的模型能解释的情况更不容易是由于巧合出现的。这个9阶的模型更可能只是完美的拟合了既有数据，对于未知数据缺乏预测能力。选择更简单的模型也被称为\"Occam的剃刀\"原则。 回到神经网络上来，对于weights小的的模型，输入的改变对于输出的改变影响很小，模型就不容易学习到数据上呈现出来的细枝末节的噪声规律。 这里接下来作者将了很多关于过拟合的启发性的思考就不讲了，太长了。。。 作为这一小节的总结，回到一开始的一个问题：为什么在L2的正则项中不考虑bias？经验上看，加不加bias并不会太大的影响结果，所以并没有特别的准则说要不要加bias。但是注意到，相对于大的weights，一个大的bias并不会使得一个神经元对输入变得更加敏感。我们也不用担心大的bias会让模型过多的学习数据中的噪声规律。而且大的bias会使得模型更加灵活，例如大的bias会让神经元更容易被saturated。所以我们通常并不对bias做正则化处理。 2.4 正则化的其它方法 除了L2正则化以外，还有很多正则化的手段。这里稍微讲一下常见的几种方法：L1正则化，dropout，人造数据。 2.4.1 L1正则化 损失函数为： $$C = C_0 + \\frac{\\lambda}{n}\\sum_w |w| \\quad (95)$$ 求导后： $$\\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n}sgn(w) \\quad (96)$$ 其中 \\(sgn(w)\\) 函数当， \\(w\\) 为正时为1， \\(w\\) 为负时为-1。于是有： $$w \\rightarrow w' = w - \\frac{\\eta \\lambda}{n} sgn(w) -\\eta \\frac{\\partial C_0}{\\partial w} \\quad (97)$$ 之前L2的情形： $$w \\rightarrow w' = w(1 - \\frac{\\eta \\lambda}{n}) - \\eta \\frac{\\partial C_0}{\\partial w} \\quad (98)$$ 从两种式子可以看出，在更新 \\(w\\) 之前，weights都先被缩小了。不过两者的方式不同，L1是按一个常数 \\(\\frac{\\eta \\lambda}{n}\\) 进行缩小的（这个缩小应该理解为绝对值的缩小），L2则是按照一定的比例 \\(1-\\frac{\\eta \\lambda}{n}\\) 在原有基础上缩小。所以说，对于一个 \\(|w|\\) 很大的权重，L1正则化对其的减小程度要远小于L2正则化对其的减小程度。反之，对于一个 \\(|w|\\) 小的权重，L1的作用更加明显于L2。这也将导致，L1正则化更倾向于保留神经网络中更加重要的少量链接，而将其它的权重减小到0。 上面其实还有一个细节就是当 \\(w=0\\) 的时候， \\(sgn(w)\\) 导数是不存在的这个时候怎么办。这个时候只要当成没有正则化项就好了，这也很好理解，L1本来就是将权重减小到0，既然它已经是0了，当然不能再减小了。 2.4.2 Dropout Dropout是一种完全不同于L1，L2正则化的方法，它并不依赖于修改损失函数，而是直接去修改神经网络的结构。 假设我们现在在训练这样一个神经网络： 设想有一个训练输入 \\(x\\) 和对应的输出 \\(y\\) 。正常情况下，我们一次前向传播遍历所有神经元，然后通过后向传播计算梯度。Dropout的过程则不同。开始的时候，随机删除掉隐藏层的一半神经元： 同样的方法，在修改过后的神经网络上进行前向和后向传播。在完成一次mini-batch后，更新weights和biases。然后复原神经元，再随机删掉隐藏层的神经元，重复这样的过程。 这样，我们就能学习到一系列的weights和biases。但是这是在只有一半隐藏层神经元的情况下得到的，所以当运行完整神经网络的时候，隐藏神经元都会起作用，为了补偿这种差异，对隐藏层的参数取一半值。 当我们dropout不同的神经元的时候，就相当于训练了多个不同的神经网络。所以dropout的过程就有点像对大量神经网络的影响取平均的过程。不同的神经网络会以不同的方式产生过拟合，dropout就会减少这种过拟合。 2.4.3 人造数据 从之前的训练结果可以看到，当只使用1000张图片进行训练的时候，准确率下降到85%左右。这很正常，因为数据不够，没办法完全体现手写数字的多样性。现在试着慢慢增大数据量，观看其准确率： 可以看出训练数据越多，准确率越好。 获取更多的训练数据是个好方法，不过实际情况下这并不总是可行的。另外一种替代方案是根据现有数据去人为制作一些训练数据。例如对于MNIST中的一副图片数字5: 对其旋转15度得到： 可以看到这仍然是一个合理的样本5，通过这样的方式就达到了增大样本数据集的目的。 这个想法是很有效而且在手写数字以外的问题上也被广泛使用。变化数据的原则是变换的操作要符合真实世界的情况。例如对于语音识别，人类可以在有背景噪声等情况下识别出正确的信息，我们于是可以对语音数据加上背景噪声作为对数据的扩展。 练习 上面提到对MNIST训练数据扩展的时候使用了微小的旋转操作，如果操作幅度很大的话会发生什么呢？ 我的感觉是如果幅度非常大的话相当于引入了一些完全不是该数字的样本，相当于引入了错误的标注样本，会降低学习效果。 除了神经网络，增大数据集会对其他的机器学习算法造成什么影响呢？看一下SVM下的结果： 可以看到，虽然在小数据集上svm的效果差很多，但是随着数据量的增加，它的结果开始逼近神经网络的结果。而且svm在50000张图片的效果是好与神经网络在5000张的效果的。所以说，增大数据集有的时候可以弥补不同机器学习算法之间的差距。 还可以发现，算法的好坏很多时候是跟数据集有关的，只有结合特定的数据集才可以得出算法A要好与算法B的结论。 以上我们讨论了过拟合和正则化，当然这并没有结束，以后还会经常接触到它们。要知道，过拟合在神经网络中是经常会遇到的问题，特别是随着现在神经网络结构的复杂性增加，使用有效的正则化方法就显得尤为重要了。 三. 参数初始化 前面在做参数初始化的时候使用的是 \\(G(0,1)\\) 的标准正态分布，虽然这是可行的，但是我们还是想看看有没有更好的方法，没准还能够加速算法的学习过程。 很容易发现标准正态分布初始化并不是一个很好的选择。设想对于这样一个神经网络，有1000个输入神经元，考虑第一层隐藏层的第一个神经元，对于它与输入层链接的参数适用标准正态分布进行初始化： 为了简化问题，假设这1000个输入神经元中一半为1，一半为0。考虑隐藏层神经元的加权输入 \\(z=\\sum_j w_j x_j + b\\) ，这就导致 \\(z\\) 相当于是501个正态分布随机变量的和，然后就可以得到 \\(z\\) 服从均值为0，标准差为 \\(\\sqrt{501}\\approx 22.4\\) 。这就说明 \\(z\\) 是一个非常\"宽\"的正态分布： 于是就会出现 \\(|z|\\) 非常大的情况，导致 \\(z>>1\\) 或者 \\(z<<-1\\) 。这就导致 \\(\\sigma (z)\\) 非常接近0或者1。也就是说这个隐藏层的神经元被saturated了。于是改变这些权重对它的激活只会造成很小的影响，进而对后续神经网络也只能造成很小的影响，对于最终的损失函数也将只造成很小的影响。于是在梯度下降算法中，这些权重的更新就非常缓慢。之前介绍的交叉熵损失函数只适用于解决输出层神经元saturated的情况，对于隐藏层神经元saturated就无能无力了。 一种更好的方式是对于一个有 \\(n_{in}\\) 个输入权重的时候，对这些权重的初始化按照均值为0，标准差为 \\(\\frac{1}{\\sqrt{n_{in}}}\\) 的正态分布初始化，对于bias的话则还是按标准正态分布初始化（理由稍后再说）。这样就得到的 \\(z=\\sum_j w_j x_j + b\\) 就是一个均值为0，更窄的的正态分布。设想，对于之前500个输入为0，500个输入为1的例子，这样的 \\(z\\) 的标准差为 \\(\\sqrt{3/2} = 1.22\\) 。这个正态分布更窄，更不容易出现saturated状态。 练习 验证上面提到的情况下， \\(z=\\sum_j w_j x_j +b\\) 的标准差为 \\(\\sqrt{3/2}\\) 答案 ： 根据概率学的知识知道：相互独立的随机变量的和的方差等于它们各自方差的和。由于其中 \\(x\\) 有500个分量为0，最终只有501个随机变量相加，得到 \\(500*\\frac{1}{1000} + 1 = \\frac{3}{2}\\) 。 之前提到bias的初始化并没有变化，因为事实上，它对神经元是否容易saturated并没有什么影响。接下来比较两种初始化下，算法的结果： 虽然最后两者都获得了不错的结果，但是新的初始化方式无疑使算法收敛的更快。之后在第四章的一些例子还将表明，在某些情况下，这种初始化不但能加快算法收敛，还能提高模型的准确度。 拓展 L2正则化与上述weights初始化之间的联系 L2正则化有的时候会起到和上述weights的初始方法一样的作用。设想我们使用老的使用标准正态分布的初始化方法：对于以下情况： (1) \\(\\lambda\\) 是一个不大小的数，导致第一次epoch后，weights受到 \\(1-\\frac{\\eta \\lambda}{m}\\) 的影响已经衰减到很小了 (2) \\(\\lambda\\) 符合 \\(\\eta \\lambda << n\\) ，每次epoch后，weights按因子 \\(e&#94;{-\\frac{\\eta \\lambda}{m}}\\) 衰减 (3) \\(\\lambda\\) 是一个不太大的数，weights逐渐衰减到 \\(\\frac{1}{\\sqrt{n}}\\) 左右，其中 \\(n\\) 是神经网络中所有权重的数量。 讨论它们之间的联系 参考这位大神的这篇博客： http://charleshm.github.io/2016/03/Regularized-Regression/ 四. 其它方法 上面已经讲了很多最经典的方法，下面介绍一些其它方法，它们大多是对上述基本方法的改进。 4.1 随机梯度下降算法的改进 基于Hessian矩阵 对于关于变量 \\(w=w_1, w_2, ...\\) ，损失函数 \\(C\\) 的泰勒展开为： $$C(w+\\Delta w) = C(w) + \\sum_j \\frac{\\partial C}{\\partial w_j}\\Delta w_j + \\frac{1}{2}\\sum_{jk}\\Delta w_j \\frac{\\partial&#94;2 C}{\\partial w_j \\partial w_k}\\Delta w_k + ... \\quad (103)$$ 可以写作为： $$C(w+\\Delta w) = C(w) + \\bigtriangledown C\\cdot \\Delta w + \\frac{1}{2}\\Delta w&#94;TH\\Delta w+ ..., \\quad (104)$$ 其中 \\(\\bigtriangledown C\\) 是之前已经定义的梯度向量， \\(H\\) 为Hessian矩阵，其中第 \\(j\\) 行第 \\(k\\) 列的元素为 \\(\\frac{\\partial&#94;2 C}{\\partial w_j \\partial w_k}\\) 。假设不考虑高阶项对 \\(C\\) 进行近似处理： $$C(w+\\Delta w) \\approx C(w) + \\bigtriangledown C \\cdot \\Delta w + \\frac{1}{2}\\Delta w&#94;T H \\Delta w \\quad (105)$$ 通过微分可以得到上面右式在 $$\\Delta w = -H&#94;{-1}\\bigtriangledown C \\quad (106)$$ 时达到最小值。 于是我们可以将 \\(w\\) 移动到 \\(w+\\Delta w = w - H&#94;{-1}\\bigtriangledown C\\) 使得损失函数减小。于是得到如下算法： 选择初始点 \\(w\\) 更新 \\(w\\) ： \\(w' = w - H&#94;{-1}\\bigtriangledown C\\) ，其中 \\(H\\) 和 \\(\\bigtriangledown C\\) 为在 \\(w\\) 下求得 更新 \\(w'\\) ： \\(w'' = w' - H&#94;{'-1}\\bigtriangledown 'C\\) ，其中 \\(H'\\) 和 \\(\\bigtriangledown 'C\\) 为在 \\(w'\\) 下求得 ... 实际情况下，（105）式只是一个近似，最好选择小一点的步伐更新 \\(w\\) ，通常乘上一个学习率因子 \\(\\Delta w = -\\eta H&#94;{-1}\\bigtriangledown C\\) 。 Hessian方法通常比标准的梯度下降算法收敛更快，但是实际中并不好实现。因为Hessian矩阵的存在，对其求逆矩阵是一个非常耗时的操作。实际中通常使用其它基于Hessian矩阵的算法，而不是直接对Hessian矩阵求逆矩阵。 基于惯性的梯度下降算法 Hessian矩阵优化的优点是它不仅可以处理梯度的信息，更能够获取梯度变化的信息。基于惯性的方法也是基于类似的考量，但是避免了大量的矩阵运算。 惯性是物理上的名词，在这里引入肯定就需要对原有算法做一些修改。首先是引入速度的概念，梯度影响的是加速度，可以改变速度，但是并不能直接改变位置，速度才对位置起直接改变的作用。其次是引入摩擦力的概念，它会逐渐减小速度。 对于每个变量 \\(w_j\\) ，引入对应的速度 \\(v = v_1, v_2, ...\\) ，于是得到，代替原来的参数更新公式 \\(w\\rightarrow w' = w - \\eta \\bigtriangledown C\\) ： $$v \\rightarrow v' = \\mu v - \\eta \\bigtriangledown C \\quad (107)$$ $$w \\rightarrow w' = w + v' \\quad (108)$$ 其中 \\(\\mu\\) 是影响着系统摩擦力的超参。为了理解这两个方程，不妨令 \\(\\mu = 1\\) ，即没有摩擦力的情况。 \\(\\bigtriangledown C\\) 为影响速度的因素，然后速度影响位置 \\(w\\) 的变化率。速度的产生依靠梯度项的不断叠加导致，这就意味着，如果几次学习过程中梯度都是一样的方向，就可以获得一定规模的速度。 例如当我们沿着斜坡下降的时候由于速度在增大，相对于标准的梯度下降就会更快的到达谷底。但是就会出现跑过头的情况。这也就是引入 \\(\\mu\\) 的原因。当 \\(\\mu = 1\\) 时，没有摩擦，当 \\(\\mu = 0\\) 时，摩擦非常大，速度无法维持，（107）式，（108）式又变成了没有惯性的情况。通常情况下 \\(\\mu\\) 是在(0,1)之间的数。 练习 问题一： 如果 \\(\\mu>1\\) 会出现什么情况 答案： 大于1说明，速度不但会随着梯度项进行积累，每次学习之间还会按 \\(\\mu\\) 倍增大，导致在梯度很小的时候，速度还是很大，无法保障最后时刻顺利收敛。 问题二： 如果 \\(\\mu<0\\) 会出现什么情况 答案： 每次速度方向，可能在前进的时候会来回震荡 4.2 其它神经元模型 前面的所有讨论一般都是给予sigmoid神经元的。实际中很多情况下，其它神经元有的时候会有比sigmoid神经元更好的表现。 tanh函数 将sigmoid函数替换为tanh函数： $$tanh(w\\cdot x+b) \\quad (109)$$ 其中tanh函数形式为： $$tanh(z) = \\frac{e&#94;z - e&#94;{-z}}{e&#94;z + e&#94;{-z}} \\quad (110)$$ 容易得到： $$\\sigma (z) = \\frac{1 + tanh(z/2)}{2} \\quad (111)$$ 这样就可以将tanh看成是sigmoid函数的一个伸缩变换后的版本。得到其图像为： 一个不同就是tanh的值域为-1到1，而不再是0到1。 练习 问题： 证明式（111） 答案： \\(1+tanh(z/2) = 1+ \\frac{e&#94;{z/2} - e&#94;{-z/2}}{e&#94;{z/2} + e&#94;{-z/2}} = \\frac{2e&#94;{z/2}}{e&#94;{z/2} + e&#94;{-z/2}} = \\frac{2}{1+e&#94;{-z}}\\) RELU rectified linear unit函数： $$max(0 , w\\cdot x + b) \\quad (112)$$ 可以看到relu和sigmoid或者tanh之间还是有很大的差距的。那么什么时候该选择它呢？目前并没有非常好的理论来帮助我们进行选择。不过由前面知道，由于 \\(\\sigma '\\) 项的原因，会使神经元处于saturated状态，tanh也会面临一样的问题，不过对于relu来说，增大其输入并不会导致神经元的saturation。另一方面，当其输入为负时，该神经元的学习就会完全终止。 本文只是一个笔记性质的总结，其实还有很多作者启发性的思考实在太长了，就没有写，想了解的推荐大家看原文。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"深度学习","url":"shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-3.html"},{"title":"《深度学习与神经网络》笔记2","text":"上一章中我们遗留了一个问题，就是在神经网络的学习过程中，在更新参数的时候，如何去计算损失函数关于参数的梯度。这一章，我们将会学到一种快速的计算梯度的算法：反向传播算法。 这一章相较于后面的章节涉及到的数学知识比较多，如果阅读上有点吃力的话也可以完全跳过这一章，把反向传播当成一个计算梯度的黑盒即可，但是学习这些数学知识可以帮助我们更深入的理解神经网络。 反向传播算法的核心目的是对于神经网络中的任何weight或bias计算损失函数 \\(C\\) 关于它们的偏导数 \\(\\frac{\\partial C}{\\partial w}\\) . 这个式子能够帮助我们知道当我们改变 \\(w\\) 或 \\(b\\) 的时候，损失函数 \\(C\\) 是怎么变化的。虽然计算这个式子可能有一点复杂，但是它提供了一种自然的，直观的解释，所以说反向传播算法并不仅仅是一种快速学习算法，它提供给我们具体的见解，帮助我们理解改变神经网络的参数是如何改变神经网络行为的。所以说反向传播算法是很值得我们去深入学习的。 当然跳过这章也没问题，把反向传播当作求梯度的黑盒也并不影响阅读作者后续的章节。虽然后面会有一些涉及到本章知识的地方，不过跳过这些依然不会影响我们理解文章的主要内容。 一. 一种基于矩阵运算快速计算神经网络输出的方法 在介绍反向传播之前，先介绍怎么利用矩阵运算快速的计算神经网络输出。其实在上一章对这一块也提到过，不过不够详细。这里再介绍一下，帮助大家逐渐适应基于矩阵运算的表示方式。 我们先引入一个能够明确表示连接神经网络中某两层之间的某两个神经元的权重的符号： \\(w&#94;l_{jk}\\) ，它表示第 \\(l-1\\) 层的第 \\(k\\) 个神经元和第 \\(l\\) 层的第 \\(j\\) 个神经元连接的权重。例如下图中的 \\(w&#94;3_{24}\\) 表示第二层的第4个神经元到第三层的第2个神经元的权重： 这个符号初看起来可能有点复杂冗长而且需要一点功夫去适应这样的定义。而且大多数人应该有跟我一样的疑惑就是为什么不用j表示输入，k表示输出，而是这样反其道而行之。在下文中作者将解释这样定义的原因。 同样的我们可以用类似的符号定义神经元的bias和受到激活后的输出。我们用 \\(b&#94;l_j\\) 表示第 \\(l\\) 层第 \\(j\\) 个神经元的bias，用 \\(a&#94;l_j\\) 表示第 \\(l\\) 层第 \\(j\\) 个神经元的输出。类似下图： 有了这些概念，我们可以把 \\(a&#94;l_j\\) 也就是第 \\(l\\) 层第 \\(j\\) 个神经元的输出同上一层神经元的输出联系起来。 $$a&#94;l_j = \\sigma (\\sum_k w&#94;l_{jk}a&#94;{l-1}_k + b&#94;l_j) \\quad (23)$$ 这个式子中的求和部分就是对上一层的神经元输出进行加权求和的过程。 为了用矩阵的形式重写这个式子，我们定义一个权重矩阵 \\(w&#94;l\\) 表示与第 \\(l\\) 层所有神经元链接的权重，这个矩阵的第 \\(j\\) 行，第 \\(k\\) 列的值 \\(w&#94;l_{jk}\\) 表示第 \\(l-1\\) 层第 \\(k\\) 个神经元与第 \\(l\\) 层第 \\(j\\) 个神经元链接的权重。同样的定义一个bias向量 \\(b&#94;l\\) 表示第 \\(l\\) 层神经元的bias，然后 \\(a&#94;l\\) 表示第 \\(l\\) 层神经元的输出。 借助于函数向量化的思想，就是将一个函数作用于一个向量上等价于将一个函数分别作用于该向量上每一个分量。例如对于函数 \\(f(x)=x&#94;2\\) ,有： $$f(\\begin{bmatrix} 2\\\\ 3 \\end{bmatrix}) = \\begin{bmatrix} f(2)\\\\f(3)\\end{bmatrix}=\\begin{bmatrix}4\\\\9\\end{bmatrix} \\quad (24)$$ 有了这些概念就可以将公式(23)改写为： $$a&#94;l = \\sigma (w&#94;l a&#94;{l-1} + b&#94;l) \\quad (25)$$ 去掉了那些表示神经元的序号 \\(j\\) 和 \\(k\\) ，这个式子帮助我们更宏观的理解上一层神经元的输出是怎么影响下一次神经元的。我们将权重矩阵乘以上一层神经元输出，再加上这一层神经元自身的bias，再经过 \\(\\sigma\\) 函数得到的就是这一层神经元的输出。 这也是作者定义 \\(w&#94;l_{jk}\\) 时用 \\(k\\) 表示 \\(l-1\\) 层的神经元序列而不是用 \\(j\\) 表示的原因。试想一下，当我们需要计算第 \\(l\\) 层第一个神经元输出的时候，需要将矩阵 \\(w&#94;l\\) 的第一行所有值 \\(w&#94;l_{1k}\\) 表示的向量和向量 \\(a&#94;{l-1}\\) 相乘，也就是加权求和，所以说这样定义更符合我们矩阵运算的规则，否则的话在进行计算的时候还需要将权重矩阵 \\(w&#94;l\\) 转置带来不必要的麻烦。 相较于纠结神经元间的联系，用矩阵表示的话更容易理解和感受层级间的联系。而且矩阵表示还有个好处就是在实际工程中，有很多快速矩阵运算的实现。 在计算公式(25)的时候，我们可以定义一个 \\(l\\) 层的加权输入的概念： \\(z&#94;l = w&#94;l a&#94;{l-1} + b&#94;l\\) ，就是在经过 \\(\\sigma\\) 函数输出之前的部分。公式(25)于是可以表示成 \\(a&#94;l = \\sigma (z&#94;l)\\) 。 \\(z&#94;l\\) 当然也是一个向量，其中每个分量 \\(z&#94;l_j\\) 表示第 \\(l\\) 层第 \\(j\\) 个神经元的加权输入。 二. 关于损失函数的两个假设 反向传播算法是为了计算损失函数的偏导数 \\(\\frac{\\partial C}{\\partial w}\\) 和 \\(\\frac{\\partial C}{\\partial b}\\) ，为了使算法可行，我们需要对损失函数的形式作两个假设。在介绍这些假设之前，我们先来看一个最常见的二次损失函数： $$C=\\frac{1}{2n}\\sum_x ||y(x)-a&#94;L (x)||&#94;2 \\quad (26)$$ 其中 \\(n\\) 是训练样本总数，对所有训练样本损失求平均， \\(y(x)\\) 是输入为 \\(x\\) 时对应的真实的输出，而 \\(L\\) 表示神经网络的层数，也就是说 \\(a&#94;L (x)\\) 表示的是输入为 \\(x\\) 时，神经网络最后一层输出层的输出，也就是神经网络的输出。 第一个假设是所有训练样本总的损失函数可以被表示成单个样本损失函数和的平均值，即： \\(C=\\frac{1}{n}\\sum_x C_x\\) . 我们很容易可以验证这个假设对于二次损失函数成立， \\(C_x = \\frac{1}{2}||y-a&#94;L||&#94;2\\) 。这个假设其实大部分时候都是成立，除了对于少数比较另类的损失函数，不过本文并不涉及。 我们需要这个假设的原因是因为方向传播算法实际是对于单个样本计算偏导数 \\(\\frac{\\partial C_x}{\\partial w}\\) 和 \\(\\frac{\\partial C_x}{\\partial b}\\) ,随后再通过对这些单样本的偏导数求平均作为 \\(\\frac{\\partial C}{\\partial w}\\) 和 \\(\\frac{\\partial C}{\\partial b}\\) 。事实上，在对 \\(w\\) 和 \\(b\\) 求偏导的时候，我们将输入 \\(x\\) 当作是固定值，所以方便起见，暂时将 \\(C_x\\) 写作 \\(C\\) ,后面再写回来。 第二个假设是损失函数可以表示成神经网络输出的函数，即 \\(C = C(a&#94;L)\\) 例如，二次损失函数就满足这样的假设，因为对于一个训练样本 \\(x\\) 来说，有： $$C=\\frac{1}{2}||y-a&#94;L||&#94;2=\\frac{1}{2}\\sum_j (y_j - a&#94;L_j)&#94;2 \\quad (27)$$ 这样就表示成了输出的函数，因为对于一个输入 \\(x\\) 来说，它实际正确的输出 \\(y\\) 是个固定值，并不是我们可以修改的变量。我们可以改变的只能是通过改变weights和biases来改变神经网络的输出 \\(a&#94;L\\) 从而影响到损失函数的值。 三. Hadamard积 - \\(s\\odot t\\) 反向传播算法基于一些常见的线性代数操作：向量的相加，向量与矩阵的积等等。其中有一种操作不是很常见，这里简单介绍一下。假设 \\(s\\) 和 \\(t\\) 是两个相同维度的向量，我们使用 \\(s\\odot t\\) 定义两个向量中对应分量相乘的操作，即 \\((s \\odot t)_j = s_j t_j\\) ,例如： $$\\begin{bmatrix} 1\\\\ 2 \\end{bmatrix} \\odot \\begin{bmatrix} 3\\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 1*3 \\\\ 2*4\\end{bmatrix}=\\begin{bmatrix}3\\\\8 \\end{bmatrix} \\quad (28)$$ 这样的乘法操作被称为Hadamard积或Schur积。 四. 反向传播算法背后的四个基本方程 反向传播算法是关于理解改变weights和biases是如何改变损失函数C，也就是计算 \\(\\frac{\\partial C}{\\partial w&#94;l_{jk}}\\) 和 \\(\\frac{\\partial C}{\\partial b&#94;l_j}\\) 。在介绍如何计算这些偏导数之前，先引入一个中间变量， \\(\\delta&#94;l_j\\) ，称其为第 \\(l\\) 层第 \\(j\\) 个神经元的误差error，反向传播算法会先计算这个中间变量，随后再将其与需要的偏导数关联起来。 为了明白这个error的定义，想象在我们的神经网络中有一个小恶魔： 这个小恶魔位于第 \\(l\\) 层第 \\(j\\) 个神经元处，它会在该神经元接收到输入的时候使坏，在上一层的加权输入和传到这个神经元的时候添加一个改变量 \\(\\Delta z&#94;l_j\\) ，导致该神经元的输出就不再是 \\(\\sigma (z&#94;l_j)\\) 而变成了 \\(\\sigma (z&#94;l_j + \\Delta z&#94;l_j)\\) 。这个改变就会一直传到下去直到最后一个输出层，使得总的损失函数改变了 \\(\\frac{\\partial C}{\\partial z&#94;l_j}\\Delta z&#94;l_j\\) 。 现在假设这个恶魔是个好恶魔，它会通过找到合适的 \\(\\Delta z&#94;l_j\\) 来帮我们减小损失函数的值。假设 \\(\\frac{\\partial C}{\\partial z&#94;l_j}\\) 是一个绝对值大的数(可以是正数或者负数)，那我们可以取一个和其正负相反的一个数 \\(\\Delta z&#94;l_j\\) 来减小损失函数。但是如果 \\(\\frac{\\partial C}{\\partial z&#94;l_j}\\) 是一个接近0的数，这个恶魔通过影响 \\(z&#94;l_j\\) 来减小损失函数的方式就显得力不从心了。此时，这个恶魔就会说，这个神经元已经接近最优状态了（当然这是在 \\(\\Delta z&#94;l_j\\) 是相对小的值的情况下的，我们会约束这个恶魔的能力，使其只能进行这些比较小的干扰）。这就有点类似于说损失函数在这个神经元上已经达到极值，没有继续优化的空间了。 这就给我们一种直觉说 \\(\\frac{\\partial C}{\\partial z&#94;l_j}\\) 可以作为该神经元error的一种评价方式。基于此，我们定义第 \\(l\\) 层第 \\(j\\) 个神经元的error为： $$\\delta&#94;l_j = \\frac{\\partial C}{\\partial z&#94;l_j}\\quad (29)$$ 反向传播算法将会对每一层 \\(l\\) 计算 \\(\\delta&#94;l\\) 然后再得到对应的 \\(\\frac{\\partial C}{\\partial w&#94;l_{jk}}\\) 和 \\(\\frac{\\partial C}{\\partial b&#94;l_j}\\) 。 也许我们会有疑问为什么这个恶魔影响的是输入 \\(z&#94;l_j\\) 而不是直接去影响输出 \\(a&#94;l_j\\) ，那样我们就可以用 \\(\\frac{\\partial C}{\\partial a&#94;l_j}\\) 作为我们在神经元上error的衡量。实际上这样做并不会对结果产生影响，只是会使得反向传播算法的计算公式更复杂一点而已，所以我们继续使用 \\(\\delta&#94;l_j = \\frac{\\partial C}{\\partial z&#94;l_j}\\) 作为error的衡量。 接下来就要介绍反向传播算法基于的四个方程了。作者在这强调，这些方程是有难度的，一开始不理解也不要灰心。在这章中，我们会多次学习它们，作者还给出了这些方程的简单证明和伪代码实现，并且还一步步将伪代码实现成了python代码。通过本章的学习，不光是知道这些公式方程，作者还将会使我们对反向传播方程有直觉上的理解，还有就是人们是怎么发现这些方程的。在这期间，我们会不断的提及这四个方程，使我们最终将会对它有更深入的理解。 方程一：输出层的error \\(\\delta &#94;L\\) $$\\delta&#94;L_j = \\frac{\\partial C}{\\partial a&#94;L_j}\\sigma '(z&#94;L_j) \\quad (BP1)$$ 根据定义我们可以验证 \\(\\delta&#94;L_j = \\frac{\\partial C}{\\partial z&#94;L_j} = \\frac{\\partial C}{\\partial a&#94;L_j}\\frac{\\partial a&#94;L_j}{\\partial z&#94;L_j} = \\frac{\\partial C}{\\partial a&#94;L_j}\\sigma '(z&#94;L_j)\\) 注意到这个式子的每个部分都不难计算的到， \\(z&#94;L_j\\) 和 \\(\\sigma '(z&#94;L_j)\\) 在计算神经网络输出的时候可以得到，左边的部分在确定了损失函数的形式之后也可以计算得到。 公式(BP1)是针对输出层上某一个神经元而言的，为了方便反向传播计算，我们将其改写为矩阵形式： $$\\delta&#94;L = \\bigtriangledown_aC \\odot \\sigma '(z&#94;L) \\quad (BP1a)$$ 对于这里的二次损失函数，有 \\(\\bigtriangledown_a C = \\bigtriangledown_a (\\frac{1}{2}\\sum_j (y_j - a&#94;L_j)&#94;2) = a&#94;L - y\\) , 注意这里求导的过程只是针对当前分量 \\(j\\) 求导，其余的分量就为0.于是有： $$\\delta&#94;L = (a&#94;L - y) \\odot \\sigma'(z&#94;L) \\quad (30)$$ 方程二：用当前层error表示下一层error $$\\delta&#94;l = ((w&#94;{l+1})&#94;T\\delta&#94;{l+1}) \\odot \\sigma'(z&#94;l) \\quad (BP2)$$ 这个方程虽然看上去比较复杂，但是每个部分都有很明确的解释。假设我们知道 \\(l+1\\) 层的error： \\(\\delta&#94;{l+1}\\) ，当同这一层的权重矩阵相乘的时候就类似于将这个error传到上一层，最后再利用Hadamard积得到 \\(l\\) 层的error。详细的推导证明在下一小节中。 有了公式(BP1)和公式(BP2)我们就可以通过先计算输出层的error进而计算每一层的error。 方程三：error等价于损失函数 \\(C\\) 对bias的变化率 $$\\frac{\\partial C}{\\partial b&#94;l_j} = \\delta&#94;l_j \\quad (BP3)$$ 这说明error \\(\\delta&#94;l_j\\) 等价于 \\(\\frac{\\partial C}{\\partial b&#94;l_j}\\) 。于是可以很容易将其写成： $$\\frac{\\partial C}{\\partial b} = \\delta \\quad (31)$$ 方程四：损失函数 \\(C\\) 对weights的变化率 $$\\frac{\\partial C}{\\partial w&#94;l_{jk}} = a&#94;{l-1}_k\\delta&#94;l_j \\quad (BP4)$$ 这个方程说明，当我们要计算某两个神经元链接的权重对损失函数影响的时候，可以先计算上一层的 \\(a&#94;{l-1}_k\\) 和下一层的 \\(\\delta&#94;l_j\\) 。而这两个值我们根据先前的知识已经知道怎么计算了。这个方程也可以被写为： $$\\frac{\\partial C}{\\partial w} = a_{in}\\delta_{out} \\quad (32)$$ 更直观的如下图所示： 从公式(32)中可以看出来，当 \\(a_{in}\\) 很小 \\(a_{in}\\approx 0\\) 的时候， \\(\\partial C/\\partial w\\) 也会很小，那样这个权重的学习就会很慢，意味着在梯度下降的学习过程中，这个权重不会发生大的变化。换句话说就是，输出小的神经元的权重学习也慢。 介绍完了这四个方程，我们再来聊聊一些关于这四个方程的理解。对于输出层，考虑公式(BP1)中的 \\(\\sigma'(z&#94;L_j)\\) ，我们在第一章学习过 \\(\\sigma\\) 函数的图像，知道它在 \\(\\sigma (z&#94;L_j)\\) 接近0或1的时候是趋于平的，也就是导数是趋于0的。所以我们就能得出结论，如果对于输出层的神经元，如果其输出非常大（ \\(\\approx 1\\) ）或非常小（ \\(\\approx 0\\) ），在这种情况下，其参数的更新是非常缓慢的。我们称这种状态的输出神经元为saturated的状态，这种状态下，参数停止更新（或更新很慢）。 根据公式（BP2）我们对其他层可以得出同样的结论，对任一处于saturated状态的神经元来说，其 \\(\\delta&#94;l_j\\) 趋向于很小的值，然后就会导致它的参数的学习会很慢。（当然要是 \\(w&#94;{l+1}\\delta&#94;{l+1}\\) 足够大，即使乘上一个很小的数也能保证积足够大的话就不是这种情形了，不过作者这里说明的只是普遍的情况） 总之，就是一个神经元在低激活或者高激活状态，或者说在saturated状态时，它的参数的更新就会很慢。 上面的一些理解并不难从观察方程得到，但是它仍然能够帮助我们更进一步在脑海中构建神经网络模型。而且我们随后会发现，证明上述方程不需要用到任何 \\(\\sigma\\) 函数的性质，所以我们可以将激活函数换成任何函数。甚至我们可以根据学习的需要设计自己的激活函数。例如，我们使用一个函数 \\(f\\) ，有 \\(f'>0\\) 恒成立，并且不会接近于0，这样就可以使神经元避免saturated的状态，就不会减慢参数的学习。在本书的后续章节，我们的确会看到很多使用自己定义的激活函数的例子，现在先让我们牢记(BP1)-(BP4)这四个方程，只有这样到时候才能明白为什么要那样修改激活函数，这样修改会造成什么样的影响。 五. 四个方程的证明(选学) 现在要开始上述四个方程的证明了，这些证明主要用到了多元函数微分的链式法则，如果对这块很熟悉的话，完全可以自己自行证明。 先来公式(BP1)的证明，这个其实在我上面介绍的时候已经证明过了，这里在介绍一下作者是怎么证明的。 公式BP1: $$\\delta&#94;L_j = \\frac{\\partial C}{\\partial z&#94;L_j} \\quad (36)$$ 根据链式法则有 \\(\\delta&#94;L_j = \\sum_k \\frac{\\partial C}{\\partial a&#94;L_k}\\frac{\\partial a&#94;L_k}{\\partial z&#94;L_j} \\quad (37)\\) ，但是我们知道对于第 \\(k\\) 个神经元的输出 \\(a&#94;L_k\\) 只决定于第 \\(k\\) 个神经元的输入 \\(a&#94;L_k\\) ，所以对于 \\(k \\neq j\\) 的情况，导数为0，所以可以去掉求和符号（其他项都为0）最后有： $$\\delta&#94;L_j = \\frac{\\partial C}{\\partial a&#94;L_j}\\frac{\\partial a&#94;L_j}{\\partial z&#94;L_j} \\quad (38)$$ 由于 \\(a&#94;L_j = \\sigma (z&#94;L_j)\\) ，所以上式又可以被写为： $$\\delta&#94;L_j = \\frac{\\partial C}{\\partial a&#94;L_j}\\sigma '(z&#94;L_j) \\quad (39)$$ 公式BP1就得到了证明。 现在证明BP2.根据链式法则可以得到： $$\\delta&#94;l_j = \\frac{\\partial C}{\\partial z&#94;l_j} \\quad (40)$$ $$\\delta&#94;l_j = \\sum_k \\frac{\\partial C}{\\partial z&#94;{l+1}_k} \\frac{\\partial z&#94;{l+1}_k}{\\partial z&#94;l_j} \\quad (41)$$ $$\\delta&#94;l_j = \\sum_k \\frac{\\partial z&#94;{l+1}_k}{\\partial z&#94;l_j} \\delta&#94;{l+1}_k \\quad (42)$$ 又根据神经网络模型知道： $$z&#94;{l+1}_k = \\sum_j w&#94;{l+1}_{kj}a&#94;l_j+b&#94;{l+1}_k = \\sum_j w&#94;{l+1}_{kj} \\sigma(z&#94;l_j) + b&#94;{l+1}_k \\quad (43)$$ 然后将其对 \\(z&#94;l_j\\) 求导得到： $$\\frac{\\partial z&#94;{l+1}_k}{\\partial z&#94;l_j} = w&#94;{l+1}_{kj}\\sigma '(z&#94;l_j) \\quad(44)$$ 将其带入公式(42)得到： $$\\delta&#94;l_j = \\sum_k w&#94;{l+1}_{kj}\\delta &#94;{l+1}_k\\sigma '(z&#94;l_j) \\quad (45)$$ 这样就完成了BP2的证明。 BP3和BP4的证明留做练习题。 练习 问题： 证明方程BP3和BP4 答案： BP3的证明： 根据链式法则展开为 \\(\\frac{\\partial C}{\\partial b&#94;l_j} = \\sum_k \\frac{\\partial C}{\\partial z&#94;l_k} \\frac{\\partial z&#94;l_k}{\\partial b&#94;l_j} = \\sum_k \\delta&#94;l_k \\frac{\\partial (\\sum_j w&#94;l_{kj}a&#94;{l-1}_j + b&#94;l_k)}{\\partial b&#94;l_j} = \\delta&#94;l_j * 1 = \\delta&#94;l_j\\) 因为只有在 \\(k = j\\) 的时候右边部分才为1，其余均为0，方程BP3就得到了证明。 BP4的证明： 跟BP3几乎一模一样，为了避免误解，这里的下标稍微改一下， \\(\\frac{\\partial C}{\\partial w&#94;l_{jk}} = \\sum_a \\frac{\\partial C}{\\partial z&#94;l_a} \\frac{\\partial z&#94;l_a}{\\partial w&#94;l_{jk}} = \\sum_a \\delta&#94;l_a \\frac{\\partial (\\sum_b w&#94;l_{ab}a&#94;{l-1}_b + b&#94;l_a)}{\\partial w&#94;l_{jk}}\\) 很显然， \\(\\frac{\\partial b&#94;l_a}{w&#94;l_{jk}}=0\\) ,然后另一部分只有在 \\(a=j,b=k\\) 时才不为0，此时就可以去掉求和符号得到： $$\\frac{\\partial C}{\\partial w&#94;l_{jk}} = a&#94;{l-1}_k \\delta&#94;l_j$$ 方程BP4证明完毕。 六. 反向传播算法 有了这些方程，我们再看看反向传播算法是如何进行梯度计算的。 设置输入层的输出 \\(a&#94;1\\) 为原始输入 \\(x\\) 前向依次计算 \\(l=2,3,...,L\\) 层的加权输入和输出: \\(z&#94;l = w&#94;la&#94;{l-1}+b&#94;l\\) 和 \\(a&#94;l = \\sigma (z&#94;l)\\) 输出层的error \\(\\delta&#94;L\\) : \\(\\delta&#94;L = \\bigtriangledown_a C \\odot \\sigma '(z&#94;L)\\) 反向传播依次计算 \\(l=L-1, L-2, ..., 2\\) 层的error： \\(\\delta&#94;l = ((w&#94;{l+1})&#94;T\\delta&#94;{l+1}) \\odot \\sigma '(z&#94;l)\\) 根据上面的结果计算各个梯度： \\(\\frac{\\partial C}{\\partial w&#94;l_{jk}} = a&#94;{l-1}_k\\delta&#94;l_j\\) 和 \\(\\frac{\\partial C}{\\partial b&#94;l_j} = \\delta&#94;l_j\\) 从算法的流程也可以明白为什么它被称为反向传播算法，因为我们是从最后一层的 \\(\\delta&#94;L\\) 开始反向计算前面的 \\(\\delta&#94;l\\) 的。因为损失函数是关于神经网络输出的函数，所以为了得到损失函数关于前面层的参数的梯度就需要不断的应用求导链式法则，一层层向前推导得到我们需要的关系的表达式。 练习 问题一： 假设我们对神经网络中的一个神经元进行替换，使其激活函数不再是sigmoid函数，而是 \\(f(\\sum_jw_jx_j + b)\\) , 这种情况下反向传播算法应该如何调整？ 答案： 只需要修改用到激活函数的部分，首先是该神经元的输出 \\(a&#94;l_j\\) 用函数 \\(f\\) 求，其次就是上面在求 \\(\\sigma '(z&#94;l)\\) 的时候，现在改为函数 \\(f'(z&#94;l_j)\\) （只影响其中一个分量，因为只替换了一个神经元） 问题二： 将神经网络中所有的sigmoid激活函数替换为线性激活函数 \\(\\sigma (z) = z\\) ，写出该情形下的反向传播算法。 答案： 使用这个激活函数的话，一个显著特征就是 \\(a=z\\) 和 \\(\\sigma '(z&#94;L) = \\vec{1}\\) ，然后就可以去掉 \\(\\odot\\) 操作，因为右边的向量各分量都为1。其实算法没什么变化，只是这些特殊情况的激活函数会简化算法中某些式子。 之前已经提到过，反向传播算法是基于每一个样本计算梯度的即 \\(C = C_x\\) 。实际中，经常会涉及到将多个梯度的情况，例如在随机梯度下降算法中使用反向传播。例如给定mini-batch大小为 \\(m\\) ，那么算法的做法是： 先针对mini-batch中每一个训练样本进行上述反向传播的计算，计算各个参数（这样就会出现m组参数） 在梯度下降更新参数的时候对m组梯度取平均后再应用到更新参数的公式中： $$w&#94;l \\rightarrow w&#94;l - \\frac{\\eta}{m}\\sum_x \\delta&#94;{x, l} (a&#94;{x, l-1})&#94;T$$ $$bl \\rightarrow b&#94;l - \\frac{\\eta}{m}\\sum_x \\delta&#94;{x, l}$$ 七. 反向传播算法的代码实现 理解了反向传播算法后，我们再来看前一章相关代码就更容易理解了。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 classNetwork ( object ): defupdate_mini_batch ( self , mini_batch , eta ): ## Update the network's weights and biases by applying ## gradient descent using backpropagation to a single mini batch. ## The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\" ## is the learning rate. nabla_b = [ np . zeros ( b . shape ) for b inself . biases ] nabla_w = [ np . zeros ( w . shape ) for w inself . weights ] for x , y in mini_batch : delta_nabla_b , delta_nabla_w = self . backprop ( x , y ) nabla_b = [ nb + dnb for nb , dnb in zip ( nabla_b , delta_nabla_b )] nabla_w = [ nw + dnw for nw , dnw inzip ( nabla_w , delta_nabla_w )] self . weights = [ w - ( eta / len ( mini_batch )) * nw for w , nw in zip ( self . weights , nabla_w )] self . biases = [ b - ( eta / len ( mini_batch )) * nb for b , nb in zip ( self . biases , nabla_b )] 之前已经说过，主要的计算部分在第11行的backprop函数，它返回的其实就是在当前样本下的 \\(\\frac{\\partial C_x}{\\partial b&#94;l_j}\\) 和 \\(\\frac{\\partial C_X}{\\partial w&#94;l_{jk}}\\) ，然后nabla_b和nabla_w是讲对应位置的结果叠加，以供在第14行和第16行更新参数时用到平均梯度时使用。下面是backprop函数的代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 classNetwork ( object ): def backprop ( self , x , y ): ## Return a tuple \"(nabla_b, nabla_w)\" representing the ## gradient for the cost function C_x. \"nabla_b\" and ## \"nabla_w\" are layer-by-layer lists of numpy arrays, similar ## to \"self.biases\" and \"self.weights\". nabla_b = [ np . zeros ( b . shape ) for b in self . biases ] nabla_w = [ np . zeros ( w . shape ) for w inself . weights ] ## feedforward activation = x activations = [ x ] ## list to store all the activations, layer by layer zs = [] ## list to store all the z vectors, layer by layer for b , w in zip ( self . biases , self . weights ): z = np . dot ( w , activation ) + b 16 zs . append ( z ) activation = sigmoid ( z ) activations . append ( activation ) ## backward pass delta = self . cost_derivative ( activations [ - 1 ], y ) sigmoid_prime ( zs [ - 1 ]) 22 nabla_b [ - 1 ] = delta nabla_w [ - 1 ] = np . dot ( delta , activations [ - 2 ] . transpose ()) ## Note that the variable l in the loop below is used a little ## differently to the notation in Chapter 2 of the book. Here, ## l = 1 means the last layer of neurons, l = 2 is the ## second-last layer, and so on. It's a renumbering of the ## scheme in the book, used here to take advantage of the fact ## that Python can use negative indices in lists. for l in xrange ( 2 , self . num_layers ): z = zs [ - l ] 32 sp = sigmoid_prime ( z ) delta = np . dot ( self . weights [ - l + 1 ] . transpose (), delta ) * sp nabla_b [ - l ] = delta nabla_w [ - l ] = np . dot ( delta , activations [ - l - 1 ] . transpose ()) return ( nabla_b , nabla_w ) def cost_derivative ( self , output_activations , y ): ## Return the vector of partial derivatives \\partial C_x / \\partial a for the output activations. return ( output_activations - y ) def sigmoid ( z ): ## The sigmoid function. return 1.0 / ( 1.0 + np . exp ( - z )) def sigmoid_prime ( z ): ## Derivative of the sigmoid function. return sigmoid ( z ) * ( 1 - sigmoid ( z )) 现在稍微解释一下这段代码： 8，9行是初始化一下需要计算的梯度值。 14-18行的for循环是计算所有的 \\(z&#94;l_j\\) 和 \\(a&#94;l_j\\) 20行是计算最后一层的error： \\(\\delta&#94;L\\) ，cost_derivate函数计算 \\(\\frac{\\partial C}{\\partial a&#94;L_j}\\) ，在损失函数为二次函数的情况下为 \\(a&#94;L_j - y_j\\) , sigmoid_prime函数就是计算sigmoid函数的导数。 22行和23行就是损失函数对输出层的参数的导数。 30行开始的for循环就是根据那四个方程进行反向传播的过程。 可见只要了解了那四个方程，反向传播算法的代码并不难理解。 作者最后强调他实现的代码并没有完全实现矩阵化，因为在mini-batch阶段，作者是使用for循环遍历其中的每个样本的。实际上这一步是可以依靠矩阵运算实现的，矩阵运算比遍历for循环要快。实际上，大部分的库中反向传播的实现都是依靠矩阵运算而不是循环遍历的。 八. 反向传播为什么被认为是快速的算法？ 我们之前提到反向传播是一种快速计算梯度的算法，那么为什么称它快速呢，这是跟什么比才说它快呢？ 为了回答这个问题，我们先看另一种计算梯度的方法。考虑损失函数为 \\(C=C(w)\\) （先不考虑bias），根据导数的概念我们有： $$\\frac{C}{w_j}\\approx \\frac{C(w+\\epsilon e_j) - C(w)}{\\epsilon} \\quad (46)$$ 其中 \\(\\epsilon>0\\) 是一个很小的正数， \\(e_j\\) 是 \\(j\\) 方向上的一个单位向量。这样的话通过计算分子上的损失函数C的两个值，就可以得到我们需要的梯度 \\(\\frac{\\partial C}{\\partial w_j}\\) ，同样的方法我们可以计算 \\(\\frac{\\partial C}{\\partial b}\\) 。 这个方法看起来很完美，而且代码实现更加容易，似乎比我们的反向传播算法更好。但是事实是，当我们尝试实现使用这种方法的时候，就会发现它的运行效率非常低下。假设我们的神经网络中有一百万个权重，那样的话为了计算梯度肯定就需要计算这一百万个 \\(C(w+\\epsilon e_j)\\) 。但是每次计算这个损失函数的值都必须从输入层开始一层层计算直到输出层（每个样本都需要经过这样的计算）。除此之外当然还需要计算一百万个 \\(C(w)\\) ，不过这个只需要计算一次神经网络的输出即可得到。 反向传播算法的聪明之处在于我们只需要一次正向遍历神经网络和一次反向遍历神经网络，就可以计算出所有的 \\(\\frac{\\partial C}{\\partial w_j}\\) 。正向遍历计算 \\(a&#94;l_j\\) 和 \\(z&#94;l_j\\) ，反向遍历计算各个 \\(\\delta &#94;l_j\\) ，然后经过简单计算就得到了需要的梯度值。所以说虽然从形式上可能会觉得反向传播算法更复杂，但其实它的计算量更少，算法更高效。 自从反向传播算法被发现，它就解决了许多神经网络上的问题。但是反向传播算法也不是万能的，尤其是在用来训练拥有非常多隐藏层的深度神经网络的时候。我们会在本书的后面章节介绍现代计算机和一些前人聪明的想法是怎么使这样的深度神经网络的学习变得可能的。 九. 反向传播概貌 到目前为止，反向传播还给我们留下了两个疑团。首先，这个算法本质上做了些什么，我们已经知道了error从输出层反向传播这个关键步骤。那我们能否了解的更深入一点，能否建立起一种直觉明白在这些矩阵向量运算背后究竟发生了什么？其次，读懂反向传播算法及其证明并不困难，但是读懂并不意味着你能凭空发现这个算法，是否存在合理的直觉或方法可以引导我们去发现这个算法呢？这一节将会围绕这两个疑团进行讨论。 为了让我们更直观的感受算法的行为，我们想象对神经网络的参数 \\(w&#94;l_{jk}\\) 进行了微调 \\(\\Delta w&#94;l_{jk}\\) ，如下图： 当然这个变化会引起相关联的神经元的输出： 这个神经元的输出的变化又会引起和它相连接的所有的神经元的变化。就这样一层层的影响，最后就到输出层了对损失函数直接造成影响： 其中 \\(\\Delta C\\) 与权重改变的关系为： $$\\Delta C \\approx \\frac{\\partial C}{\\partial w&#94;l_{jk}}\\Delta w&#94;l_{jk}\\quad (47)$$ 这就给我们带来一种计算 \\(\\frac{\\partial C}{\\partial w&#94;l_{jk}}\\) 的方法：通过给 \\(w&#94;l_{jk}\\) 引入细微的变化，然后再仔细的追踪这个变化最终对 \\(C\\) 的影响。我们按照这种思路，从权重改变处一层层往输出层推导，最后应该就可以计算出 \\(\\frac{\\partial C}{\\partial w&#94;j_{jk}}\\) 。 假设 \\(\\Delta w&#94;l_{jk}\\) 会引起第 \\(l\\) 层 \\(j\\) 个神经元的输出改变 \\(\\Delta a&#94;l_j\\) ： $$\\Delta a&#94;l_j \\approx \\frac{\\partial a&#94;l_j}{\\partial w&#94;l_{jk}}\\Delta w&#94;l_jk \\quad (48)$$ 这个输出的变化量 \\(\\Delta a&#94;l_j\\) 又将会影响到下一层所有的神经元的输出。假设我们现在只关心下一层的一个神经元 \\(q\\) ： 该神经元输出 \\(a&#94;{l+1}_q\\) 的改变为： $$\\Delta a&#94;{l+1}_q \\approx \\frac{\\partial a&#94;{l+1}_q}{\\partial a&#94;l_j}\\Delta a&#94;l_j \\quad(49)$$ 将其代入公式(48)得到： $$\\Delta a&#94;{l+1}_q \\approx \\frac{\\partial a&#94;{l+1}_q}{\\partial a&#94;l_j}\\frac{\\partial a&#94;l_j}{\\partial w&#94;l_{jk}}\\Delta w&#94;l_{jk} \\quad (50)$$ 然后当然 \\(\\Delta a&#94;{l+1}_q\\) 又会影响下一层的神经元的输出，这样一层层直到输出层。我们考虑其中的一条路径，假设经过的神经元输出为 \\(a&#94;l_j, a&#94;{l+1}_q, ..., a&#94;{L-1}_n, a&#94;L_m\\) ，那么它对 \\(C\\) 的影响为： $$\\Delta C \\approx \\frac{\\partial C}{\\partial a&#94;L_m}\\frac{\\partial a&#94;L_m}{\\partial a&#94;{L-1}_n}\\frac{\\partial a&#94;{L-1}_n}{\\partial a&#94;{L-2}_p}...\\frac{\\partial a&#94;{l+1}_q}{\\partial a&#94;l_j}\\frac{\\partial a&#94;l_j}{\\partial w&#94;l_{jk}}\\Delta w&#94;l_{jk}\\quad (51)$$ 当然这只是所有影响 \\(C\\) 路径中的一条，为了计算总的变化，我们对所有可能路径的影响进行求和： $$\\Delta C \\approx \\sum_{mnp...q} \\frac{\\partial C}{\\partial a&#94;L_m}\\frac{\\partial a&#94;L_m}{\\partial a&#94;{L-1}_n}\\frac{\\partial a&#94;{L-1}_n}{\\partial a&#94;{L-2}_p}...\\frac{\\partial a&#94;{l+1}_q}{\\partial a&#94;l_j}\\frac{\\partial a&#94;l_j}{\\partial w&#94;l_{jk}}\\Delta w&#94;l_{jk} \\quad (52)$$ 然后同公式（47）对比就能得到： $$\\frac{\\partial C}{\\partial w&#94;l_{jk}} = \\sum_{mnp...q} \\frac{\\partial C}{\\partial a&#94;L_m}\\frac{\\partial a&#94;L_m}{\\partial a&#94;{L-1}_n}\\frac{\\partial a&#94;{L-1}_n}{\\partial a&#94;{L-2}_p}...\\frac{\\partial a&#94;{l+1}_q}{\\partial a&#94;l_j}\\frac{\\partial a&#94;l_j}{\\partial w&#94;l_{jk}} \\quad (53)$$ 方程(53)看上去挺复杂，但是它有一个非常好的直观解释。这个方程告诉我们，当我们在计算 \\(C\\) 对于权重的变化率时，任意两个神经元之间的连接都相当于引入了一个变化率，这个变化率就是两个被连接的神经元的输出之间的导数即 \\(\\frac{\\partial a&#94;{l+1}_q}{\\partial a&#94;l_j}\\) , 然后每一条路径带来的变化就是这些值的积，总的变化就是所有路径带来变化的和，如下图： 上面给大家提供一种思考，就是当你影响神经网络一个参数的时候，这期间会发生什么并进而影响最后的损失函数。让我们简述一下有这些知识你还可以做哪些更进一步的讨论。首先，我们可以获得方程(53)中所有单个偏导数的明确计算式，这只需要一些积分即可。然后就可以将这个求和的运算改成矩阵相乘的形式。你就会渐渐发现我们现在做的就是反向传播所做的事情，所以说反向传播算法可以被认为是计算所有路径上偏导数积的和的一种方法。或者说，反向传播提供了一种追踪权重变化对神经网络输出影响的途径。 反向传播算法的介绍就到这里了，之前还看到过 一篇脱离神经网络单独介绍反向传播的好的博文 ，以后再抽时间介绍。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"深度学习","url":"shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-2.html"},{"title":"《深度学习与神经网络》笔记1","text":"深度学习算是现在机器学习领域非常热门的方向了，虽然一直有了解并且简单用过，但是对于其中的详细原理和来龙去脉都是略知一二，于是一直想系统学习一下该领域的相关知识。 《Neural Networks and Deep Learning》 是一份非常好的入门材料，讲解详细而且不光是介绍了理论知识，更重要的是介绍了每一步的来龙去脉以及为什么要这样做。在线文档是英文版的，我这份总结笔记的很大部分是结合原文根据自己的理解加以提炼翻译过来的，英文水平有限，出现问题请指正。想看原版完整文档的同学可以点击上面的链接。 一. 本书内容 传统的计算机算法在解决问题的时候，通常都是由程序员制定好规则将问题进行分解，一步步进行解决。应用神经网络我们一般并不需要告诉计算机应该怎么去解决问题，而是只要给到足够的观测数据就可以了，它将会自动从这些数据中提取出解决方法。 本书主要的目的是帮助读者掌握包括深度学习相关技术在内的神经网络领域的核心知识。在掌握了本书内容后，可以使用深度学习模型解决遇到的问题，更进一步可以设计自己的神经网络用以解决特定的问题。 当然这本书也不会完全是理论知识，作者通过\"手写数字识别\"这个常见但是普通编程方法很难解决的问题介绍了神经网络的基本知识。通过这本书，他还基于python一步一步实现了一个简单的神经网络库，使得大家在接触到其他新的神经网络库的时候也能很快的理解并读懂代码。 二. 手写字体识别 对于这样一副图片，人脑很容易就能识别出来其中的数字。但是对于计算机就没有这么容易了，按照以前老的解决问题的思路，就是规则的堆砌，比如说\"上部分有个圈，右下方有条垂直线，这个数字就是9\"，很显然，对于手写字体这样肯定是不切实际的。因为手写数字太不规范了，不同人的写法不一样，很多规则并不通用，通过制定精确的规则来解决这个问题的可行性微乎其微。 神经网络则不一样，它能够从训练数据自动提炼出可以识别不同数字的\"规则\"，增加训练数据量的话，这样的识别结果通常就会更加准确。在这章的最后，我们会实现一个简单的神经网络程序，虽然代码量不多，但是却可以取得不错的96%的准确率。在随后的章节，我们会逐步优化我们的神经网络，使它可以取得高达99%的准确率。 手写数字识别问题很经典，不难理解，而且计算量也不大，在本书的最后，我们也会讨论一下怎么讲在这个问题上获得的思想应用到其他的领域。 这章的重点当然不仅仅是实现神经网络代码，我们将会了解到很多重要的神经网络基本概念，包括两种最重要的神经元（感知机和sigmoid），神经网络模型最基本的学习算法（随即梯度下降算法）。作者通过大段的讨论着重讲解为什么要这样做，培养大家对于神经网络的直觉，使读者可以对神经网络有更深的理解，为后续理解深度学习打下扎实基础。 三. 感知机 最简单的感知机模型如下： 其中 \\(x_1, x_2, x_3...\\) 为模型的输入，可以是0或1的非数值型变量，也可以是数值型变量，其中每个输入维度上都有对应的一个权重 \\(w_1, w_2, w_3\\) ，模型的输出是由 \\(\\sum_jw_jx_j\\) 与阈值的大小关系决定的，等价于公式： $$output = \\left\\{\\begin{aligned} 0 &\\quad if \\sum_jw_jx_j \\leqslant threshold \\\\ 1 &\\quad if \\sum_jw_jx_j > threshold \\end{aligned}\\quad(1)\\right.$$ 通过这个公式可以看到，感知机的作用很简单，就是对输入进行加权求和，然后和阈值对比。比阈值大，表示这个神经元受到激发了，输出1，否则输出0，这样就相当于实现了一个简单二分类器。 当然这样简单的一个神经元肯定无法满足我们的需求，通常的神经网络都是有许多个神经元组合而成的网状结构，类似下图这样的： 这里的输入是以箭头的形式表示的，通常我们是将其表示为一个特殊的神经元： 该神经元没有输入，只有一个固定的输出就等价于上面input中的一个维度。 其次这个神经网络是多层的，上一层的输出作为下一层的输入。注意到，每个神经元其实只有一个输出，但是上面图里面有些神经元却有多个输出箭头，这其实是为了直观的表示这个神经元的输出会作为后一层多个神经元的输入，也就是被使用了多次。 除此之外，我们通常的表示方法并不使用上文中阈值的表达形式，而是使用类似于回归分析中的intercept的表达形式。我们设定 \\(bias\\) ， \\(b = -threshold\\) ，这样的话公式（1）就可以被写为 $$output = \\left\\{\\begin{aligned} 0 &\\quad if w*x+b \\leqslant 0 \\\\ 1 &\\quad if w*x + b > 0 \\end{aligned}\\quad(2)\\right.$$ 其中 \\(bias\\) 可以理解为表示该神经元被激发的难易程度， \\(bias越大\\) ，越容易被激发。 四. Sigmoid神经元 在介绍第二种重要的神经元sigmoid之前，我们先讲一下学习算法的设计思路。假设我们现在有一个用来解决手写数字识别问题的基于感知机的神经网络，输入是手写图片展开后的特征向量，输出是识别的数字，但是其中的参数（weights和bias）是未知的。我们想做的就是通过学习获得合适的参数使得这个网络能够正确的区分手写数字。 为了理解学习算法是怎么进行的，假设我们对一些weight或者bias引入很小的变化，使得这些小变化会对神经网络的输出也产生对应的很小的变化，如下图所示： 基于这样的假设，我们可以逐步的调整参数使得神经网络可以朝着我们需要的方向进行优化，也就是说它对手写数字的识别越来越准确。例如，在学习过程中，神经网络将\"9\"错分为\"8\"，我们可以稍微调整weights和biases使得它更偏向于将这个\"9\"正确分为\"9\"。我们重复这样的步骤，直到模型对数字的识别越来越准确。 但是由于感知机的输出非0即1，任何一个感知机的参数的细微变化都有可能导致该感知机的输出在0，1上不停的跳变，这样就会导致后续的网络结构的输出可能产生非常大的变化。也就是说，可能这个\"9\"模型正确识别了，却会导致其他数字的识别发生非常大的改变，这样就导致我们的算法很难收敛，很难通过逐渐改变参数进行优化这样的方法去得到一个稳定的准确的模型。 通过引入sigmoid神经元就可以克服这个问题。sigmoid神经元和感知机类似，但是不同的是，修改它的参数对输出只会造成很小的影响，不会出现0到1这样截然不同的变化，因为sigmoid函数是一个连续可导的函数。 sigmoid函数： $$\\sigma(z)=\\frac{1}{1+e&#94;{-z}}\\quad(3)$$ 代入输入 \\(x_1,x_2,...,\\) ,权重 \\(w_1,w_2,...\\) 和偏差 \\(b\\) ，上式可以写为： $$\\frac{1}{1+exp(-\\sum_jwjxj-b)}\\quad (4)$$ 虽然从公式上看感知机和sigmoid貌似相差很多，但其实它们两者有很多相同点，为了理解这些，我们假设 \\(z=w*x+b\\) ,当 \\(z\\rightarrow+\\infty\\) 时，有 \\(e&#94;{-z}\\approx 0\\) 和 \\(\\sigma(z)\\approx 1\\) ,sigmoid函数达到最大值。另一方面，当 \\(z\\rightarrow-\\infty\\) 时，有 \\(e&#94;{-z}\\approx +\\infty\\) 和 \\(\\sigma(z)\\approx 0\\) ,sigmoid函数达到最小值，这两种特殊情况都是等价于感知机的情形的。sigmoid和感知机的不同发生在 \\(z=w*x+b\\) 值较小的情况，通过它们的函数图像可以更直观的看到这点。 感知机的激活函数是一个阶跃函数： 可以看到感知机的激活函数在0处是不可导的，而sigmoid可以看作是一个平滑版的感知机，它的函数处处可导连续。sigmoid函数的平滑性意味着参数的细微改变 \\(\\Delta w_j\\) 和 \\(\\Delta b\\) 会对输出产生一个细微的变化 \\(\\Delta output\\) ， 利用微分的概念可以近似得到 $$\\Delta output \\approx \\sum_j\\frac{\\partial output}{\\partial w_j}\\Delta w_j+\\frac{\\partial output}{\\partial b}\\Delta b\\quad (5)$$ 上面的式子看起来有点复杂，但是它表明一个简单的事实，就是 \\(\\Delta output\\) 和参数改变量 \\(\\Delta w_j\\) 和 \\(\\Delta b\\) 的线形联系。这就使得我们可以很容易的找到合适的参数变化量来实现我们需要的在输出值上的改变。 在后面的章节中，我们会看到sigmoid也只是众多激活函数中的一种，还存在其他很多合适的激活函数，但是目前来说这些都不重要。对于我们的公式（5）来说，改变激活函数只是在改变它对于变量的偏导数，并不影响我们对于流程的理解。我们先讲解sigmoid函数只是指数函数在求导时的简洁属性，而且sigmoid也是使用最广泛的激活函数。 不像感知机的输出0和1，能够很直接的表示一个数字是与否，sigmoid函数的输出是[0,1]区间上的连续实数，但是注意到 \\(\\sigma(z)\\) 关于 \\(z\\) 在 \\((0,0.5)\\) 处是一个非常完美的中心对称形式，我们很容易想到用0.5作为判断sigmoid函数输出的阈值，例如当其输出不小于0.5时表示\"9\"，小于0.5时表示它不是\"9\"。熟悉逻辑回归的话，也能想到[0，1]的值域还可以当作是一个概率空间。 练习 问题一： 对于一个感知机网络，假设我们把它所有的 \\(w_j\\) 和 \\(b\\) 同时乘上一个正数 \\(c>0\\) ，证明这个感知机网络并没有改变。 答案： 感知机的激活函数为比较 \\(\\sum_jw_jx_j+b\\) 和0的大小（大于0输出1，小于0输出0），乘以一个正数 \\(c\\) 以后， \\(\\sum_jcw_jx_j+cb = c(\\sum_jw_jx_j+b)\\) ，并不影响它跟0的相对大小关系，当然也不会影响感知机的输出。 问题二： 对于一个感知机网络，对于给定的输入 \\(x\\) ，假设对于其中每个感知机神经元都有 \\(w*x+b\\neq 0\\) .假如我们把所有的感知机神经元替换成sigmoid神经元，然后将所有 \\(w_j\\) 和 \\(b\\) 乘一个正数 \\(c>0\\) 。证明当 \\(c\\rightarrow+\\infty\\) 时，这些sigmoid模型等价于感知机模型，如果存在一个神经元对当前输入有 \\(w*x+b=0\\) 呢？ 答案： 首先我们知道根据公式(2),感知机的输出为0和1，对于任意sigmoid函数，有 \\(\\frac{1}{1+exp(-c*wx-c*b)}=\\frac{1}{1+exp(c*(-wx-b))}\\) ，然后如果 \\(wx+b>0\\) ，由于 \\(c\\rightarrow +\\infty\\) ,则有 \\(\\sigma \\rightarrow \\frac{1}{1+0}=1\\) ，如果 \\(wx+b<0\\) ，则有 \\(\\sigma \\rightarrow \\frac{1}{1+\\infty}=0\\) ,和感知机模型的行为一样！显然如果 \\(wx+b=0\\) ，则没有这样的结论，因为 \\(\\sigma = \\frac{1}{1+1} = 0.5\\) 和感知机情形不符合。 五. 神经网络的结构 下一节中，我们将会引入一个处理手写数字识别的神经网络，但在这之前，我们先解释神经网络中常用的一些概念。对于这样一个神经网络： 其中最左边的一层被称为输入层，最右边的一层称为输出层，其中中间的一层是隐藏层，这里只有一层隐藏层，但是隐藏层是可以有很多层的。 输入层和输出层的结构通常可以根据具体问题直接得到。比如说我们要判断一副手写数字图片表示的是\"9\"还是不是。最简单的方法，就是把图片的像素展开为一维向量，也就是对于64*64=4096的图片，输入层就是4096个神经元，输出层只有一个神经元，因为只要判断是否是\"9\"。 但是对于隐藏层就没有这么简单了，通常很难总结出设计隐藏层的通用方法。于是神经网络研究者就通过启发式的方法开发了许多隐藏层的设计。比如，一些启发式的想法就是通过考虑神经网络的训练时间来权衡隐藏层的数量。我们将在本书的后面见到其他的一些启发式的设计。 到目前为止，我们讨论的神经网络都是上一层的输出作为后一层的输入。这种神经网络也被称为前馈（feedback）神经网络，意味着在这种网络中不存在循环结构。输出不能通过反馈影响到输入。 然而也有存在含有反馈的循环神经网络（recurrent neural networks RNN）。这种模型的设计是考虑到一个神经元受到激发后，激发状态并不会随着输入的改变而立刻变化，而是会存在一段时间，通过反馈影响到输入。（RNN的详细情况，我目前也不是很清楚，以后再专门介绍一下。） RNN相对于前馈神经网络来说应用没有那么广泛，对于RNN的学习算法不够成熟。但是RNN还是非常有意义的，因为相对于前馈神经网络来说，它更符合我们人脑的行为。不过，本书还是更关注前馈神经网络多一点。 六. 一个用于手写数字识别的简单神经网络 在定义了神经网络后，我们回到手写数字识别这个问题上。实际中，这个问题其实包含两个字问题，(1)对一串数字进行分割成一个一个的数字，也就是说确定一串数字中，数字与数字之间的边界。(2)对每个分割出的数字进行判断，识别出它表示哪个数字。 第一个问题不是我们的重点，我们主要关注第二个问题，而且我们的训练数据MNIST也是分割好的数字，一张图片就是一个数字。 为了解决这样一个识别问题，我们使用一个三层的神经网络： 由于MNIST数据都是28*28=784的图片，所以这里我们使用的神经网络的输入层也有784个神经元组成（图中只画出了部分），每个维度输入值的都是该像素点的灰度值，0表示白，1表示黑，0到1之间的小数表示不能灰度的灰色。 第二层为网络的隐藏层，我们定义它包含 \\(n\\) 个神经元，我们将会测试不同的 \\(n\\) 找出最佳的结构。图中的例子显示的是一个比较小的隐藏层结构，只有 \\(n=15\\) 个神经元。 输出层由10个神经元组成，显然对应的是0-9的10个数字。假如第一个神经元被激发，输出1，表示这个数字是0，第二个神经元被激发则表示是1，以此类推。具体的说，它们的输出是[0，1]上的实数，可能会出现多个神经元被激发的情况，我们取输出最大的那个神经元代表的数字作为输出结果。 可能有人会有疑问，是不是只需要四个神经元就够了，因为类似于二进制编码， \\(2&#94;4=16>10\\) ，四个神经元就足够可以表示10种不同的情形了。为什么我们要用10个呢？这样是不是显得效率不够高？其实最终作出使用10个神经元的决定也是经验性的，我们可以尝试不同的结构，最后发现，对于这个问题，10个神经元作为输出层的模型要好于4个神经元作为输出层的模型。 但是为什么会出现这样的情况呢？是否存在直觉能提前告诉我们，我们应该使用10个而不是4个吗？ 为了理解为什么这样做，我们思考一下这里神经网络是怎么运行的。先考虑使用10个神经元的输出。对于第一个输出神经元，也就是决定数字是否为0的那个输出神经元，它所做的是对隐藏层的输出结果进行加权求和。那么隐藏层呢，设想隐藏层第一个神经元是为了检测图片中是否含有这样的模式： 这个神经元可以通过对这些区域的像素点给予大的权重，而对其他区域的像素点给予小的权重实现这样的功能。同样的，设想隐藏层中的第二个，第三个，第四个神经元分别可以检测下面这些图片特征： 可以看到，这个例子，也就是这四个特征刚好组成了0这个数字： 所以如果这几个隐藏层的神经元都被激发的话，我们就可以判断这个数字是0了。当然，我们判断是0的特征并不是只有这些，实际中0的写法也是多种多样的。 这样的话似乎解释了为什么使用10个输出神经元比4个输出神经元效果好。因为假如用4个输出的话，就不大好将数字的特征和最终的输出联系起来。 不过这其实也是一种启发式的想法。并没有什么证据证明这个三层的神经网络必须按照这种方式进行数字的识别：一个隐藏神经元识别图像的一部分特征形状。也许会存在更好的学习算法使得4个神经元输出的神经网络一样可以达到很好的效果。 练习 如下图所示，通过在原有三层神经网络的基础上再加上一个新的由四个神经元组成的输出层，就可以实现数字的二进制表达形式的输出，假设这个三层神经网络识别准确度非常高，也就是正确数字所在的神经元的激活值不小于0.99，其他神经与激活值小于0.01，试找出最后一层的weights和bias 答案： 这个题目答案并不唯一，考虑到可以这样： \\(0\\rightarrow 0001, 1\\rightarrow 0010, 2\\rightarrow 0011...\\) ，然后发现最后一层第一个神经元为1的时候对应的数字有7，8，9，第二神经元位1的时候对应的数字有3，4，5，以此类推。然后按照这些规律把对应有联系的地方的权重设置为1，没联系的可以设置为0，再稍微调节bias就可以了，这里就不具体算了。 七. 梯度下降学习算法 在设计好了我们的神经网络之后，它怎样才能识别数字呢？首先需要通过训练集进行学习。这里我们使用MNIST数据集。MNIST大家应该都了解过，都是28*28=784像素的图片。 这里我们定义训练输入为 \\(x\\) ，长度为784的一维向量。定义 \\(y=y(x)\\) 为输入为 \\(x\\) 下正确的输出，是一个10维的向量。例如：对于数字为6的一副图片， \\(y(x) = (0,0,0,0,0,0,1,0,0,0)&#94;T\\) . 我们想要的是算法可以帮我们找到合适的weights和biases使得我们神经网络的输出可以尽可能的接近它的真实值。定义一个损失函数： $$C(w, b) = \\frac{1}{2n}\\sum_x||y(x)-a||&#94;2\\quad (6)$$ 这里 \\(w\\) 表示所有权重， \\(b\\) 表示所有偏差， \\(n\\) 是训练数据集的大小， \\(a\\) 是输入为 \\(x\\) 时我们的神经网络的输出。我们的目的就是找到合适的参数使损失函数的值尽量小。我们选择这个二次损失函数，而不是考虑正确分类的数量主要是因为这个函数是平滑的，连续可导。我们随后也会做一些调整，使用其他的平滑函数。 为了解决这个最小化的问题，接下来先介绍梯度下降算法。 假设我们想要最下华函数 \\(C(v)\\) ，函数的参数是 \\(v=v_1,v_2,...\\) 当然这些参数可以是任意的，为了直观表现我们假设有两个参数，图像如下： 这个函数图像很简单，直接就能看出来就是曲面的谷底就是最小值所在的地方。但是实际的情况很定存在很多各种各样非常复杂的函数，不能都能靠肉眼就能直接找出最小值。 另一种方法就是用解析的方法去对函数求导，然后再去求导数为0的解析解，也就是函数极值所在的地方。这个看上去很完美，因为找到的可以是全局的最优解，但是实际情况是函数过于复杂，或者变量非常多，根本就没法找到解析解，这种方法实际上是不可行的。 那就换个思路，类似于上图中那样，我们把函数想象成一个山谷。我们有一个球在山谷里滚动，慢慢的就会滚落到谷底了，也就是函数的极值点。也就是说，我们假设一个初始点，让这个点模仿这个球的运动在函数上移动，然后就可以找到这样的极值点了。 要实现这样的目的，我们就可以对函数求导，函数的导数值就可以表明这个山谷每处的形状，是上升的还是下降的还是平的，对应的就是导数值大于0，小于0，等于0的地方。 假设我们让球沿着 \\(v_1\\) 方向移动 \\(\\Delta v_1\\) ,沿着 \\(v_2\\) 方向移动 \\(\\Delta v_2\\) 。我们得到函数 \\(C\\) 的值的变化为 $$\\Delta C \\approx \\frac{\\partial C}{\\partial v_1}\\Delta v_1 + \\frac{\\partial C}{\\partial v_2}\\Delta v_2 \\quad (7)$$ 因为我们的目的是找到 \\(C\\) 的最小值，当然就希望每次的 \\(\\Delta C\\) 是负值。设 \\(\\Delta v = (\\Delta v_1, \\Delta v_2)&#94;T\\) ,定义函数 \\(C\\) 在方向 \\(v_1,v_2\\) 上的梯度为 $$\\bigtriangledown C= (\\frac{\\partial C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2})&#94;T \\quad (8)$$ . 于是我们可以将公式(7)改写为： $$\\Delta C \\approx \\bigtriangledown C * \\Delta v\\quad (9)$$ 这个公式也解释了为什么将 \\(\\bigtriangledown C\\) 称为梯度向量，它联系了 \\(v\\) 的变化与 \\(C\\) 的变化。 接下来回到上面我们的问题，怎么移动 \\(v\\) ，也就是怎么对 \\(\\Delta v\\) 取值，才能保证 \\(\\Delta C\\) 是负数。不妨取 $$\\Delta v = -\\eta \\bigtriangledown C\\quad (10)$$ 其中 \\(\\eta\\) 是一个很小的正数（通常被称为学习率）。然后 \\(\\Delta C \\approx -\\eta\\bigtriangledown C \\cdot \\bigtriangledown C = -\\eta||\\bigtriangledown C||&#94;2\\) ,这就可以保证 \\(\\Delta C\\leqslant 0\\) , \\(C\\) 就可以朝着减小的方向一直优化下去。公式(10)即是我们需要的\"运动规则\"，我们每次将位置为 \\(v\\) 的小球移动到： $$v \\rightarrow v' = v - \\eta \\bigtriangledown C \\quad (11)$$ 总的来说，梯度下降算法，就是我们不停的计算当前位置的梯度 \\(\\bigtriangledown C\\) , 然后朝着相反的方向移动，直到下降到谷底，类似于下图： 当然梯度下降并不是真实的物理运动，现实中，运动是存在惯性的，以后也会看到有的时候我们也会模拟惯性去解决局部最优的问题。不过这里，我们的选择就是一路朝的低谷运动，到了就停下，不考虑惯性的影响。 学习率 \\(\\eta\\) 的选择对算法有着很重要的影响， \\(\\eta\\) 必须足够小才能使公式(9)近似成立，如果 \\(\\eta\\) 过大的话会导致 \\(\\Delta C>0\\) ，而如果过小的话，又会导致每次梯度下降的过于缓慢。所以实际中， \\(\\eta\\) 是一个变量使得公式(9)成立，并且算法的效率可以接受。 上面介绍2维的情形只是方便从图像上进行直观的理解，我们可以很容易的将其推广到多维的情形。假设 \\(C\\) 是关于 \\(m\\) 个变量 \\(v_1,v_2,...,v_m\\) 的函数，于是有 $$\\Delta C \\approx \\bigtriangledown C\\cdot \\Delta v \\quad (12)$$ ，类似于2维的情形，其中 \\(\\Delta v = (\\Delta v_1, ..., \\Delta v_m)&#94;T\\) , 梯度 \\(\\bigtriangledown C\\) 为： $$\\bigtriangledown C = (\\frac{\\partial C}{\\partial v_1},...,\\frac{\\partial C}{\\partial v_m})&#94;T \\quad (13)$$ 然后我们选择 $$\\Delta v = -\\eta \\bigtriangledown C \\quad (14)$$ 每次 \\(v\\) 的变化如下 $$v \\rightarrow v' = v - \\eta \\bigtriangledown C \\quad (15)$$ 这个公式也可以被认为是梯度下降算法的定义，它提供了一种迭代的改变 \\(v\\) 的位置去寻找最小值的方法。但是它无法保证总能找到全局最小值，后面的章节会讨论这点。 梯度下降算法寻找最小值的优化策略还有另外一种理解方式。假设现在需要在某个方向上移动 \\(\\Delta v\\) 使得 \\(C\\) 可以减小最多。因为 \\(\\Delta C\\) 是负数，所以等价于最小化 \\(\\Delta C \\approx \\bigtriangledown C \\cdot \\Delta v\\) ，假设每次移动的距离为一个固定值 \\(||\\Delta v||=\\epsilon>0\\) ,能够证明当 \\(\\Delta v = -\\eta \\Delta C\\) ,其中 \\(\\eta = \\epsilon / ||\\bigtriangledown C||，||\\Delta v||=\\epsilon\\) 时， \\(\\bigtriangledown C \\cdot \\Delta v\\) 达到最小值。于是梯度下降可以被认为是不断的朝着使得该位置 \\(C\\) 处减小最多的方向移动一个很小的距离的过程。 练习 问题一： 证明上面最后一段中梯度下降算法的另一种描述。提示：柯西不等式 答案： 这里其实用不用柯西不等式无所谓，根据高中数学也能理解，两个向量相乘，如果向量的模都是固定值的话，当这两个向量方向相反的时候，乘积最小。 \\(\\Delta C \\approx \\bigtriangledown C \\cdot \\Delta v = ||\\bigtriangledown C|| \\cdot ||\\Delta v|| \\cdot cos\\theta\\) 其中 \\(cos\\theta\\) 是两个向量的夹角的余弦值，而且 \\(||\\bigtriangledown C||\\) 和 \\(||\\Delta v||\\) 是固定值所以当 \\(cos\\theta = -1\\) 时，值最小，此时有两个向量方向相反，即 \\(\\frac{\\Delta v}{||\\Delta v||} = -\\frac{\\bigtriangledown C}{||\\bigtriangledown C||}\\) ,于是有 \\(\\Delta v = -||\\Delta v||\\cdot \\frac{\\bigtriangledown C}{||\\bigtriangledown C||} = -\\epsilon \\cdot \\frac{\\bigtriangledown C}{||\\bigtriangledown C||} = -\\eta \\bigtriangledown C\\) 其中 \\(\\eta = \\epsilon/||\\bigtriangledown C||\\) 问题二： 作者解释了梯度下降算法在二维和多维的情形，给出在一维情形下类似的定义 答案： 这个基本是一样了，无非二维是三维图上的最低点，一维就是二维图上的最低点，类似于一条二次曲线，最小值在导数为零的谷底，一维就是直接对自变量求导数，而多维是对每个自变量求偏导数获得梯度，迭代的过程中都是朝着导数的反方向移动。 梯度下降算法有很多变种，其中有些更接近真实的物理运动。但是通常情况下，这些算法需要计算 \\(C\\) 对于各个自变量的二阶导数，这个运算是非常耗时的，于是便有了各种拟牛顿法等解决这类问题的技巧，不过作为一本入门书，本书中基本只使用梯度下降算法。 公式(15)应用到神经网络的学习中，得到如下公式： $$w_k \\rightarrow w'_k = w_k - \\eta \\frac{\\partial C}{\\partial w_k}\\quad (16)$$ $$b_l \\rightarrow b'_l = b_l - \\eta \\frac{\\partial C}{\\partial b_l} \\quad (17)$$ 然后不断地应用这个公式对神经网络的各个参数进行迭代直到达到停止条件。 让我们回到损失函数公式(6)，可以发现这个损失函数用到了所有的训练样本，求它们的损失的平均值。然后在求导数的时候，也会求平均值，即 \\(\\bigtriangledown C = \\frac{1}{n}\\sum_x\\bigtriangledown C_x\\) ,即每次迭代都会用到所有的样本，当训练样本过大的时候，就会导致学习的过程非常漫长。 于是便产生了随机梯度下降(Stochastic gradient descent)这种算法。它的思想是在每次迭代过程求梯度的过程中只用到了随机选择的部分训练样本，达到加速训练的目的。 比如它每次随机选择 \\(m\\) 个训练样本： \\(X_1, X_2, ..., X_m\\) ,称其为mini-batch。 \\(m\\) 的选择通常是使 \\(\\bigtriangledown C_{X_j}\\) 和 \\(\\bigtriangledown C_x\\) 近似相等，即： $$\\frac{\\sum&#94;m_{j=1}\\bigtriangledown C_{X_j}}{m}\\approx \\frac{\\sum_x\\bigtriangledown C_x}{n} = \\bigtriangledown C\\quad (18)$$ $$\\bigtriangledown C \\approx \\frac{1}{m}\\sum&#94;m_{j=1}\\bigtriangledown C_{X_j} \\quad (19)$$ 这样就保证了参数更新时的合理性。 于是公式(16),(17)就变成了 $$w_k \\rightarrow w'_k = w_k - \\frac{\\eta}{m} \\sum_j\\frac{\\partial C_{X_j}}{\\partial w_k} \\quad (20)$$ $$b_l \\rightarrow b'_l = b_l - \\frac{\\eta}{m} \\sum_j\\frac{\\partial C_{X_j}}{\\partial b_l} \\quad (21)$$ 每次只使用了一次mini-batch中的 \\(m\\) 个样本，当我们遍历玩所有训练样本的时候，相当于完成了一次epoch，需要的话，可以按照同样的方法进行下一次epoch。 注意到求不求平均其实影响不大，很多情况下前面的 \\(\\frac{1}{n},\\frac{1}{m}\\) 并不会造成多大的影响，因为我们可以通过增大或者减小学习率来抵消掉影响。 随机梯度下降算法虽然会存在一定的统计上的波动，但是我们关心的只是下降的方向，并不是梯度的准确值。随机梯度下降算法在实际中是一种应用最为广泛神经网络的优化技术。 练习 问题： 梯度下降的一个极端版本是采用大小为1的mini-batch。也就是说对每个训练样本都要更新一次模型参数。这种方式被称为online learning。说出这种方式与大小为2的mini-batch的随机梯度下降算法相比较的一个优点和一个缺点。 答案： 先说优点吧，online learning使用更为灵活，在数据量不足时不需要考虑冷启动的问题，而且每次更新模型只要考虑当前输入的一个样本，计算简单。而且模型无时无刻都在更新，因此可以处理复杂的真实情况。 缺点，我觉得可能是对异常值敏感，算法可能在有些情况不稳定。我也没怎么研究过这一块，不是很清楚。 八. 数字识别神经网络的实现 接下来就是我们亲手去实现代码这块了，在此之前先把MNIST数据下载下来。 git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git 不同于一开始说的60000张训练图片和10000张测试图片，我们将训练图片分为50000张训练集和10000张验证集。这一章我们先不用验证集，在后面的章节我们将使用它来选择\"超参\"。 先来看一下神经网络类的初始化代码： 1 2 3 4 5 6 classNetwork ( object ): def __init__ ( self , sizes ): self . num_layers = len ( sizes ) self . sizes = sizes self . biases = [ np . random . randn ( y , 1 ) for y in sizes [ 1 :]] self . weights = [ np . random . randn ( y , x ) for x , y in zip ( sizes [: - 1 ], sizes [ 1 :])] 这里sizes是一个列表，包含了每一层的神经元个数。所以如果我们想要创建一个第一层油2个神经元，第二层有3个神经元，第三层有一个神经元的神经网络的话，可以这样调用构造函数： 1 net = Network ([ 2 , 3 , 1 ]) 6，7行的代码是对参数进行初始化作为我们接下来梯度下降算法的开始点。注意到第一层没有进行bias的初始化，因为我们这里默认第一层是输入层，不需要bias。 注意到weights和biases都是由矩阵组成的list，例如net.weights[1]就是第二层到第三层神经元链接的权重。不妨定义该矩阵为 \\(w\\) ，其中 \\(w_{jk}\\) 表示第二层第k个神经元和第三层第j个神经元链接的权重。那么第三层的输出值向量为： $$a' = \\sigma (wa+b)\\quad (22)$$ 其中a为第二层的输出。 练习 问题： 给出公式(22)的分量形式 答案： 对于第三层中的第j个神经元，它的输出为： $$a'_j = \\sigma (\\sum&#94;K_{k=1}(w_{jk} * a_k) + b_j)$$ 其中K为第二层神经元的个数。 有了这些理解，很容易就能写出计算神经网络输出的代码： 1 2 def sigmoid ( z ): return 1.0 / ( 1.0 + np . exp ( - z )) 随后是feedforward函数，用来在给定输入a的情况下，计算神经网络最终的输出： 1 2 3 4 5 def feedforward ( self , a ): ## Return the output of the network if \"a\" is input. for b , w in zip ( self . biases , self . weights ): a = sigmoid ( np . dot ( w , a ) + b ) return a 接下来就是算法学习的部分了，梯度下降算法的实现： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def SGD ( self , training_data , epochs , mini_batch_size , eta , test_data = None ): ## Train the neural network using mini-batch stochastic gradient descent. ## The \"training_data\" is a list of tuples \"(x, y)\" representing the training inputs and the desired outputs. ## The other non-optional parameters are self-explanatory. ## If \"test_data\" is provided then the network will be evaluated against the test data after each epoch, ## and partial progress printed out. This is useful for tracking progress, but slows things down substantially. if test_data : n_test = len ( test_data ) n = len ( training_data ) for j inxrange ( epochs ): random . shuffle ( training_data ) mini_batches = [ training_data [ k : k + mini_batch_size ] for k inxrange ( 0 , n , mini_batch_size )] for mini_batch in mini_batches : self . update_mini_batch ( mini_batch , eta ) if test_data : print \"Epoch {0}: {1} / {2}\" . format ( 22 j , self . evaluate ( test_data ), n_test ) else : print \"Epoch {0} complete\" . format ( j ) 先看输入参数，有训练数据，epoch次数，mini-batch的大小，学习率eta，test_data控制是否需要在每次epoch后评价一下当前模型。 每次epoch时，现将训练数据打散分配到每个mini-batch起到随机抽样的效果，随后利用eta和当前mini-batch中的数据更新模型参数。这个更新过程就是梯度下降算法的更新过程： 1 2 3 4 5 6 7 8 9 10 11 12 def update_mini_batch ( self , mini_batch , eta ): ## Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. ## The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\" is the learning rate. nabla_b = [ np . zeros ( b . shape ) for b in self . biases ] nabla_w = [ np . zeros ( w . shape ) for w in self . weights ] for x , y in mini_batch : delta_nabla_b , delta_nabla_w = self . backprop ( x , y ) nabla_b = [ nb + dnb for nb , dnb in zip ( nabla_b , delta_nabla_b )] nabla_w = [ nw + dnw for nw , dnw inzip ( nabla_w , delta_nabla_w )] self . weights = [ w - ( eta / len ( mini_batch )) * nw for w , nw in zip ( self . weights , nabla_w )] self . biases = [ b - ( eta / len ( mini_batch )) * nb for b , nb in zip ( self . biases , nabla_b )] 其中大部分的工作其实是由backprop这个函数完成的，也就是我们下一章要讲的反向传播算法。它可以快速的计算损失函数的梯度，剩下的工作只是对当前mini-batch里的训练数据计算梯度值，然后再更新模型参数。完整的代码可以从上面那个git命令得到。 有了这些代码，通过训练数据训练出模型后再应用到测试数据上，根据作者给的参数： 1 net = network . Network ([ 784 , 30 , 10 ]) net . SGD ( training_data , 30 , 10 , 3.0 , test_data = test_data ) 运行了一下结果如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Epoch 0 : 8335 / 10000 Epoch 1 : 8424 / 10000 Epoch 2 : 8482 / 10000 Epoch 3 : 8493 / 10000 Epoch 4 : 8551 / 10000 Epoch 5 : 8552 / 10000 Epoch 6 : 8554 / 10000 Epoch 7 : 8572 / 10000 Epoch 8 : 8595 / 10000 Epoch 9 : 8602 / 10000 Epoch 10 : 8597 / 10000 Epoch 11 : 8613 / 10000 Epoch 12 : 8611 / 10000 Epoch 13 : 8603 / 10000 Epoch 14 : 8582 / 10000 Epoch 15 : 8628 / 10000 Epoch 16 : 8613 / 10000 Epoch 17 : 8614 / 10000 Epoch 18 : 8614 / 10000 Epoch 19 : 8614 / 10000 Epoch 20 : 8611 / 10000 Epoch 21 : 8613 / 10000 Epoch 22 : 8807 / 10000 Epoch 23 : 9458 / 10000 Epoch 24 : 9486 / 10000 Epoch 25 : 9464 / 10000 Epoch 26 : 9482 / 10000 Epoch 27 : 9445 / 10000 Epoch 28 : 9487 / 10000 Epoch 29 : 9497 / 10000 虽然开始时并没有作者运行时的准确率高，不过最后还是达到了95%左右的不错的准确率。仅仅是第一次测试就有这样的准确率已经很不错了，试着将隐藏层的神经元个数改为100，结果获得了提升到了96%左右，但是运行速度也慢了好多。 除了隐藏层的神经元个数，还有很多参数可以调节。通常来说，给神经网络调参是一件富有挑战的事，特别是当你初始参数选的很糟糕的时候。但是这门课程会让我们知道这其实并不是很重要。我们需要去培养一些直觉，一些如何去选择超参和合适的模型结构的直觉。我们将会在本书中继续讨论上述参数是怎么选择出来的。 练习 问题： 利用代码创建一个只有两层的神经网络，即只有输入输出层，训练后进行测试得到了多少准确率？ 答案： 1 net = network . Network ([ 784 , 10 ]) net . SGD ( training_data , 30 , 10 , 3.0 , test_data = test_data ) 我测试的是准确率下降了，在84%左右。 之前提到我们的神经网络算法在测试集上获得了不错的准确率94%，那么这个比较的基准是什么呢？如果是胡乱猜，那就是10%的准确率。还有一种是根据图片中平均灰度值，这个有人试验过获得了22.25%的准确率，相比10%已经提高了很多了。其他还有很多可以提高准确率的想法，但是一般要提高到50%以上就要依靠机器学习算法了。SVM直接用在原来的数据上大概获得了94%的准确，也很不错，而且在最优化的参数选择下，SVM可以达到98.5%的非常高的准确率。不过神经网络也有很大的改进余地，最好的结果更是高达99.79%，而且算法的改进也并不复杂，涉及的都是这篇文章提到的概念。作者也给出了这样一句话，对于很多问题来说： ophisticated algorithm ≤ simple learning algorithm + good training data 九. 关于深度学习 虽然我们的神经网络在数字识别这个问题上获得了很不错的表现，但是它还是有一些神秘。因为无论是weights还是biases都是自动学习得到的，对于模型我们没办法给出一个合理性的解释。是否存在一些途径使我们理解神经网络究竟是使用哪些原则进行数字识别的呢，如果知道了这些原则，是否可以获得更高的准确率？ 为了回答这些问题，我们先回到一开始神经元的解释，就是一个对多种迹象（输入）进行加权求和的过程。假设我们想要判断下面的图片中是否存在人脸： 我们可以采用类似于解决手写字体识别方法解决这个问题，将图片展开为多维向量作为神经网络的输入，然后输出就一个可以判断是否是脸的神经元。 不过不同的是，这次我们不用学习算法，我们人为设置权重和偏差。根据直觉我们将问题分解为多个子问题：左上是否有眼睛？右上是否有眼睛？中间是否有鼻子？等等。 然后如果有多个回答是\"yes\"，我们就认定这是一个脸，否则就不是。 不过这只是我们根据直觉考虑的，它存在很多问题。比如说图片角度问题就会导致脸部变形，或者眼睛识别不准等等。然而直觉告诉我们，如果可以使用神经网络解决这些子问题，那么我们也许就可以根据这些子问题的结果来解决人脸识别的问题，类似于下图中结合多个子神经网络： 不过这并不是人脸识别技术采用的方法，这里只是帮助我们建立神经网络是如何运转的直觉。 然后子神经网络同样可以被再次分解： 通过这样一层层的神经网络，就可以将问题分解的越来越小。神经网络通过这种方法，在前面的层上解决一些简单具体的问题，后面的层上解决更加复杂抽象的问题。这种一层层堆积，多层的结构被称为深度神经网络。 当然作者这里并没有真的去手动设置神经网络的各个参数，这些参数还是得靠学习算法决定。这里主要是让大家理解神经网络的层级这个概念，它一层层的结构对于这个模型意味着什么。 自2006年后，随着一系列技术的发展，使得深度学习成为可能。在很多问题上，深度学习想较于浅层神经网络获得了很大的效果提升。原因当然是，深度网络可以建立一个更加复杂的层级结构去解决复杂的问题。类似于我们在传统编程语言中的模块化设计和抽象设计去实现具有更加复杂功能的程序。 之后的文章可能不会像这篇一样了，翻译原文太长了，还是尽量精简提炼一下吧。。。 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"深度学习","url":"shen-du-xue-xi-yu-shen-jing-wang-luo-bi-ji-1.html"},{"title":"Spark ML下实现的多分类AdaBoost + NaiveBayes算法","text":"1. Naive Bayes算法 朴素贝叶斯算法算是生成模型中一个最经典的分类算法之一了，常用的有Bernoulli和Multinomial两种。在文本分类上经常会用到这两种方法。在词袋模型中，对于一篇文档 \\(d\\) 中出现的词 \\(w_0,w_1,...,w_n\\) , 这篇文章被分类为 \\(c\\) 的概率为 $$p(c|w_0,w_1,...,w_n) = \\frac{p(c,w_0,w_1,...,w_n)}{p(w_0,w_1,...,w_n)} = \\frac{p(w_0,w_1,...,w_n|c)*p(c)}{p(w_0,w_1,...,w_n)}$$ 对于一篇给定文章，分母为常数，基于朴素贝叶斯的各词在一篇文章中出现独立性假设，最后我们需要比较的就是在不同类别 \\(c\\) 下 \\(p(w_0|c)*p(w_1|c)*...*p(w_n|c)*p(c)\\) 的大小。 naive bayes模型的参数就是在每个类别 \\(c\\) 下各词出现的概率的 \\(p(w_0|c),p(w_1|c),...,p(w_n|c))\\) 和该类别出现的概率 \\(p(c)\\) ，参数的估计通常就是根据训练样本进行词频的统计并计算相应概率，其中 $$p(c) = \\frac{count(c)}{count(doc)}$$ ，即为训练数据中c类别文章的总数量除以训练集中文章的总数量。针对 \\(p(w_i|c)\\) 的估计，Bernoulli和Multinomial略有不同。 Bernoulli 文章中某词 \\( w_i\\) 出现过，则记为1，所以 $$p(w_i|c) = \\frac{count(w_i,c)}{count(c)}$$ 即为在类别为c的训练集文章中出现词 \\(w_i\\) 的文章数量除以训练集中为别为c的文章总数量 Multinomial 这种情况下文章的词并不是非0即1的one hot特征，而是带有权重的数值特征，通常可以使用tf或者tf-idf值。 $$p(w_i|c) = \\frac{T_{ci}}{\\sum_{t}{T_{ct}}}$$ 其中 \\(T_{ci}\\) 为类别c的训练文章中词 \\(w_i\\) 的所有权重和， \\(\\sum_{t}{T_{ct}}\\) 为类别c的文章中所有词的权重之和。预测的时候对于词 \\(w_i\\) 计算该词在该文章中的权重 \\(T_i\\) ，使用 \\(p(w_i|c)&#94;{T_i}\\) 作为连乘部分的概率。不过实际上经常使用对数概率，所以可以将指数运算变为乘法运算，在代码中就可以利用矩阵相乘直接计算。 还有一些细节问题，例如数据稀疏，平滑处理等因为不是本文的重点，这里就不详细解释了。 2. Adaboost算法 作为一种boosting方法，adaboost在很多算法上都有着不俗的表现。不过在基于naive bayes的文档分类领域，貌似实际效果很一般。在stack overflow上也看到有人讨论，说adaboost对于多个弱分类器的提升效果很不错，但是naive bayes的文档分类通常已经有很不错的表现了，提升效果一般。不过不管效果提升怎么样，实现一下试试也没什么坏处，顺便还可以熟悉一下spark的相关操作。经典的adaboost算法适用于二分类的情况，但是我们的文本是多分类的情况，依靠多个二分类器表决不失为一种方法，但是比较麻烦，好在找到了介绍多分类adaboost算法的论文，照着论文依葫芦画瓢也不难。下面先分别多分类和二分类的adaboost 2.1 二分类adaboost 对于给定的二类分类的训练数据集 $$T = {(x_1, y_2),(x_2, y_2)...,(x_N, y_N)}$$ 其中每个 \\(x\\) 是一个样本的特征向量， \\(y\\in\\{-1, +1\\}\\) ，算法流程如下： 初始化各个样本的权重为 $$D_1 = (w_{11}, w_{12}, ... , w_{1i}, ... , w_{1N}), w_{1i} = \\frac{1}{N}, i = 1, 2, ... , N$$ 对于第m次迭代， \\(m = 1, 2, ..., M\\) ： 每次迭代使用带有当前权重 \\(D_m\\) 的样本进行训练，得到一个基本分类器 \\(G_m(x)\\) 计算在分类器 \\(D_m\\) 下，训练样本分类结果的误差率 $$e_m = \\sum&#94;{N}_{i = 1}{w_{mi}I(G_m{x_i} \\neq{y_i})}$$ ，因为每一步权重都做了归一化，所以分母不用再除以样本权重之和 根据误差率 \\(e_m\\) 计算分类器 \\(D_m\\) 的系数 $$\\alpha_m = log\\frac{1-e_m}{e_m}$$ 根据系数 \\(\\alpha_m\\) 更新各样本的权重 $$D_{m+1} = {w_{m+1, 1}, w_{m+1, 2}, ... , w_{m+1, N}}$$ $$w_{m+1, i} = w_{m, i} * exp(\\alpha_m * I(G_m{x_i} \\neq{y_i}))$$ 对 \\(D_{m+1}\\) 做归一化处理，使 \\(\\sum_{i = 1}&#94;{N}{w_{m+1, i}} = 1\\) 最后对多个分类器 \\(D_m\\) 的结果进行加权表决， $$c(x) = argmax_k\\sum_{m = 1}&#94;{M}{\\alpha_m*I(D_m(x) = k)}$$ 注意到对于二分类的adaboost需要每次的分类误差率 \\(e_m \\leq{\\frac{1}{2}}\\) ，否则的话将会导致 \\(\\alpha_m < 0\\) ，然后样本权重的更新将会朝着反方向进行。 2.2 多分类adaboost 对于K分类的情况，算法基本与二分类的情况一致。但是要求每次的分类误差率 \\(e_m \\leq{\\frac{1}{2}}\\) 是非常困难的，联系到二分类误差率阈值选择 \\(\\frac{1}{2}\\) ，K分类的情况选择误差率为 \\(\\frac{K-1}{K}\\) ，然后 \\(\\alpha_m\\) 的计算改为 $$\\alpha_m = log(\\frac{1-e_m}{e_m}) + log(K-1)$$ 容易验证只要 \\(e_m \\leq{\\frac{K-1}{K}}\\) ，则有 \\(\\alpha_m \\geq{log(\\frac{1-\\frac{K-1}{K}}{\\frac{K-1}{K}}) + log(K-1)} = log(\\frac{1}{K-1}) + log(K-1) = 0\\) ，这种情况下，多分类adaboost对于被误分的样本的侧重加大了，因为 \\(\\alpha_m\\) 因为添加了正项 \\(log(K-1)\\) 而增大了。 adaboost的一种解释是模型为加法模型，损失函数为指数函数，学习算法为前向分步算法的分类算法，这个以后再另外写一篇。这里给出一个比较直观好懂的解释： 迭代过程中误差率小的模型具有大的模型系数，也就是说表现好的子模型在最后加权的时候具有更大的\"话语权\" 迭代过程中上一次被误分的样本在下一次训练时将会具有更大的权重，更容易被分类正确 3. Spark ML的使用 提到Spark ml就不得不提Spark mllib，两者的区别主要在于ml面向的数据是Dataset，而mllib面向的是rdd，Dataset相当于在底层rdd的基础上做了进一步的优化。而且ml中一系列算法更适合创建包含从数据清洗到特征工程再到模型训练等一系列工作的ML pipelines，这个类似于sklearn中的pipeline，非常简洁好用。 pipeline中的Transformer，Estimator，Stage等概念 官方文档 上写的很清楚，而且还有事例，就不在这里解释了。这里以naive bayes为例简单介绍一下怎么利用spark ml的pipelines进行机器学习模型的训练和预测。 首先是pipelines的创建： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // pipeline for train def createPipiline ( dataset : Dataset [ _ ]) : Pipeline ={ //step 1 sentence 拆成 words val tokenizer = new RegexTokenizer (). setInputCol ( \"sentence\" ). setOutputCol ( \"words\" ). setPattern ( \",\" ) //step 2 label 转化为以0开始的labelIndex 为了适应spark.ml val indexer = new StringIndexer (). setInputCol ( \"label\" ). setOutputCol ( \"labelIndex\" ). fit ( dataset ) //step3 统计tf词频 val countModel = new CountVectorizer (). setInputCol ( \"words\" ). setOutputCol ( \"rawFeatures\" ) //step4 tf-idf 10 val idfModel = new IDF (). setInputCol ( \"rawFeatures\" ). setOutputCol ( \"features\" ) //step5 normalize tf-idf vector val normalizer = new Normalizer (). setInputCol ( \"features\" ). setOutputCol ( \"normalizedFeatures\" ) //step6 naive bayes model val naiveBayes = new NaiveBayes (). setFeaturesCol ( \"normalizedFeatures\" ). setLabelCol ( \"labelIndex\" ). setWeightCol ( \"obsWeights\" ). setPredictionCol ( \"prediction\" ). setModelType ( \"multinomial\" ). setSmoothing ( 1.0 ) //step7 predict label to real label val labelConverter = new IndexToString (). setInputCol ( \"prediction\" ). setOutputCol ( \"predictedLabel\" ). setLabels ( indexer . labels ) newPipeline (). setStages ( Array ( tokenizer , indexer , countModel , idfModel , normalizer , naiveBayes , labelConverter )) } 这里注意到我在创建这个pipeline的时候还传入了训练数据，但是一般情况下训练数据是在拟合模型而不是在模型建立的时候就提前传入的。这里是因为最后面那个labelConverter的transformer需要使用indexer.labels这个参数，而indexer要获取这个参数就要提前拟合训练数据，也就是indexer的创建发生在整个pipeline的拟合之前，所以我就先穿入了训练数据集。注意到这里训练数据就相当于被训练了两次，所以可以先cache()操作一下。 pipeline创建好以后的使用就相对简单多了，传入数据就可以了。 1 2 3 4 5 6 7 8 9 val pipeline = ModelUsage . createPipeline ( dataRDDTrain ) // train and test val combinedModel = pipeline . fit ( dataRDDTrain ) val predictResult = combinedModel . transform ( dataRDDTest ). select ( \"predictedLabel\" , \"label\" ). rdd . map ( row => ( row . getDouble ( 0 ), row . getDouble ( 1 ))) val evaluator = newMulticlassMetrics ( predictResult ) println ( \"confusionMatrix:\" ) println ( evaluator . confusionMatrix ) println ( evaluator . accuracy ) 注意到ML拟合的结果都是Double类型的，比如说我一个label是55但是输出是55.0，评估模型准确度的时候注意一下就好，影响不大。 Spark ML的一个好处是数据dataset像水一样通过预先创建好的pipeline，可以指定每一个stage处理的column名，再添加生成的数据到新的一列。自始至终，这些中间数据都在结果的dataset里，想要哪些数据指定列名就可以了。这样的话就避免了每次都要处理数据使它们符合中间模型的输入结构，而且最后还要自己再整合需要的字段到一起。 由于我们的文章数据特点比较鲜明，没有任何参数调优，在4w(80% train 20% test)的四分类数据上就已经有了95%的正确率了。 4. 自定义扩展Spark ML 既然直接用现有naive bayes模型就已经有了95%的正确率，那要是加上adaboost呢？ 直接实现adaboost算法很简单，但是毕竟spark ml的pipeline这么好用，而dataset这么好的封装加上这么多现有的类似StringIndxer等工具类transformer总不能全部重写吧。所以就想到怎么去自定义一个跟Spark ML兼容的model，上网查了查相关资料，在已有的naivebayes模型基础上进行了改进实现了与Spark ML兼容的adaboost naivebayes model。 注意 由于我们的模型需要先拟合训练数据得到模型，随后才能使用模型，这里面分别涉及到estimator和transformer，因此我们需要分别实现这两个部分。 我要实现的adaboost+naivebayes模型是一个概率模型，因此我的Estimator和Transformer分别继承自ProbabilisticClassifier和ProbabilisticClassificationModel，而不是最原始的Estimator和Transformer，这样就减少了很多不必要的代码重写，但是如果是想玩玩整整自己实现一个模型的话就要从最基本的一点点开始写了，可以参照上面第一篇文章所讲，这里就不多细说了。 当然可能会有疑问，既然可以继承ProbabilisticClassifier，那为什么不直接集成NaiveBayes不是更简单么？我一开始也是这样想的，但是发现Spark ML里NaiveBayes里大部分方法和属性都是私有或者受保护的，我要改就得修改Spark源码，但是我的Spark程序是在公司服务器运行的，总不能每次都让公司用我改过之后的Spark包吧。。。 4.1 模型参数 首先，对于任何一个模型模型的训练，我们一般都会需要传递一些参数，这里利用scala的trait实现一个参数接口。 1 2 3 4 5 6 7 8 9 10 11 12 13 trait AdaboostNaiveBayesParams extendsParams { //进行adaboost时的最高迭代次数 final val maxIter : IntParam = new IntParam ( this , \"maxIter\" , \"max number of iterations\" ) def getMaxIter : Int = $ ( maxIter ) //进行adaboost时准确率变化小于某个阈值时迭代提前终止 final val threshold : DoubleParam = new DoubleParam ( this , \"threshold\" , \"improvement threshold among iterations\" ) def getThreshold : Double = $ ( threshold ) //朴素Bayes的平滑系数 final val smoothing : DoubleParam = new DoubleParam ( this , \"smoothing\" , \"naive bayes smooth\" ) def getSmoothing : Double = $ ( smoothing ) //朴素Bayes类型\"multinomial\"(default) and \"bernoulli\" final val modelType : Param [ String ] = new Param [ String ]( this , \"modelType\" , \"naive bayes model type\" ) def getModelType : String = $ ( modelType ) 14 } 这一部分没什么解释的，都是一些模型常用参数。 4.2 模型Estimator 这一部分可以说是最重要的部分，Estimator拟合好了，Transformer基本属于调用一下就好了。先贴代码，再一行行解释。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class AdaboostNaiveBayes ( override val uid : String ) extends ProbabilisticClassifier [ Vector , AdaboostNaiveBayes , AdaboostNaiveBayesModel ] with AdaboostNaiveBayesParams { def this () = this ( Identifiable . randomUID ( \"AdaboostNaiveBayes\" )) //model parameters assignment def setMaxIter ( value : Int ) : this. type = set ( maxIter , value ) def setThreshold ( value : Double ) : this. type = set ( threshold , value ) def setSmoothing ( value : Double ) : this. type = set ( smoothing , value ) def setModelType ( value : String ) : this. type = set ( modelType , value ) setMaxIter ( 20 ) setThreshold ( 0.02 ) setSmoothing ( 1.0 ) setModelType ( \"multinomial\" ) //method used by fit() override protected def train ( dataset : Dataset [ _ ]) : AdaboostNaiveBayesModel ={ val datasetSize = dataset . count (). toInt val labelSize = dataset . select ( \"label\" ). distinct (). count () //各子模型及其权重 val modelWeights = newArray [ Double ]( $ ( maxIter )) val modelArray = newArray [ NaiveBayesModel ]( $ ( maxIter )) var alpha = 0.0 //初始化各样本等权重 val dataWeight : ( Double , Double , Double ) => Double = ( obsWeight : Double , labelIndex : Double , prediction : Double ) =>{ if ( labelIndex == prediction ) { obsWeight } else { obsWeight * math . exp ( alpha ) } } val sqlfunc = udf ( dataWeight ) //初始化还没有prediction var temp = dataset . withColumn ( \"obsWeights\" , lit ( 1.0 )) var i = 0 42 var error1 = 2.0 var error2 = 1.0 //&& (error1 - error2) > $(threshold) var weightSum = datasetSize . toDouble * datasetSize while ( i < $ ( maxIter )) { val naiveBayes = new NaiveBayes (). setFeaturesCol ( $ ( featuresCol )). setLabelCol ( $ ( labelCol )). setWeightCol ( \"obsWeights\" ). setPredictionCol ( $ ( predictionCol )). setModelType ( $ ( modelType )). setSmoothing ( $ ( smoothing )). fit ( temp ) temp = naiveBayes . transform ( temp ). cache () var error = temp . select ( \"labelIndex\" , \"prediction\" , \"obsWeights\" ). rdd . map ( row =>{ if ( row ( 0 ) != row ( 1 )) row . getDouble ( 2 ) else 0.0 }). sum ()/( datasetSize ) error1 = error2 error2 = error alpha = Math . log (( labelSize - 1 ) * ( 1 - error ) / error ) modelWeights ( i ) = alpha modelArray ( i ) = naiveBayes //更新权重 temp = temp . withColumn ( \"obsWeights\" , sqlfunc ( col ( \"obsWeights\" ), col ( $ ( labelCol )), col ( $ ( predictionCol )))); weightSum = temp . select ( \"obsWeights\" ). rdd . map ( row => ( row . getDouble ( 0 ))). sum () temp = temp . drop ( $ ( predictionCol ), $ ( rawPredictionCol ), $ ( probabilityCol )) temp = temp . withColumn ( \"obsWeights\" , col ( \"obsWeights\" )/( weightSum / datasetSize )) i += 1 } newAdaboostNaiveBayesModel ( uid , i , modelWeights , modelArray ) } override def copy ( extra : ParamMap ) : AdaboostNaiveBayes = defaultCopy ( extra ) } 1-3行是继承ProbabilisticClassifer和实现前面我们自己定义的AdaboostNaiveBayesParam参数接口，ProbabilisticClassifer的继承使用看看源码里NaiveBayes是怎么做的就可以照着学了。 5行是一个最基本的构造函数，分配给对象一个id值 77行是一个拷贝构造函数，这个必须要实现，最简单的可以直接像这里一样调用defaultCopy函数就好了。这个函数用来在引入新的参数的时候复制当前stage返回加入新参数后的一个新模型 8-11行是给模型设定初始参数用的，这几个函数没有定义在AdaboostNaiveBayesParam里是因为这些参数的传入只发生在模型拟合前，在预测的时候是不能设定的，所以对后面的Transformer应该是不可见的，因此只在这里定义。注意到这些函数的返回类型和模型类型一致，其实就是每一步都返回一个加入的参数的新的模型，这里就利用了之前的拷贝构造函数。 13-16行是给模型设定默认参数。 19行开始的train函数就是我们在对模型调用fit方法时使用的函数。返回的是一个AdaboostNaiveBayesModel，是我们随后需要定义的跟AdaboostNaiveBayes这个Estimator对应的Transformer。 21-22行分别获取数据集的数量和其中label的数量 25-26是初始化所有子模型及其权重，因为adaboost每一次迭代都会生成一个新的模型并计算该模型在最终结果投票时的权重。 30-38是一个自定义udf函数，对每个样本计算预测的label和真实label，并根据该样本的现有权重obsWeight进行更新，可以理解为如果分类正确，其权重不变，否则增大其权重。 40行 初始化所有样本为等权重，如果样本数据非常不平衡的话，可以尝试在这一步就引入偏差权重，我由于使用的数据各个类之间数量是一样的，所以全部初始话为1 41-44行初始化一些错误率等参数 46行开始进行adaboost迭代过程。 47-49行是在当前样本权重情况下调用普通的NaiveBayes进行训练的到当前迭代下的子模型 49行这个cache一定不能少，否则迭代的速度只能呵呵了，毕竟temp用到了非常多次的action。 51-57行是计算该模型的错误率 59-61行是更新误差，并计算该模型的权重alpha 63-64行是保存当前子模型和权重 66-69行是利用之前定义的udf函数更新所有样本的权重并对其进行归一化 74 行是利用计算得到的参数去构建一个AdaboostNaiveBayesModel，这里传入所有的子模型及其权重，i表示的是总迭代次数，就是子模型的数量。 4.3 模型Transformer 这里要实现的AdaboostNaiveBayesModel是从ProbabilisticClassificationModel，因此要手动实现对应的必须要的几个方法。 代码如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class AdaboostNaiveBayesModel ( override val uid : String , val iternums : Int , val modelWeights : Array [ Double ], val modelArray : Array [ NaiveBayesModel ]) extendsProbabilisticClassificationModel [ Vector , AdaboostNaiveBayesModel ] with AdaboostNaiveBayesParams { override val numClasses = modelArray ( 0 ). pi . size private def multinomialCalculation ( features : Vector ) : Vector = { val result : Vector = new DenseVector ( newArray ( numClasses )) for ( i <- 0 until iternums ) { val prob : Vector = modelArray ( i ). theta . multiply ( features ) prob . foreachActive { ( index , value ) =>{ prob . toArray ( index ) = value + modelArray ( i ). pi ( index ) } } result . toArray ( prob . argmax ) = result ( prob . argmax ) + modelWeights ( i ) } result } override def predictRaw ( features : Vector ) : Vector ={ multinomialCalculation ( features ) } override def raw2probabilityInPlace ( rawPrediction : Vector ) : Vector = { rawPrediction match { case dv : DenseVector => var i = 0 28 val size = dv . size val maxLog = dv . values . max for ( i <- 0 until size ) { dv . values ( i ) = math . exp ( dv . values ( i ) - maxLog ) } val probSum = dv . values . sum for ( i <- 0 until size ) { dv . values ( i ) = dv . values ( i ) / probSum } dv case sv : SparseVector => throw new RuntimeException ( \"Unexpected error in AdaboostNaiveBayesModel: raw2probabilityInPlace encountered SparseVector\" )}} override def copy ( extra : ParamMap ) = { defaultCopy ( extra ) } } 第5行是读取一下总的标签个数以供后面使用 44-46行是拷贝构造函数 20-22行是对一个输入计算它在各个label下的得分，这个得分的大小表示的是判断到该标签概率的大小，但是并不是概率值，因为我们的BayesModel模型参数是做了log变换的 24-43行是怎么讲结果向量转化为和为1的概率值，30-32行是个小技巧，我一开始好奇为什么一定要减掉maxLog，因为这个按理说并不会影响到后面的计算结果，后来发现这样能避免浮点数的问题，因为不减的话，会出现求完math.exp后值约为零的情况，导致后面的计算出现问题 这样就完成了概率模型需要的几个方法了，可以对一个输入给出一个概率向量，每个维度代表在这个类的概率。 5. 写在最后 利用自定义的adaboost+naivebayes模型，测试准确率从95%增加到了96.5%左右。由于训练数据比较好，95%已经很不错了，这里主要是通过写一个自定义模型学习一下Spark ML方面的知识。之前都只是听说过，从来没用过，学习一下还是很有必要的，毕竟不能总指望着单机就能搞定所有问题。 不过注意到这里我并不是从0开始造轮子，我是从ProbabilisticClassification继承过来加以修改的，如果想要做其他模型的修改还是推荐看上面的两篇文章，然后多看看Spark ML源码里类似的模型并根据自己的需要进行修改。 然后scala也是为了用Spark ML现学的，代码可以优化的地方估计很多。 这是本人第一篇博客，希望以后可以坚持写，作为对自己工作学习的总结笔记。 参考资料 [1] Multi-class AdaBoost - T Hastie, S Rosset, J Zhu, H Zou [2] Extend Spark ML for your own model/transformer types [3] spark的NaiveBayes实现源码 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"机器学习","url":"spark-mlxia-shi-xian-de-duo-fen-lei-adaboost-naivebayessuan-fa.html"}]}